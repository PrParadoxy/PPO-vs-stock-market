{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7817ba",
   "metadata": {},
   "source": [
    "# Random walk hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d493c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "import torch.nn as nn\n",
    "import torch as th\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf6c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0008545906459064592\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data.csv\", delimiter=\"\\t\")\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"<DATE>\"] + \" \" + df[\"<TIME>\"], format=\"%Y.%m.%d %H:%M:%S\")\n",
    "df = df.set_index(\"Datetime\")\n",
    "df = df.rename(columns={\n",
    "    \"<OPEN>\": \"Open\",\n",
    "    \"<HIGH>\": \"High\",\n",
    "    \"<LOW>\": \"Low\",\n",
    "    \"<CLOSE>\": \"Close\"\n",
    "})\n",
    "prices = df[\"Close\"]\n",
    "train_price = prices[:-10**4]\n",
    "test_price = prices[-10**4:]\n",
    "print(prices.diff().abs().mean(skipna=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92301ab",
   "metadata": {},
   "source": [
    "## Defining Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c71e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, prices, max_steps=np.inf, n=10, penalty=0.00001, scale=1, start_from=0):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        \n",
    "        self.prices = np.array(prices)\n",
    "        self.n = n\n",
    "        self.penalty = penalty\n",
    "        self.scale = scale\n",
    "        self.max_steps = max_steps\n",
    "        self.start_from = start_from\n",
    "\n",
    "        # Action space: 0 = sell, 1 = hold, 2 = buy\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "         \n",
    "        # Flattened Observation space: (prices, prev_action, action_price)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(n + 2,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        prices_window = self.prices[self.current_idx - self.n:self.current_idx]\n",
    "        return np.concatenate([\n",
    "            prices_window,\n",
    "            [self.prev_action],\n",
    "            [self.action_price]\n",
    "        ]).astype(np.float32)\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if self.start_from is None:\n",
    "            self.current_idx = self.n + self.np_random.integers(0, len(self.prices) - self.n)\n",
    "        else:\n",
    "            self.current_idx = self.n + self.start_from\n",
    "        self.prev_action = 1\n",
    "        self.action_price = 0\n",
    "        self.steps = 0\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        curr_price = self.prices[self.current_idx - 1]\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        reward = 0\n",
    "\n",
    "        # Repeating the same action (buy or sell) is impossible,\n",
    "        # prev_action and action_price will not be updated.\n",
    "        # Doing nothing is discouraged as well. \n",
    "        if action == self.prev_action or action == 1: \n",
    "            reward = -self.penalty\n",
    "        elif np.abs(self.prev_action - action) == 2: # Closing a position\n",
    "            reward = (action - 1)* (self.action_price - curr_price) * self.scale\n",
    "            self.prev_action = 1\n",
    "            self.action_price = 0\n",
    "        elif self.prev_action == 1: # Openning a new position\n",
    "            self.prev_action = action\n",
    "            self.action_price = curr_price\n",
    "\n",
    "        \n",
    "        self.current_idx += 1\n",
    "        self.steps += 1\n",
    "\n",
    "        if self.current_idx >= len(self.prices):\n",
    "            terminated = True\n",
    "        \n",
    "        if self.steps >= self.max_steps:\n",
    "            truncated = True\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, reward, terminated, truncated, {}\n",
    "    \n",
    "    \n",
    "def create_env(train=True, penalty=0.00001, start_from=0, max_steps=np.inf):\n",
    "    data = train_price if train else test_price\n",
    "    vec_env = make_vec_env(lambda: TradingEnv(data, penalty=penalty, \n",
    "            start_from=start_from, max_steps=max_steps), n_envs=1)\n",
    "    return VecNormalize(vec_env, norm_obs=True, norm_reward=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34869670",
   "metadata": {},
   "source": [
    "The following combines different features before feeding it to the PPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b5734c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=64):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        self.n_prices = observation_space.shape[0] - 2 # action is one hotcoded\n",
    "        self.price_net = nn.Sequential(\n",
    "            nn.Linear(self.n_prices, 32),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(1, 2),\n",
    "            nn.SELU()\n",
    "        )\n",
    "        self.action_price_net = nn.Sequential(\n",
    "            nn.Linear(1, 2),  \n",
    "            nn.SELU(),\n",
    "        )\n",
    "        self.final_net = nn.Sequential(\n",
    "            nn.Linear(36, features_dim),\n",
    "            nn.SELU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        prices = obs[:, :self.n_prices]\n",
    "        prev_action = obs[:, -2:-1]\n",
    "        prev_price = obs[:, -1:]\n",
    "        p_out = self.price_net(prices)\n",
    "        a_out = self.action_net(prev_action)\n",
    "        ap_out = self.action_price_net(prev_price)\n",
    "        return self.final_net(th.cat([p_out, a_out, ap_out], dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaac65c0",
   "metadata": {},
   "source": [
    "## RandomAgent\n",
    "We first check the peformance of the RandomAgent that only takes random actions. In a real world scenario, there is no penalty for holding a position, but during the training it is necessary to stop the model from becoming lazy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8f7d28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Random: (np.float64(-0.0491881), np.float64(0.09071702770367865))\n",
      "Test - Random - No Penality: (np.float64(-0.005329), np.float64(0.07567183875788933))\n"
     ]
    }
   ],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def predict(self, observation, state=None, episode_start=None, deterministic=False):\n",
    "        action = self.action_space.sample()\n",
    "        return np.array([action]), None\n",
    "\n",
    "\n",
    "env = create_env(train=False)\n",
    "print(f\"Test - Random: {evaluate_policy(RandomAgent(env.action_space), env, n_eval_episodes=100)}\")\n",
    "\n",
    "env = create_env(train=False, penalty=0)\n",
    "print(f\"Test - Random - No Penality: {evaluate_policy(RandomAgent(env.action_space), env, n_eval_episodes=100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7baa985",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d8d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017418193 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0329     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 0.000146    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.002       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 20000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008520481 |\n",
      "|    clip_fraction        | 0.0661      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.766       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0253     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00443    |\n",
      "|    value_loss           | 6.52e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00433     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 30000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009294236 |\n",
      "|    clip_fraction        | 0.0784      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | -6.25       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0116     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    value_loss           | 0.000113    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=0.09 +/- 0.06\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0867     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01149631 |\n",
      "|    clip_fraction        | 0.0944     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.08      |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0391    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00579   |\n",
      "|    value_loss           | 9.15e-05   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0318      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012499565 |\n",
      "|    clip_fraction        | 0.0919      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0427     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    value_loss           | 2.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 60000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012423584 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.562      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00393    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00365    |\n",
      "|    value_loss           | 1.68e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=-0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0534     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026189113 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -4.53       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0232     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 4.72e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-0.02 +/- 0.07\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0216     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014571623 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.354      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0274     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00971    |\n",
      "|    value_loss           | 0.000161    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0504      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069804452 |\n",
      "|    clip_fraction        | 0.0767       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.943       |\n",
      "|    explained_variance   | -0.491       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0156      |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00454     |\n",
      "|    value_loss           | 1.23e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | 0.176    |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 139      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 100000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004477243 |\n",
      "|    clip_fraction        | 0.0638      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00442    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00453    |\n",
      "|    value_loss           | 5.96e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004298469 |\n",
      "|    clip_fraction        | 0.0473      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | -0.019      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0295     |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.009      |\n",
      "|    value_loss           | 3.9e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 120000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017213123 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.432       |\n",
      "|    explained_variance   | 0.587        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00738      |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | 0.000674     |\n",
      "|    value_loss           | 1.13e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00752    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 130000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009320112 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.773      |\n",
      "|    explained_variance   | -0.402      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00217     |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.00394    |\n",
      "|    value_loss           | 3.39e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=0.10 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0962      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 140000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010138966 |\n",
      "|    clip_fraction        | 0.0733      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.907      |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.044       |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 3.1e-05     |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=150000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.102       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 150000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007517923 |\n",
      "|    clip_fraction        | 0.0531      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.513      |\n",
      "|    explained_variance   | -0.0308     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0804      |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00316    |\n",
      "|    value_loss           | 1.68e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=0.08 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0819      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 160000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023676798 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.696      |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00499    |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00554    |\n",
      "|    value_loss           | 2.83e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=0.12 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.118        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 170000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038951896 |\n",
      "|    clip_fraction        | 0.0799       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.872       |\n",
      "|    explained_variance   | -1.4         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0186       |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    value_loss           | 3.54e-05     |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=0.12 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.123      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01126326 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.934     |\n",
      "|    explained_variance   | 0.296      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00346    |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.00676   |\n",
      "|    value_loss           | 1.02e-05   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.294   |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=0.06 +/- 0.07\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0579      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 190000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006625028 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.767      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0263     |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    value_loss           | 3.61e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011993784 |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.758      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    value_loss           | 2.02e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 210000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039245086 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | 0.547        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00238      |\n",
      "|    n_updates            | 1020         |\n",
      "|    policy_gradient_loss | -0.0004      |\n",
      "|    value_loss           | 1.19e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 220000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011375116 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.906      |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0192      |\n",
      "|    n_updates            | 1070        |\n",
      "|    policy_gradient_loss | -0.00158    |\n",
      "|    value_loss           | 2.43e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 230000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009716114 |\n",
      "|    clip_fraction        | 0.0723      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.926      |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 1120        |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    value_loss           | 6.63e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00871    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 240000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006222735 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.994      |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00249    |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 8.74e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 250000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026656972 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.968      |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00272    |\n",
      "|    n_updates            | 1220        |\n",
      "|    policy_gradient_loss | -0.00818    |\n",
      "|    value_loss           | 9.44e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0418     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 260000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014672462 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0327     |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.00841    |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 270000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009313922 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.964      |\n",
      "|    explained_variance   | 0.034       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0235      |\n",
      "|    n_updates            | 1310        |\n",
      "|    policy_gradient_loss | -0.00777    |\n",
      "|    value_loss           | 8.66e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.421   |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 132      |\n",
      "|    time_elapsed    | 416      |\n",
      "|    total_timesteps | 270336   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 280000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009492613 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.763      |\n",
      "|    explained_variance   | 0.803       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000255   |\n",
      "|    n_updates            | 1360        |\n",
      "|    policy_gradient_loss | -0.00527    |\n",
      "|    value_loss           | 3.29e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 290000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068070465 |\n",
      "|    clip_fraction        | 0.041        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.659       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0164      |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    value_loss           | 1.34e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 300000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040648105 |\n",
      "|    clip_fraction        | 0.0348       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.673       |\n",
      "|    explained_variance   | 0.633        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0111       |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -0.000497    |\n",
      "|    value_loss           | 1.39e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 310000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013588176 |\n",
      "|    clip_fraction        | 0.0176       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.3         |\n",
      "|    explained_variance   | 0.48         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00073     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -0.000964    |\n",
      "|    value_loss           | 2.08e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 320000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012724684 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.41        |\n",
      "|    explained_variance   | 0.626        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00105     |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -0.000401    |\n",
      "|    value_loss           | 8.62e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 330000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058228327 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.66        |\n",
      "|    explained_variance   | 0.479        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000736     |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.00316     |\n",
      "|    value_loss           | 5.37e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 340000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0158443 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.716    |\n",
      "|    explained_variance   | 0.759     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0395   |\n",
      "|    n_updates            | 1660      |\n",
      "|    policy_gradient_loss | -0.00738  |\n",
      "|    value_loss           | 3.31e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 350000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005984719 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.722      |\n",
      "|    explained_variance   | 0.554       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00469    |\n",
      "|    n_updates            | 1700        |\n",
      "|    policy_gradient_loss | 0.000189    |\n",
      "|    value_loss           | 7.24e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 360000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065465905 |\n",
      "|    clip_fraction        | 0.0842       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.743       |\n",
      "|    explained_variance   | 0.463        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00164     |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    value_loss           | 7.12e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.589   |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 554      |\n",
      "|    total_timesteps | 360448   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 370000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073049213 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.543       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00786      |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.00346     |\n",
      "|    value_loss           | 1.92e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 380000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039935536 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.634       |\n",
      "|    explained_variance   | 0.763        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0247      |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -0.00119     |\n",
      "|    value_loss           | 1.51e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 390000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005916139 |\n",
      "|    clip_fraction        | 0.0516      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.652       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00251    |\n",
      "|    n_updates            | 1900        |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    value_loss           | 2.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 400000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037297062 |\n",
      "|    clip_fraction        | 0.0698       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.735       |\n",
      "|    explained_variance   | 0.7          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0227      |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    value_loss           | 1.41e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 410000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005956047 |\n",
      "|    clip_fraction        | 0.077       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.609       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0134     |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -0.00494    |\n",
      "|    value_loss           | 7.67e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 420000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030577658 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.736       |\n",
      "|    explained_variance   | 0.0421       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0175      |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.0045      |\n",
      "|    value_loss           | 5.37e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 430000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029298817 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.726       |\n",
      "|    explained_variance   | 0.355        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00291     |\n",
      "|    n_updates            | 2090         |\n",
      "|    policy_gradient_loss | -0.000482    |\n",
      "|    value_loss           | 1.78e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 440000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025691462 |\n",
      "|    clip_fraction        | 0.0337       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.368       |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00258     |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.000856    |\n",
      "|    value_loss           | 2.2e-05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 450000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005807683 |\n",
      "|    clip_fraction        | 0.0632      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.526       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00206    |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | 0.000954    |\n",
      "|    value_loss           | 1.16e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.61    |\n",
      "| time/              |          |\n",
      "|    fps             | 650      |\n",
      "|    iterations      | 220      |\n",
      "|    time_elapsed    | 692      |\n",
      "|    total_timesteps | 450560   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0672       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 460000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048314496 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | 0.875        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00402      |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 3.45e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0728       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 470000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041134376 |\n",
      "|    clip_fraction        | 0.0345       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.371       |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00976      |\n",
      "|    n_updates            | 2290         |\n",
      "|    policy_gradient_loss | -0.000646    |\n",
      "|    value_loss           | 1.49e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 480000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004643649 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.109       |\n",
      "|    explained_variance   | 0.787        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000191    |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    value_loss           | 5.57e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 490000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00494452 |\n",
      "|    clip_fraction        | 0.0266     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.283     |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0145     |\n",
      "|    n_updates            | 2390       |\n",
      "|    policy_gradient_loss | 0.0042     |\n",
      "|    value_loss           | 1.22e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0            |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 500000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022208854 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 0.555        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.67e-05     |\n",
      "|    n_updates            | 2440         |\n",
      "|    policy_gradient_loss | 0.00283      |\n",
      "|    value_loss           | 1.62e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0135      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 510000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078079393 |\n",
      "|    clip_fraction        | 0.0599       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.598       |\n",
      "|    explained_variance   | 0.527        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0104       |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | 0.00178      |\n",
      "|    value_loss           | 6.52e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.00782     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 520000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065393522 |\n",
      "|    clip_fraction        | 0.0674       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.578       |\n",
      "|    explained_variance   | 0.265        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000507    |\n",
      "|    n_updates            | 2530         |\n",
      "|    policy_gradient_loss | 0.000969     |\n",
      "|    value_loss           | 3.07e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=0.03 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0324      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 530000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005965531 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.814      |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    value_loss           | 6.42e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.051        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 540000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060424884 |\n",
      "|    clip_fraction        | 0.0788       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.849       |\n",
      "|    explained_variance   | 0.656        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00112      |\n",
      "|    n_updates            | 2630         |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 6.67e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.647   |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 832      |\n",
      "|    total_timesteps | 540672   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00237     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 550000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008364801 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.751      |\n",
      "|    explained_variance   | 0.66        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00115    |\n",
      "|    n_updates            | 2680        |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    value_loss           | 3.55e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 560000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009951891 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.713       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00746    |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.00938    |\n",
      "|    value_loss           | 2.92e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0372      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 570000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006304099 |\n",
      "|    clip_fraction        | 0.0913      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.953      |\n",
      "|    explained_variance   | -0.206      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000896    |\n",
      "|    n_updates            | 2780        |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 8.37e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0337      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 580000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016391315 |\n",
      "|    clip_fraction        | 0.0766      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.975      |\n",
      "|    explained_variance   | -0.38       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0229     |\n",
      "|    n_updates            | 2830        |\n",
      "|    policy_gradient_loss | -0.00368    |\n",
      "|    value_loss           | 1.76e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0745      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 590000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020908218 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.982      |\n",
      "|    explained_variance   | -0.578      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0135      |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 7.64e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=0.05 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0475      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 600000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009051515 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.918      |\n",
      "|    explained_variance   | 0.313       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0098     |\n",
      "|    n_updates            | 2920        |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 5.76e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=0.01 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 610000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005740025 |\n",
      "|    clip_fraction        | 0.0471      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.845      |\n",
      "|    explained_variance   | -0.0162     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 2970        |\n",
      "|    policy_gradient_loss | -0.000573   |\n",
      "|    value_loss           | 1.31e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.012      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 620000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011041766 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.815      |\n",
      "|    explained_variance   | -0.0906     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0364     |\n",
      "|    n_updates            | 3020        |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    value_loss           | 7.59e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=0.06 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0605      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 630000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012594689 |\n",
      "|    clip_fraction        | 0.0873      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.0397      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 3070        |\n",
      "|    policy_gradient_loss | -0.0022     |\n",
      "|    value_loss           | 1.49e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.665   |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 308      |\n",
      "|    time_elapsed    | 971      |\n",
      "|    total_timesteps | 630784   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=0.05 +/- 0.07\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0458      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 640000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017552929 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.657       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00838     |\n",
      "|    n_updates            | 3120        |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    value_loss           | 3.57e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0522      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 650000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016483512 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.822      |\n",
      "|    explained_variance   | -0.782      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0267     |\n",
      "|    n_updates            | 3170        |\n",
      "|    policy_gradient_loss | -0.00665    |\n",
      "|    value_loss           | 3.7e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0294       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 660000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042990753 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.851       |\n",
      "|    explained_variance   | 0.59         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0049       |\n",
      "|    n_updates            | 3220         |\n",
      "|    policy_gradient_loss | -0.00139     |\n",
      "|    value_loss           | 3.75e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=0.06 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0587      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 670000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004922254 |\n",
      "|    clip_fraction        | 0.0872      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | -0.0137     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 3270        |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    value_loss           | 1.56e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.044      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 680000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03008271 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.822     |\n",
      "|    explained_variance   | -0.289     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0656     |\n",
      "|    n_updates            | 3320       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    value_loss           | 6e-06      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=0.11 +/- 0.07\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.11         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 690000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060298764 |\n",
      "|    clip_fraction        | 0.0589       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.834       |\n",
      "|    explained_variance   | 0.00235      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00252     |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -0.000241    |\n",
      "|    value_loss           | 6.43e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=0.06 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0567      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 700000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009828353 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.839      |\n",
      "|    explained_variance   | 0.0021      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0197      |\n",
      "|    n_updates            | 3410        |\n",
      "|    policy_gradient_loss | -0.00212    |\n",
      "|    value_loss           | 9.82e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=0.06 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0598      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 710000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008903364 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.78       |\n",
      "|    explained_variance   | 0.0142      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 3460        |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 7.86e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=0.07 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0746      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 720000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009831945 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.793      |\n",
      "|    explained_variance   | -0.0471     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 3510        |\n",
      "|    policy_gradient_loss | -0.0009     |\n",
      "|    value_loss           | 8.74e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.613   |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 1110     |\n",
      "|    total_timesteps | 720896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=0.07 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0691      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 730000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006455454 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.775      |\n",
      "|    explained_variance   | 0.0566      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00861    |\n",
      "|    n_updates            | 3560        |\n",
      "|    policy_gradient_loss | -0.00455    |\n",
      "|    value_loss           | 1.72e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.053       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 740000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011439421 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00286    |\n",
      "|    n_updates            | 3610        |\n",
      "|    policy_gradient_loss | -0.00716    |\n",
      "|    value_loss           | 2.13e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=0.07 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 750000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029028792 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.891      |\n",
      "|    explained_variance   | 0.00954     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0316      |\n",
      "|    n_updates            | 3660        |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    value_loss           | 4.74e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=-0.01 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | -0.00612  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 760000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0072345 |\n",
      "|    clip_fraction        | 0.0781    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.919    |\n",
      "|    explained_variance   | -0.0302   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0122    |\n",
      "|    n_updates            | 3710      |\n",
      "|    policy_gradient_loss | -0.00168  |\n",
      "|    value_loss           | 9.05e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=0.11 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.112       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 770000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008951635 |\n",
      "|    clip_fraction        | 0.094       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.674      |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00631    |\n",
      "|    n_updates            | 3750        |\n",
      "|    policy_gradient_loss | -0.000404   |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0195      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 780000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008081323 |\n",
      "|    clip_fraction        | 0.0821      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.842      |\n",
      "|    explained_variance   | 0.515       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0128     |\n",
      "|    n_updates            | 3800        |\n",
      "|    policy_gradient_loss | -0.00333    |\n",
      "|    value_loss           | 4.8e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=0.04 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0413     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 790000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01362177 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.797     |\n",
      "|    explained_variance   | -0.048     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0258     |\n",
      "|    n_updates            | 3850       |\n",
      "|    policy_gradient_loss | -0.00227   |\n",
      "|    value_loss           | 7.5e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=0.10 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0996       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 800000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048223324 |\n",
      "|    clip_fraction        | 0.0802       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.782       |\n",
      "|    explained_variance   | -0.101       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0159      |\n",
      "|    n_updates            | 3900         |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    value_loss           | 6.04e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=0.08 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0803      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 810000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007474767 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.765      |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 3950        |\n",
      "|    policy_gradient_loss | 0.00167     |\n",
      "|    value_loss           | 5.47e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.561   |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 396      |\n",
      "|    time_elapsed    | 1249     |\n",
      "|    total_timesteps | 811008   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.076       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 820000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005734476 |\n",
      "|    clip_fraction        | 0.0538      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.58       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00146     |\n",
      "|    n_updates            | 4000        |\n",
      "|    policy_gradient_loss | -0.000676   |\n",
      "|    value_loss           | 2.55e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=0.04 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0401      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 830000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022163028 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.474       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0529     |\n",
      "|    n_updates            | 4050        |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 2.24e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=0.00 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.00292      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 840000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074024284 |\n",
      "|    clip_fraction        | 0.0831       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.827       |\n",
      "|    explained_variance   | 0.000497     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0187      |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 5.29e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=0.07 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0718     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 850000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01118586 |\n",
      "|    clip_fraction        | 0.0826     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.581     |\n",
      "|    explained_variance   | 0.267      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0233    |\n",
      "|    n_updates            | 4150       |\n",
      "|    policy_gradient_loss | 0.000907   |\n",
      "|    value_loss           | 1.53e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=0.02 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 860000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008582497 |\n",
      "|    clip_fraction        | 0.0983      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 4190        |\n",
      "|    policy_gradient_loss | 0.000769    |\n",
      "|    value_loss           | 1.28e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=0.01 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.012        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 870000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051802583 |\n",
      "|    clip_fraction        | 0.0591       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.474       |\n",
      "|    explained_variance   | 0.414        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0175      |\n",
      "|    n_updates            | 4240         |\n",
      "|    policy_gradient_loss | -0.00321     |\n",
      "|    value_loss           | 3.46e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0321       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 880000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042937538 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.473       |\n",
      "|    explained_variance   | 0.13         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.017       |\n",
      "|    n_updates            | 4290         |\n",
      "|    policy_gradient_loss | -0.004       |\n",
      "|    value_loss           | 9.06e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00682     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 890000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008462623 |\n",
      "|    clip_fraction        | 0.0789      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.0766      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0149      |\n",
      "|    n_updates            | 4340        |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    value_loss           | 9.66e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=-0.01 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.00689     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 900000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041757375 |\n",
      "|    clip_fraction        | 0.0415       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.369       |\n",
      "|    explained_variance   | 0.144        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00841     |\n",
      "|    n_updates            | 4390         |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    value_loss           | 1.55e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.54    |\n",
      "| time/              |          |\n",
      "|    fps             | 649      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 1388     |\n",
      "|    total_timesteps | 901120   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=-0.05 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0509      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 910000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079296045 |\n",
      "|    clip_fraction        | 0.0861       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | 0.601        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.006        |\n",
      "|    n_updates            | 4440         |\n",
      "|    policy_gradient_loss | -0.00427     |\n",
      "|    value_loss           | 2.42e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=0.11 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.111       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 920000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017866734 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.66       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00536     |\n",
      "|    n_updates            | 4490        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 2.37e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=0.10 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0965       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 930000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021101679 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.251       |\n",
      "|    explained_variance   | 0.0854       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00454      |\n",
      "|    n_updates            | 4540         |\n",
      "|    policy_gradient_loss | 0.00156      |\n",
      "|    value_loss           | 1.13e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=0.13 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 940000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009890145 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.759      |\n",
      "|    explained_variance   | 0.0636      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000724   |\n",
      "|    n_updates            | 4580        |\n",
      "|    policy_gradient_loss | -0.000835   |\n",
      "|    value_loss           | 1.91e-05    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=950000, episode_reward=-0.03 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0278     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 950000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008866963 |\n",
      "|    clip_fraction        | 0.0986      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0309      |\n",
      "|    n_updates            | 4630        |\n",
      "|    policy_gradient_loss | 0.00321     |\n",
      "|    value_loss           | 1.74e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=0.07 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0703      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 960000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008044936 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.845      |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00598    |\n",
      "|    n_updates            | 4680        |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 5e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=0.04 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0401      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 970000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013362765 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.909      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00579    |\n",
      "|    n_updates            | 4730        |\n",
      "|    policy_gradient_loss | -0.00211    |\n",
      "|    value_loss           | 6.8e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0203     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 980000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015529836 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.765      |\n",
      "|    explained_variance   | 0.0802      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.025       |\n",
      "|    n_updates            | 4780        |\n",
      "|    policy_gradient_loss | -0.000521   |\n",
      "|    value_loss           | 1.54e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=0.04 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.041        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 990000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063095195 |\n",
      "|    clip_fraction        | 0.0638       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.696       |\n",
      "|    explained_variance   | 0.384        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0127      |\n",
      "|    n_updates            | 4830         |\n",
      "|    policy_gradient_loss | 0.000617     |\n",
      "|    value_loss           | 1.14e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.537   |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 484      |\n",
      "|    time_elapsed    | 1528     |\n",
      "|    total_timesteps | 991232   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00919     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009220544 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.0358      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 4880        |\n",
      "|    policy_gradient_loss | -0.00108    |\n",
      "|    value_loss           | 2.67e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0674      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1010000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007771196 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.675      |\n",
      "|    explained_variance   | -0.35       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0193     |\n",
      "|    n_updates            | 4930        |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 2.8e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0228       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1020000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031762747 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.699       |\n",
      "|    explained_variance   | 0.0492       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00243      |\n",
      "|    n_updates            | 4980         |\n",
      "|    policy_gradient_loss | 0.00079      |\n",
      "|    value_loss           | 3.79e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0459      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1030000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009697568 |\n",
      "|    clip_fraction        | 0.0904      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.699      |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00223    |\n",
      "|    n_updates            | 5020        |\n",
      "|    policy_gradient_loss | -0.00043    |\n",
      "|    value_loss           | 2.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.00558     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1040000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021206844 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.296       |\n",
      "|    explained_variance   | 0.1          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00873     |\n",
      "|    n_updates            | 5070         |\n",
      "|    policy_gradient_loss | -0.000403    |\n",
      "|    value_loss           | 1.57e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=-0.02 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0189     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017042633 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 5120        |\n",
      "|    policy_gradient_loss | 0.000351    |\n",
      "|    value_loss           | 6.34e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0481      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016481899 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00118     |\n",
      "|    n_updates            | 5170        |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    value_loss           | 6.25e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0478      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1070000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009343386 |\n",
      "|    clip_fraction        | 0.074       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.0485      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 5220        |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 3.61e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0969       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1080000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070889806 |\n",
      "|    clip_fraction        | 0.0842       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.489       |\n",
      "|    explained_variance   | 0.204        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00725      |\n",
      "|    n_updates            | 5270         |\n",
      "|    policy_gradient_loss | -0.000507    |\n",
      "|    value_loss           | 4.89e-06     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.525   |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 528      |\n",
      "|    time_elapsed    | 1667     |\n",
      "|    total_timesteps | 1081344  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=-0.00 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00121    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011137728 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.033      |\n",
      "|    n_updates            | 5320        |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 2.66e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=0.04 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0392      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027589122 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | -0.0867     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0559     |\n",
      "|    n_updates            | 5370        |\n",
      "|    policy_gradient_loss | 0.00307     |\n",
      "|    value_loss           | 3.85e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.036       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021282058 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0014      |\n",
      "|    n_updates            | 5410        |\n",
      "|    policy_gradient_loss | -8.42e-06   |\n",
      "|    value_loss           | 8.34e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0389       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1120000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062084184 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.639       |\n",
      "|    explained_variance   | 0.0349       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0077       |\n",
      "|    n_updates            | 5460         |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    value_loss           | 2.47e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=-0.00 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.00182     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1130000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040717474 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.152       |\n",
      "|    explained_variance   | 0.0207       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00489     |\n",
      "|    n_updates            | 5510         |\n",
      "|    policy_gradient_loss | 0.00169      |\n",
      "|    value_loss           | 3.69e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0431     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1140000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01086879 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.776     |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00929    |\n",
      "|    n_updates            | 5560       |\n",
      "|    policy_gradient_loss | -0.00388   |\n",
      "|    value_loss           | 6.57e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0549      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1150000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008231536 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00552    |\n",
      "|    n_updates            | 5610        |\n",
      "|    policy_gradient_loss | -0.00473    |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0155      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009036878 |\n",
      "|    clip_fraction        | 0.0841      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | 0.0615      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00991     |\n",
      "|    n_updates            | 5660        |\n",
      "|    policy_gradient_loss | 0.0014      |\n",
      "|    value_loss           | 8.14e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0981       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1170000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067854784 |\n",
      "|    clip_fraction        | 0.0609       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.328       |\n",
      "|    explained_variance   | 0.0944       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0197       |\n",
      "|    n_updates            | 5710         |\n",
      "|    policy_gradient_loss | 0.00357      |\n",
      "|    value_loss           | 3.58e-05     |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.507   |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 572      |\n",
      "|    time_elapsed    | 1806     |\n",
      "|    total_timesteps | 1171456  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=0.11 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.112        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1180000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069335555 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.682       |\n",
      "|    explained_variance   | 0.484        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0324      |\n",
      "|    n_updates            | 5760         |\n",
      "|    policy_gradient_loss | -0.00481     |\n",
      "|    value_loss           | 3.73e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00308     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1190000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009142827 |\n",
      "|    clip_fraction        | 0.0951      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.347      |\n",
      "|    explained_variance   | -0.29       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0215     |\n",
      "|    n_updates            | 5810        |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 3.09e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.023       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1200000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0084585305 |\n",
      "|    clip_fraction        | 0.0863       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.684       |\n",
      "|    explained_variance   | 0.324        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00976      |\n",
      "|    n_updates            | 5850         |\n",
      "|    policy_gradient_loss | -0.00313     |\n",
      "|    value_loss           | 1.42e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.00669     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1210000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060054255 |\n",
      "|    clip_fraction        | 0.0734       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.372       |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00301     |\n",
      "|    n_updates            | 5900         |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 2.59e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0212       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1220000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051752804 |\n",
      "|    clip_fraction        | 0.072        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.419       |\n",
      "|    explained_variance   | 0.0187       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0121      |\n",
      "|    n_updates            | 5950         |\n",
      "|    policy_gradient_loss | 0.00233      |\n",
      "|    value_loss           | 1.63e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0366     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1230000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010218395 |\n",
      "|    clip_fraction        | 0.093       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.374       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00744    |\n",
      "|    n_updates            | 6000        |\n",
      "|    policy_gradient_loss | -0.00328    |\n",
      "|    value_loss           | 2.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0761      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016759833 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0221     |\n",
      "|    n_updates            | 6050        |\n",
      "|    policy_gradient_loss | 0.00113     |\n",
      "|    value_loss           | 1.29e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=0.09 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0858      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009720011 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.511      |\n",
      "|    explained_variance   | 0.0291      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00561    |\n",
      "|    n_updates            | 6100        |\n",
      "|    policy_gradient_loss | 0.0029      |\n",
      "|    value_loss           | 1.5e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.128      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1260000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00701626 |\n",
      "|    clip_fraction        | 0.0777     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.369     |\n",
      "|    explained_variance   | 0.197      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00127   |\n",
      "|    n_updates            | 6150       |\n",
      "|    policy_gradient_loss | -7.17e-05  |\n",
      "|    value_loss           | 0.000117   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.497   |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 616      |\n",
      "|    time_elapsed    | 1945     |\n",
      "|    total_timesteps | 1261568  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.00677    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1270000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01128399 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.711     |\n",
      "|    explained_variance   | 0.189      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0137    |\n",
      "|    n_updates            | 6200       |\n",
      "|    policy_gradient_loss | -0.00733   |\n",
      "|    value_loss           | 2.82e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=0.04 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0421       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1280000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072334493 |\n",
      "|    clip_fraction        | 0.0947       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.652       |\n",
      "|    explained_variance   | 0.46         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.017       |\n",
      "|    n_updates            | 6240         |\n",
      "|    policy_gradient_loss | 0.000392     |\n",
      "|    value_loss           | 1.78e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=0.11 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.109       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1290000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018215397 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.812      |\n",
      "|    explained_variance   | 0.0727      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 6290        |\n",
      "|    policy_gradient_loss | 0.000909    |\n",
      "|    value_loss           | 1.59e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00189     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011071483 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00821    |\n",
      "|    n_updates            | 6340        |\n",
      "|    policy_gradient_loss | -0.000935   |\n",
      "|    value_loss           | 2.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0109     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1310000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004584349 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 6390        |\n",
      "|    policy_gradient_loss | 0.00278     |\n",
      "|    value_loss           | 6.32e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0269      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1320000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010001588 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.43        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0012      |\n",
      "|    n_updates            | 6440        |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 5.27e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0173      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1330000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009836163 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00512     |\n",
      "|    n_updates            | 6490        |\n",
      "|    policy_gradient_loss | -0.000911   |\n",
      "|    value_loss           | 8.07e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0683      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009677208 |\n",
      "|    clip_fraction        | 0.0887      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00482     |\n",
      "|    n_updates            | 6540        |\n",
      "|    policy_gradient_loss | 0.000318    |\n",
      "|    value_loss           | 5.35e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=-0.00 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00041    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008908047 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00733     |\n",
      "|    n_updates            | 6590        |\n",
      "|    policy_gradient_loss | 0.00396     |\n",
      "|    value_loss           | 1.31e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.482   |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 660      |\n",
      "|    time_elapsed    | 2085     |\n",
      "|    total_timesteps | 1351680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.00854      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1360000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0127155725 |\n",
      "|    clip_fraction        | 0.0984       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.438       |\n",
      "|    explained_variance   | 0.168        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00462     |\n",
      "|    n_updates            | 6640         |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 4.66e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.000776   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1370000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015591443 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.768      |\n",
      "|    explained_variance   | -0.000688   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00512     |\n",
      "|    n_updates            | 6680        |\n",
      "|    policy_gradient_loss | -0.00873    |\n",
      "|    value_loss           | 1.26e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=0.09 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0939       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1380000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072467485 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.716       |\n",
      "|    explained_variance   | 0.0752       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0147       |\n",
      "|    n_updates            | 6730         |\n",
      "|    policy_gradient_loss | -3.79e-05    |\n",
      "|    value_loss           | 9.89e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0403      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1390000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011399213 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.712      |\n",
      "|    explained_variance   | 0.0929      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 6780        |\n",
      "|    policy_gradient_loss | -0.00207    |\n",
      "|    value_loss           | 1.64e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0415       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1400000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042967396 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.228       |\n",
      "|    explained_variance   | 0.101        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0141      |\n",
      "|    n_updates            | 6830         |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 7.32e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0632      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1410000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011231972 |\n",
      "|    clip_fraction        | 0.0987      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 6880        |\n",
      "|    policy_gradient_loss | 0.000148    |\n",
      "|    value_loss           | 4.78e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0282      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009778766 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 6930        |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 5.89e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0425     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1430000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005352162 |\n",
      "|    clip_fraction        | 0.0761      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.347      |\n",
      "|    explained_variance   | 0.0758      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0148     |\n",
      "|    n_updates            | 6980        |\n",
      "|    policy_gradient_loss | 0.00236     |\n",
      "|    value_loss           | 1.73e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0288      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012033478 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 7030        |\n",
      "|    policy_gradient_loss | 0.00247     |\n",
      "|    value_loss           | 6.6e-06     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.446   |\n",
      "| time/              |          |\n",
      "|    fps             | 648      |\n",
      "|    iterations      | 704      |\n",
      "|    time_elapsed    | 2224     |\n",
      "|    total_timesteps | 1441792  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.124       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1450000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011757836 |\n",
      "|    clip_fraction        | 0.0995      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.0714      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 7080        |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 3.69e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.00344     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1460000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052890116 |\n",
      "|    clip_fraction        | 0.0829       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.186        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00513     |\n",
      "|    n_updates            | 7120         |\n",
      "|    policy_gradient_loss | -0.000522    |\n",
      "|    value_loss           | 1.74e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00956     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014029765 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.396      |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 7170        |\n",
      "|    policy_gradient_loss | 0.00678     |\n",
      "|    value_loss           | 3.64e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.101       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1480000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004922077 |\n",
      "|    clip_fraction        | 0.0644      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.298      |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00268    |\n",
      "|    n_updates            | 7220        |\n",
      "|    policy_gradient_loss | -0.000399   |\n",
      "|    value_loss           | 2.89e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=-0.02 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0192     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1490000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009357965 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 7270        |\n",
      "|    policy_gradient_loss | 0.00114     |\n",
      "|    value_loss           | 7.6e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0334     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008260673 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.237      |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0461      |\n",
      "|    n_updates            | 7320        |\n",
      "|    policy_gradient_loss | 0.00245     |\n",
      "|    value_loss           | 4.63e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0475      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1510000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020940034 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.388      |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0221      |\n",
      "|    n_updates            | 7370        |\n",
      "|    policy_gradient_loss | 0.0135      |\n",
      "|    value_loss           | 1.09e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0439      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010964848 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.4        |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 7420        |\n",
      "|    policy_gradient_loss | 0.00553     |\n",
      "|    value_loss           | 1.25e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=0.12 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.116       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1530000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009907959 |\n",
      "|    clip_fraction        | 0.0713      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.0517      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00887    |\n",
      "|    n_updates            | 7470        |\n",
      "|    policy_gradient_loss | 0.00256     |\n",
      "|    value_loss           | 4.13e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.441   |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 748      |\n",
      "|    time_elapsed    | 2364     |\n",
      "|    total_timesteps | 1531904  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0177     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1540000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009790521 |\n",
      "|    clip_fraction        | 0.0933      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 7510        |\n",
      "|    policy_gradient_loss | -0.0076     |\n",
      "|    value_loss           | 3.47e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0177      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1550000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007760782 |\n",
      "|    clip_fraction        | 0.0853      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.594       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 7560        |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    value_loss           | 1.59e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0224      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028477237 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.191      |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 7610        |\n",
      "|    policy_gradient_loss | 0.0221      |\n",
      "|    value_loss           | 9.11e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0812     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1570000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02983342 |\n",
      "|    clip_fraction        | 0.0696     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.259     |\n",
      "|    explained_variance   | 0.143      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 7660       |\n",
      "|    policy_gradient_loss | -0.00262   |\n",
      "|    value_loss           | 2.42e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0848      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003917451 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 7710        |\n",
      "|    policy_gradient_loss | 0.00256     |\n",
      "|    value_loss           | 7.94e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.103      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1590000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02106122 |\n",
      "|    clip_fraction        | 0.0954     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.435     |\n",
      "|    explained_variance   | 0.111      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 7760       |\n",
      "|    policy_gradient_loss | -0.00386   |\n",
      "|    value_loss           | 1.09e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=0.07 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0735     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1600000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00687576 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.553     |\n",
      "|    explained_variance   | 0.464      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 7810       |\n",
      "|    policy_gradient_loss | -0.00328   |\n",
      "|    value_loss           | 6.92e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0691       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1610000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074169086 |\n",
      "|    clip_fraction        | 0.0514       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.157        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0496       |\n",
      "|    n_updates            | 7860         |\n",
      "|    policy_gradient_loss | 0.00075      |\n",
      "|    value_loss           | 1.79e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00174     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041731916 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.345      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0057     |\n",
      "|    n_updates            | 7910        |\n",
      "|    policy_gradient_loss | -0.000197   |\n",
      "|    value_loss           | 9.71e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.442   |\n",
      "| time/              |          |\n",
      "|    fps             | 647      |\n",
      "|    iterations      | 792      |\n",
      "|    time_elapsed    | 2503     |\n",
      "|    total_timesteps | 1622016  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0467      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1630000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024104843 |\n",
      "|    clip_fraction        | 0.0801      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.368      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0419     |\n",
      "|    n_updates            | 7950        |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    value_loss           | 3.43e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0434      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024490558 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.465      |\n",
      "|    explained_variance   | 0.379       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 8000        |\n",
      "|    policy_gradient_loss | -0.00116    |\n",
      "|    value_loss           | 1.77e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0594      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1650000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008159849 |\n",
      "|    clip_fraction        | 0.0731      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.509      |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00521    |\n",
      "|    n_updates            | 8050        |\n",
      "|    policy_gradient_loss | 0.00178     |\n",
      "|    value_loss           | 9.54e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0515       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1660000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043934714 |\n",
      "|    clip_fraction        | 0.0575       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.302       |\n",
      "|    explained_variance   | 0.00292      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00365      |\n",
      "|    n_updates            | 8100         |\n",
      "|    policy_gradient_loss | 0.00233      |\n",
      "|    value_loss           | 1.29e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0127     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1670000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01533776 |\n",
      "|    clip_fraction        | 0.049      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.165     |\n",
      "|    explained_variance   | 0.459      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0089    |\n",
      "|    n_updates            | 8150       |\n",
      "|    policy_gradient_loss | -0.000838  |\n",
      "|    value_loss           | 9.06e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0626       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1680000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091885645 |\n",
      "|    clip_fraction        | 0.0924       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.476       |\n",
      "|    explained_variance   | -0.0209      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00139     |\n",
      "|    n_updates            | 8200         |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 5.89e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0519      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1690000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010988147 |\n",
      "|    clip_fraction        | 0.0841      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 8250        |\n",
      "|    policy_gradient_loss | -0.00308    |\n",
      "|    value_loss           | 8.03e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0189       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1700000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027279493 |\n",
      "|    clip_fraction        | 0.0329       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.199       |\n",
      "|    explained_variance   | 0.208        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00182     |\n",
      "|    n_updates            | 8300         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 2.49e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0115    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1710000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01340374 |\n",
      "|    clip_fraction        | 0.0865     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.552     |\n",
      "|    explained_variance   | 0.0676     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00102    |\n",
      "|    n_updates            | 8340       |\n",
      "|    policy_gradient_loss | -0.00215   |\n",
      "|    value_loss           | 5.37e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.421      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 836         |\n",
      "|    time_elapsed         | 2643        |\n",
      "|    total_timesteps      | 1712128     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014472421 |\n",
      "|    clip_fraction        | 0.035       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.453      |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 8350        |\n",
      "|    policy_gradient_loss | 0.00304     |\n",
      "|    value_loss           | 1.07e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0431      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1720000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030029736 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.457      |\n",
      "|    explained_variance   | 0.0936      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0261     |\n",
      "|    n_updates            | 8390        |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    value_loss           | 2.85e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1730000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006825613 |\n",
      "|    clip_fraction        | 0.0802      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.378      |\n",
      "|    explained_variance   | 0.227       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 8440        |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 1.38e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0354      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1740000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015689513 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00877    |\n",
      "|    n_updates            | 8490        |\n",
      "|    policy_gradient_loss | 0.00413     |\n",
      "|    value_loss           | 9.33e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0557     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1750000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01312094 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.573     |\n",
      "|    explained_variance   | -0.0768    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00939   |\n",
      "|    n_updates            | 8540       |\n",
      "|    policy_gradient_loss | -0.00178   |\n",
      "|    value_loss           | 1.2e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0789      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007099882 |\n",
      "|    clip_fraction        | 0.0659      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.418      |\n",
      "|    explained_variance   | 0.331       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000929   |\n",
      "|    n_updates            | 8590        |\n",
      "|    policy_gradient_loss | 0.00138     |\n",
      "|    value_loss           | 8.18e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0194      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1770000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007373198 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0145      |\n",
      "|    n_updates            | 8640        |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    value_loss           | 5.17e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0821      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007851397 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.706       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0182      |\n",
      "|    n_updates            | 8690        |\n",
      "|    policy_gradient_loss | 0.000762    |\n",
      "|    value_loss           | 5.6e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0307      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1790000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009663368 |\n",
      "|    clip_fraction        | 0.0818      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.121       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00922    |\n",
      "|    n_updates            | 8740        |\n",
      "|    policy_gradient_loss | 0.000887    |\n",
      "|    value_loss           | 1.56e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00425     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1800000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008213937 |\n",
      "|    clip_fraction        | 0.0782      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.0545      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00147    |\n",
      "|    n_updates            | 8780        |\n",
      "|    policy_gradient_loss | 0.00108     |\n",
      "|    value_loss           | 6.9e-06     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9e+04        |\n",
      "|    ep_rew_mean          | -0.407       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 647          |\n",
      "|    iterations           | 880          |\n",
      "|    time_elapsed         | 2782         |\n",
      "|    total_timesteps      | 1802240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051038302 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.393       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00609     |\n",
      "|    n_updates            | 8790         |\n",
      "|    policy_gradient_loss | 0.000282     |\n",
      "|    value_loss           | 1.25e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0305      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011503942 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.38       |\n",
      "|    explained_variance   | -0.0696     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 8830        |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    value_loss           | 2.66e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0509      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1820000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010909205 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00936    |\n",
      "|    n_updates            | 8880        |\n",
      "|    policy_gradient_loss | -0.00447    |\n",
      "|    value_loss           | 2.35e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.053        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1830000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065688286 |\n",
      "|    clip_fraction        | 0.0993       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.633       |\n",
      "|    explained_variance   | -0.37        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00416     |\n",
      "|    n_updates            | 8930         |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 7.6e-06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=0.09 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0852      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007674369 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.0907      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 8980        |\n",
      "|    policy_gradient_loss | -0.000979   |\n",
      "|    value_loss           | 4.25e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0138     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1850000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01235622 |\n",
      "|    clip_fraction        | 0.0975     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.261     |\n",
      "|    explained_variance   | 0.0623     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0188    |\n",
      "|    n_updates            | 9030       |\n",
      "|    policy_gradient_loss | -0.00364   |\n",
      "|    value_loss           | 1.01e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0586       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1860000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070113316 |\n",
      "|    clip_fraction        | 0.0775       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.24         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0197      |\n",
      "|    n_updates            | 9080         |\n",
      "|    policy_gradient_loss | -0.000879    |\n",
      "|    value_loss           | 1.69e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0354      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1870000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018401444 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.491       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0295      |\n",
      "|    n_updates            | 9130        |\n",
      "|    policy_gradient_loss | -0.00345    |\n",
      "|    value_loss           | 5e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00319     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007069164 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.308      |\n",
      "|    explained_variance   | 0.0302      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0642      |\n",
      "|    n_updates            | 9170        |\n",
      "|    policy_gradient_loss | 0.00299     |\n",
      "|    value_loss           | 6.11e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0425      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004425295 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 9220        |\n",
      "|    policy_gradient_loss | 0.00123     |\n",
      "|    value_loss           | 6.82e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9e+04        |\n",
      "|    ep_rew_mean          | -0.405       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 647          |\n",
      "|    iterations           | 924          |\n",
      "|    time_elapsed         | 2922         |\n",
      "|    total_timesteps      | 1892352      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0093121575 |\n",
      "|    clip_fraction        | 0.077        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.401       |\n",
      "|    explained_variance   | 0.185        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0346      |\n",
      "|    n_updates            | 9230         |\n",
      "|    policy_gradient_loss | 0.00377      |\n",
      "|    value_loss           | 2.65e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0545      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007321672 |\n",
      "|    clip_fraction        | 0.0762      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0.0633      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00439    |\n",
      "|    n_updates            | 9270        |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    value_loss           | 2.1e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0163     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013976948 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.261       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00134     |\n",
      "|    n_updates            | 9320        |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 1.37e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0082     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1920000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01002084 |\n",
      "|    clip_fraction        | 0.054      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.317     |\n",
      "|    explained_variance   | 0.212      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0103    |\n",
      "|    n_updates            | 9370       |\n",
      "|    policy_gradient_loss | 0.000424   |\n",
      "|    value_loss           | 1.13e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0298       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1930000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074318573 |\n",
      "|    clip_fraction        | 0.0675       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.293       |\n",
      "|    explained_variance   | 0.0748       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00596      |\n",
      "|    n_updates            | 9420         |\n",
      "|    policy_gradient_loss | -0.000406    |\n",
      "|    value_loss           | 1.61e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0186      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1940000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014878701 |\n",
      "|    clip_fraction        | 0.065       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.192      |\n",
      "|    explained_variance   | 0.612       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00116     |\n",
      "|    n_updates            | 9470        |\n",
      "|    policy_gradient_loss | 0.000931    |\n",
      "|    value_loss           | 1.02e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=0.09 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0905      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1950000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008040334 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.0279      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0065     |\n",
      "|    n_updates            | 9520        |\n",
      "|    policy_gradient_loss | 0.00299     |\n",
      "|    value_loss           | 6.01e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0125       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1960000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067101447 |\n",
      "|    clip_fraction        | 0.0703       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0111      |\n",
      "|    n_updates            | 9570         |\n",
      "|    policy_gradient_loss | 0.000262     |\n",
      "|    value_loss           | 5.11e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0542      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1970000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024953581 |\n",
      "|    clip_fraction        | 0.0694      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.276      |\n",
      "|    explained_variance   | -0.00901    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0118      |\n",
      "|    n_updates            | 9610        |\n",
      "|    policy_gradient_loss | 0.0297      |\n",
      "|    value_loss           | 1.48e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=0.08 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.079       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1980000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013355885 |\n",
      "|    clip_fraction        | 0.0692      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.491       |\n",
      "|    n_updates            | 9660        |\n",
      "|    policy_gradient_loss | 0.00706     |\n",
      "|    value_loss           | 7.26e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.39       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 968         |\n",
      "|    time_elapsed         | 3062        |\n",
      "|    total_timesteps      | 1982464     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029300096 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0363     |\n",
      "|    n_updates            | 9670        |\n",
      "|    policy_gradient_loss | 0.00941     |\n",
      "|    value_loss           | 1.22e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0583       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 1990000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0098984605 |\n",
      "|    clip_fraction        | 0.104        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.62        |\n",
      "|    explained_variance   | 0.249        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0191       |\n",
      "|    n_updates            | 9710         |\n",
      "|    policy_gradient_loss | 0.00217      |\n",
      "|    value_loss           | 3.3e-05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0364      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017289735 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00802     |\n",
      "|    n_updates            | 9760        |\n",
      "|    policy_gradient_loss | -0.00647    |\n",
      "|    value_loss           | 1.75e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0479     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2010000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00799006 |\n",
      "|    clip_fraction        | 0.0828     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.705     |\n",
      "|    explained_variance   | 0.172      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00598   |\n",
      "|    n_updates            | 9810       |\n",
      "|    policy_gradient_loss | 0.0072     |\n",
      "|    value_loss           | 9.39e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0468      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008343899 |\n",
      "|    clip_fraction        | 0.07        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 9860        |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 1.13e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=0.11 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.107       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2030000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009839486 |\n",
      "|    clip_fraction        | 0.0656      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.468      |\n",
      "|    explained_variance   | 0.0347      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00181    |\n",
      "|    n_updates            | 9910        |\n",
      "|    policy_gradient_loss | -0.000151   |\n",
      "|    value_loss           | 5.24e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0.12      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2040000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0448942 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.711    |\n",
      "|    explained_variance   | 0.106     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0377   |\n",
      "|    n_updates            | 9960      |\n",
      "|    policy_gradient_loss | -0.00514  |\n",
      "|    value_loss           | 6.08e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=0.09 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0908      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008675933 |\n",
      "|    clip_fraction        | 0.0691      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.481      |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0248      |\n",
      "|    n_updates            | 10000       |\n",
      "|    policy_gradient_loss | -0.00403    |\n",
      "|    value_loss           | 2.26e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.073       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012922196 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 0.0399      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00879     |\n",
      "|    n_updates            | 10050       |\n",
      "|    policy_gradient_loss | -0.00659    |\n",
      "|    value_loss           | 6.36e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0319      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2070000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024446372 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.438      |\n",
      "|    explained_variance   | 0.126       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00926    |\n",
      "|    n_updates            | 10100       |\n",
      "|    policy_gradient_loss | 0.00702     |\n",
      "|    value_loss           | 7.47e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.401      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1012        |\n",
      "|    time_elapsed         | 3201        |\n",
      "|    total_timesteps      | 2072576     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010635885 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.0542      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000221   |\n",
      "|    n_updates            | 10110       |\n",
      "|    policy_gradient_loss | 0.0021      |\n",
      "|    value_loss           | 1.43e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0464      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014542845 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.626      |\n",
      "|    explained_variance   | 0.0239      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00943    |\n",
      "|    n_updates            | 10150       |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    value_loss           | 2.45e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0586      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009483345 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0305      |\n",
      "|    n_updates            | 10200       |\n",
      "|    policy_gradient_loss | -0.00131    |\n",
      "|    value_loss           | 1.83e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0625      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009328456 |\n",
      "|    clip_fraction        | 0.0901      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.818      |\n",
      "|    explained_variance   | 0.0338      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00657     |\n",
      "|    n_updates            | 10250       |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    value_loss           | 1.05e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0802      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009464488 |\n",
      "|    clip_fraction        | 0.0793      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.699      |\n",
      "|    explained_variance   | -0.0264     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00256     |\n",
      "|    n_updates            | 10300       |\n",
      "|    policy_gradient_loss | -0.0026     |\n",
      "|    value_loss           | 1.54e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0805      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015208207 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | -0.115      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 10350       |\n",
      "|    policy_gradient_loss | -0.00741    |\n",
      "|    value_loss           | 5.73e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0383      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015487244 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.699      |\n",
      "|    explained_variance   | -0.0218     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0666      |\n",
      "|    n_updates            | 10400       |\n",
      "|    policy_gradient_loss | -8.64e-05   |\n",
      "|    value_loss           | 6.74e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0517       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2140000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077578183 |\n",
      "|    clip_fraction        | 0.0678       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.592       |\n",
      "|    explained_variance   | 0.0792       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0155      |\n",
      "|    n_updates            | 10440        |\n",
      "|    policy_gradient_loss | -0.00129     |\n",
      "|    value_loss           | 1.19e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0628      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2150000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010909738 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.23        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000278    |\n",
      "|    n_updates            | 10490       |\n",
      "|    policy_gradient_loss | -0.00057    |\n",
      "|    value_loss           | 6.46e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00954     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014899055 |\n",
      "|    clip_fraction        | 0.0982      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0143     |\n",
      "|    n_updates            | 10540       |\n",
      "|    policy_gradient_loss | 0.00704     |\n",
      "|    value_loss           | 9e-06       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.405      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1056        |\n",
      "|    time_elapsed         | 3340        |\n",
      "|    total_timesteps      | 2162688     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008640387 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.856      |\n",
      "|    explained_variance   | -0.00517    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0192     |\n",
      "|    n_updates            | 10550       |\n",
      "|    policy_gradient_loss | 0.00106     |\n",
      "|    value_loss           | 2.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0316      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009795435 |\n",
      "|    clip_fraction        | 0.0888      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00279    |\n",
      "|    n_updates            | 10590       |\n",
      "|    policy_gradient_loss | -0.00286    |\n",
      "|    value_loss           | 1.76e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=0.09 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0878      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011210276 |\n",
      "|    clip_fraction        | 0.0869      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0426      |\n",
      "|    n_updates            | 10640       |\n",
      "|    policy_gradient_loss | -0.00219    |\n",
      "|    value_loss           | 1.81e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0602      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2190000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010988038 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.884      |\n",
      "|    explained_variance   | 0.013       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00845     |\n",
      "|    n_updates            | 10690       |\n",
      "|    policy_gradient_loss | 0.000441    |\n",
      "|    value_loss           | 7.03e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=0.08 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0761       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2200000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0116376355 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.723       |\n",
      "|    explained_variance   | 0.238        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0176       |\n",
      "|    n_updates            | 10740        |\n",
      "|    policy_gradient_loss | -0.00565     |\n",
      "|    value_loss           | 1.43e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=0.09 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0878      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2210000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005635972 |\n",
      "|    clip_fraction        | 0.0751      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00771    |\n",
      "|    n_updates            | 10790       |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 6.3e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0251       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2220000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058756275 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.274       |\n",
      "|    explained_variance   | 0.0527       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0058      |\n",
      "|    n_updates            | 10830        |\n",
      "|    policy_gradient_loss | 0.00262      |\n",
      "|    value_loss           | 2.69e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0649      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2230000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015724143 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 10880       |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 1.62e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0343     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2240000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01746612 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.5       |\n",
      "|    explained_variance   | 0.117      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 10930      |\n",
      "|    policy_gradient_loss | 0.00244    |\n",
      "|    value_loss           | 7.45e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0321      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007897726 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.652      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 10980       |\n",
      "|    policy_gradient_loss | -0.00101    |\n",
      "|    value_loss           | 8.37e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.383     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1100       |\n",
      "|    time_elapsed         | 3480       |\n",
      "|    total_timesteps      | 2252800    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00870897 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.801     |\n",
      "|    explained_variance   | -0.0902    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00567   |\n",
      "|    n_updates            | 10990      |\n",
      "|    policy_gradient_loss | 0.00249    |\n",
      "|    value_loss           | 1.81e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0644      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024612417 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 11030       |\n",
      "|    policy_gradient_loss | -0.00805    |\n",
      "|    value_loss           | 2.97e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0468      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2270000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016247459 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0591     |\n",
      "|    n_updates            | 11080       |\n",
      "|    policy_gradient_loss | -0.018      |\n",
      "|    value_loss           | 1.89e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0299      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014255637 |\n",
      "|    clip_fraction        | 0.0808      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.905      |\n",
      "|    explained_variance   | 0.00545     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000499    |\n",
      "|    n_updates            | 11130       |\n",
      "|    policy_gradient_loss | 0.00402     |\n",
      "|    value_loss           | 5.8e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.07        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2290000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008132346 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.132       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 11180       |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    value_loss           | 2.71e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0368      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007913861 |\n",
      "|    clip_fraction        | 0.0972      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.478      |\n",
      "|    explained_variance   | 0.254       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 11230       |\n",
      "|    policy_gradient_loss | 0.00382     |\n",
      "|    value_loss           | 7.13e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0618      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2310000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011091215 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000537   |\n",
      "|    n_updates            | 11270       |\n",
      "|    policy_gradient_loss | -0.00196    |\n",
      "|    value_loss           | 9.2e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0159      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2320000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011961618 |\n",
      "|    clip_fraction        | 0.0991      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 11320       |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 1.25e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0.0341    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2330000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0129562 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.428    |\n",
      "|    explained_variance   | 0.186     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00346  |\n",
      "|    n_updates            | 11370     |\n",
      "|    policy_gradient_loss | -0.000462 |\n",
      "|    value_loss           | 9.81e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0.0429    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2340000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0071553 |\n",
      "|    clip_fraction        | 0.065     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.199    |\n",
      "|    explained_variance   | 0.122     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0106    |\n",
      "|    n_updates            | 11420     |\n",
      "|    policy_gradient_loss | 0.000839  |\n",
      "|    value_loss           | 8.57e-06  |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.365      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1144        |\n",
      "|    time_elapsed         | 3619        |\n",
      "|    total_timesteps      | 2342912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012100251 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0097     |\n",
      "|    n_updates            | 11430       |\n",
      "|    policy_gradient_loss | 0.00287     |\n",
      "|    value_loss           | 1.95e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0393      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013360718 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0784      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00174    |\n",
      "|    n_updates            | 11470       |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    value_loss           | 3.68e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00264     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013206601 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.0306      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00688    |\n",
      "|    n_updates            | 11520       |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    value_loss           | 1.68e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0186      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2370000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010085849 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.916      |\n",
      "|    explained_variance   | -0.00727    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0106      |\n",
      "|    n_updates            | 11570       |\n",
      "|    policy_gradient_loss | 0.00231     |\n",
      "|    value_loss           | 8.56e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0817      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007716557 |\n",
      "|    clip_fraction        | 0.0949      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.0179      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00948    |\n",
      "|    n_updates            | 11620       |\n",
      "|    policy_gradient_loss | -0.00031    |\n",
      "|    value_loss           | 1.19e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0511     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2390000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03578276 |\n",
      "|    clip_fraction        | 0.0634     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.26      |\n",
      "|    explained_variance   | 0.0893     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0242     |\n",
      "|    n_updates            | 11660      |\n",
      "|    policy_gradient_loss | 0.0126     |\n",
      "|    value_loss           | 2.68e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0591     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2400000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01176131 |\n",
      "|    clip_fraction        | 0.0546     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.391     |\n",
      "|    explained_variance   | 0.217      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0126    |\n",
      "|    n_updates            | 11710      |\n",
      "|    policy_gradient_loss | 0.00129    |\n",
      "|    value_loss           | 2.28e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0523      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2410000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015208846 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.024       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00158    |\n",
      "|    n_updates            | 11760       |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    value_loss           | 6.86e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0116     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2420000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01413393 |\n",
      "|    clip_fraction        | 0.0995     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.414     |\n",
      "|    explained_variance   | 0.184      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00573   |\n",
      "|    n_updates            | 11810      |\n",
      "|    policy_gradient_loss | 0.000148   |\n",
      "|    value_loss           | 9.87e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0383       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2430000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047903163 |\n",
      "|    clip_fraction        | 0.046        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.25        |\n",
      "|    explained_variance   | 0.0858       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0188      |\n",
      "|    n_updates            | 11860        |\n",
      "|    policy_gradient_loss | 0.00159      |\n",
      "|    value_loss           | 8.09e-06     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.373     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1188       |\n",
      "|    time_elapsed         | 3757       |\n",
      "|    total_timesteps      | 2433024    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04896138 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.591     |\n",
      "|    explained_variance   | 0.865      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 11870      |\n",
      "|    policy_gradient_loss | 0.0267     |\n",
      "|    value_loss           | 2.06e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.00731      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2440000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0139563065 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.658       |\n",
      "|    explained_variance   | 0.401        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00636     |\n",
      "|    n_updates            | 11910        |\n",
      "|    policy_gradient_loss | -0.00769     |\n",
      "|    value_loss           | 3.02e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0292      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2450000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017868306 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.558       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 11960       |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 1.38e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.000194     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2460000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048287814 |\n",
      "|    clip_fraction        | 0.0765       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.697       |\n",
      "|    explained_variance   | -0.0647      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00334      |\n",
      "|    n_updates            | 12010        |\n",
      "|    policy_gradient_loss | 0.00017      |\n",
      "|    value_loss           | 1.97e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.000104   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010219164 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.353      |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000696    |\n",
      "|    n_updates            | 12060       |\n",
      "|    policy_gradient_loss | -0.00227    |\n",
      "|    value_loss           | 1.53e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0065       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2480000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023613032 |\n",
      "|    clip_fraction        | 0.02         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.147       |\n",
      "|    explained_variance   | 0.0723       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00251     |\n",
      "|    n_updates            | 12100        |\n",
      "|    policy_gradient_loss | 0.000461     |\n",
      "|    value_loss           | 1.46e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=0.12 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.121       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2490000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014129102 |\n",
      "|    clip_fraction        | 0.0713      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00388     |\n",
      "|    n_updates            | 12150       |\n",
      "|    policy_gradient_loss | 8.64e-05    |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0184      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015539366 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0306     |\n",
      "|    n_updates            | 12200       |\n",
      "|    policy_gradient_loss | -0.00346    |\n",
      "|    value_loss           | 8.98e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0772       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2510000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078027286 |\n",
      "|    clip_fraction        | 0.0801       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.356       |\n",
      "|    explained_variance   | 0.0361       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00127      |\n",
      "|    n_updates            | 12250        |\n",
      "|    policy_gradient_loss | -0.000338    |\n",
      "|    value_loss           | 1.3e-05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0145       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2520000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052568084 |\n",
      "|    clip_fraction        | 0.0576       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.432       |\n",
      "|    explained_variance   | 0.0653       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00212     |\n",
      "|    n_updates            | 12300        |\n",
      "|    policy_gradient_loss | -5.33e-05    |\n",
      "|    value_loss           | 1.35e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.357      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1232        |\n",
      "|    time_elapsed         | 3896        |\n",
      "|    total_timesteps      | 2523136     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014357289 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.789      |\n",
      "|    explained_variance   | 0.794       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0188      |\n",
      "|    n_updates            | 12310       |\n",
      "|    policy_gradient_loss | -0.000346   |\n",
      "|    value_loss           | 2.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.023       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2530000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017366031 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.746      |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00505    |\n",
      "|    n_updates            | 12350       |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    value_loss           | 3.49e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0117     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2540000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01246843 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.592     |\n",
      "|    explained_variance   | 0.556      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0173     |\n",
      "|    n_updates            | 12400      |\n",
      "|    policy_gradient_loss | -0.00882   |\n",
      "|    value_loss           | 1.88e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0388      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2550000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011641472 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.987      |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00141     |\n",
      "|    n_updates            | 12450       |\n",
      "|    policy_gradient_loss | 0.000141    |\n",
      "|    value_loss           | 7.75e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0199      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2560000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085002575 |\n",
      "|    clip_fraction        | 0.0737       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.324       |\n",
      "|    explained_variance   | 0.121        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0128       |\n",
      "|    n_updates            | 12490        |\n",
      "|    policy_gradient_loss | 0.00139      |\n",
      "|    value_loss           | 3.48e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0284       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2570000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062854523 |\n",
      "|    clip_fraction        | 0.0861       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.531       |\n",
      "|    explained_variance   | 0.241        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0291      |\n",
      "|    n_updates            | 12540        |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 9.57e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.104       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008962265 |\n",
      "|    clip_fraction        | 0.0812      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.387      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0129     |\n",
      "|    n_updates            | 12590       |\n",
      "|    policy_gradient_loss | -0.00122    |\n",
      "|    value_loss           | 8.47e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0656     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2590000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02348211 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.405     |\n",
      "|    explained_variance   | 0.104      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0383    |\n",
      "|    n_updates            | 12640      |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    value_loss           | 9.06e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0809     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2600000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01759078 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.407     |\n",
      "|    explained_variance   | 0.0717     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0144    |\n",
      "|    n_updates            | 12690      |\n",
      "|    policy_gradient_loss | 0.00201    |\n",
      "|    value_loss           | 1.29e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0326       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2610000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022969134 |\n",
      "|    clip_fraction        | 0.0208       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.16        |\n",
      "|    explained_variance   | 0.0665       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0129      |\n",
      "|    n_updates            | 12740        |\n",
      "|    policy_gradient_loss | 0.00109      |\n",
      "|    value_loss           | 3.87e-06     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.363     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 647        |\n",
      "|    iterations           | 1276       |\n",
      "|    time_elapsed         | 4035       |\n",
      "|    total_timesteps      | 2613248    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01068937 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.713     |\n",
      "|    explained_variance   | 0.417      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0107    |\n",
      "|    n_updates            | 12750      |\n",
      "|    policy_gradient_loss | 0.000985   |\n",
      "|    value_loss           | 2.35e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=0.08 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0831      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013270316 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.864      |\n",
      "|    explained_variance   | 0.0856      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 12790       |\n",
      "|    policy_gradient_loss | -0.0069     |\n",
      "|    value_loss           | 1.99e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0963     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2630000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01582927 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.685     |\n",
      "|    explained_variance   | 0.149      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0387    |\n",
      "|    n_updates            | 12840      |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    value_loss           | 1.86e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0263      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008455908 |\n",
      "|    clip_fraction        | 0.0749      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.999      |\n",
      "|    explained_variance   | 0.0107      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 12890       |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    value_loss           | 5.36e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0764       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2650000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078941025 |\n",
      "|    clip_fraction        | 0.0681       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.179       |\n",
      "|    explained_variance   | 0.0758       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0136       |\n",
      "|    n_updates            | 12930        |\n",
      "|    policy_gradient_loss | -0.0039      |\n",
      "|    value_loss           | 2.56e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0316      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2660000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010521829 |\n",
      "|    clip_fraction        | 0.0689      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.0549      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00483    |\n",
      "|    n_updates            | 12980       |\n",
      "|    policy_gradient_loss | 0.00461     |\n",
      "|    value_loss           | 3.14e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0207    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2670000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00928899 |\n",
      "|    clip_fraction        | 0.0792     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.634     |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0047    |\n",
      "|    n_updates            | 13030      |\n",
      "|    policy_gradient_loss | 0.000322   |\n",
      "|    value_loss           | 6.76e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0079     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2680000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01777532 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.444     |\n",
      "|    explained_variance   | 0.333      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0047     |\n",
      "|    n_updates            | 13080      |\n",
      "|    policy_gradient_loss | -0.00919   |\n",
      "|    value_loss           | 9.34e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0546     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2690000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03772401 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.434     |\n",
      "|    explained_variance   | 0.114      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0391    |\n",
      "|    n_updates            | 13130      |\n",
      "|    policy_gradient_loss | 0.0402     |\n",
      "|    value_loss           | 1.14e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0336       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2700000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050708344 |\n",
      "|    clip_fraction        | 0.0322       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.175       |\n",
      "|    explained_variance   | 0.305        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00276      |\n",
      "|    n_updates            | 13180        |\n",
      "|    policy_gradient_loss | 0.00197      |\n",
      "|    value_loss           | 1.56e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.365      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1320        |\n",
      "|    time_elapsed         | 4173        |\n",
      "|    total_timesteps      | 2703360     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020117834 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.766      |\n",
      "|    explained_variance   | -0.309      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.032      |\n",
      "|    n_updates            | 13190       |\n",
      "|    policy_gradient_loss | 0.000317    |\n",
      "|    value_loss           | 2.27e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0607       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2710000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085677225 |\n",
      "|    clip_fraction        | 0.113        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.873       |\n",
      "|    explained_variance   | 0.28         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00654      |\n",
      "|    n_updates            | 13230        |\n",
      "|    policy_gradient_loss | -0.00626     |\n",
      "|    value_loss           | 2.8e-05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0376      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2720000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015455611 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | -0.161      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0381     |\n",
      "|    n_updates            | 13280       |\n",
      "|    policy_gradient_loss | -0.0127     |\n",
      "|    value_loss           | 1.95e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0281      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2730000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012071344 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.875      |\n",
      "|    explained_variance   | 0.0886      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00536    |\n",
      "|    n_updates            | 13330       |\n",
      "|    policy_gradient_loss | -0.00402    |\n",
      "|    value_loss           | 9.7e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0472      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2740000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008677481 |\n",
      "|    clip_fraction        | 0.0696      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00606    |\n",
      "|    n_updates            | 13370       |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    value_loss           | 2.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2750000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0401      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2750000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025835935 |\n",
      "|    clip_fraction        | 0.0727      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.415      |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0329     |\n",
      "|    n_updates            | 13420       |\n",
      "|    policy_gradient_loss | 0.00068     |\n",
      "|    value_loss           | 2.37e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0221       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2760000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0130845755 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.646       |\n",
      "|    explained_variance   | 0.0247       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00491     |\n",
      "|    n_updates            | 13470        |\n",
      "|    policy_gradient_loss | -0.00583     |\n",
      "|    value_loss           | 9.5e-06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0146     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2770000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020153794 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.485      |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00644    |\n",
      "|    n_updates            | 13520       |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 1.09e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0162      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039789185 |\n",
      "|    clip_fraction        | 0.0844      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.384      |\n",
      "|    explained_variance   | 0.0649      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 13570       |\n",
      "|    policy_gradient_loss | 0.0355      |\n",
      "|    value_loss           | 6.84e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0195       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2790000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063085165 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.183        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0121       |\n",
      "|    n_updates            | 13620        |\n",
      "|    policy_gradient_loss | 0.00274      |\n",
      "|    value_loss           | 7.38e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.371      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1364        |\n",
      "|    time_elapsed         | 4311        |\n",
      "|    total_timesteps      | 2793472     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029701531 |\n",
      "|    clip_fraction        | 0.0839      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.436      |\n",
      "|    explained_variance   | 0.202       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0206     |\n",
      "|    n_updates            | 13630       |\n",
      "|    policy_gradient_loss | 0.0444      |\n",
      "|    value_loss           | 2.68e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0214       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2800000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072540557 |\n",
      "|    clip_fraction        | 0.073        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | 0.0477       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000565     |\n",
      "|    n_updates            | 13670        |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    value_loss           | 3.4e-05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00281    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018514698 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | -0.325      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00408    |\n",
      "|    n_updates            | 13720       |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 3.42e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2820000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0348      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2820000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006064131 |\n",
      "|    clip_fraction        | 0.067       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.427      |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0237      |\n",
      "|    n_updates            | 13760       |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    value_loss           | 1.61e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2830000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.016       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2830000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017626122 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | 0.0663      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 13810       |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    value_loss           | 3.13e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00617     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017411694 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0122     |\n",
      "|    n_updates            | 13860       |\n",
      "|    policy_gradient_loss | -0.00081    |\n",
      "|    value_loss           | 8.39e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0042      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2850000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007956884 |\n",
      "|    clip_fraction        | 0.0634      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0117      |\n",
      "|    n_updates            | 13910       |\n",
      "|    policy_gradient_loss | -0.00065    |\n",
      "|    value_loss           | 6.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0406     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2860000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026239496 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.0218      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0176     |\n",
      "|    n_updates            | 13960       |\n",
      "|    policy_gradient_loss | -0.00927    |\n",
      "|    value_loss           | 7.26e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00382    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2870000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029630626 |\n",
      "|    clip_fraction        | 0.0893      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.393      |\n",
      "|    explained_variance   | 0.0915      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00586    |\n",
      "|    n_updates            | 14010       |\n",
      "|    policy_gradient_loss | 0.00153     |\n",
      "|    value_loss           | 1.33e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00731     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034590438 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 14060       |\n",
      "|    policy_gradient_loss | 0.0179      |\n",
      "|    value_loss           | 1.66e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.373      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1408        |\n",
      "|    time_elapsed         | 4450        |\n",
      "|    total_timesteps      | 2883584     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015506183 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0106      |\n",
      "|    n_updates            | 14070       |\n",
      "|    policy_gradient_loss | 0.00621     |\n",
      "|    value_loss           | 2.66e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0215      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006539532 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00417    |\n",
      "|    n_updates            | 14110       |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    value_loss           | 4.86e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00256    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018065264 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.48        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0357     |\n",
      "|    n_updates            | 14160       |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 2.69e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0149      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010258799 |\n",
      "|    clip_fraction        | 0.0864      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.836      |\n",
      "|    explained_variance   | 0.0811      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 14200       |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    value_loss           | 1.01e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0326      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2920000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024999369 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | -0.0183     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0183      |\n",
      "|    n_updates            | 14250       |\n",
      "|    policy_gradient_loss | 0.00363     |\n",
      "|    value_loss           | 2.79e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0045    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2930000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02530416 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.713     |\n",
      "|    explained_variance   | 0.0121     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0898     |\n",
      "|    n_updates            | 14300      |\n",
      "|    policy_gradient_loss | 0.005      |\n",
      "|    value_loss           | 1.2e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00594     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2940000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010160752 |\n",
      "|    clip_fraction        | 0.0972      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00325     |\n",
      "|    n_updates            | 14350       |\n",
      "|    policy_gradient_loss | 4.08e-05    |\n",
      "|    value_loss           | 6.46e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00522    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2950000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017006837 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | -0.212      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0163     |\n",
      "|    n_updates            | 14400       |\n",
      "|    policy_gradient_loss | -0.00553    |\n",
      "|    value_loss           | 1.03e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2960000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00736    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2960000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018243995 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.491      |\n",
      "|    explained_variance   | 0.0976      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 14450       |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 3.98e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.033      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2970000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00908004 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.685     |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.000502  |\n",
      "|    n_updates            | 14500      |\n",
      "|    policy_gradient_loss | 0.000457   |\n",
      "|    value_loss           | 5.93e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1452        |\n",
      "|    time_elapsed         | 4588        |\n",
      "|    total_timesteps      | 2973696     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.043565437 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.342      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.656       |\n",
      "|    n_updates            | 14510       |\n",
      "|    policy_gradient_loss | 0.0122      |\n",
      "|    value_loss           | 2.07e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=0.09 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0905      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2980000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015152938 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.0915      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 14550       |\n",
      "|    policy_gradient_loss | -0.00135    |\n",
      "|    value_loss           | 5.1e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0832      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2990000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022451434 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00954     |\n",
      "|    n_updates            | 14590       |\n",
      "|    policy_gradient_loss | -0.00812    |\n",
      "|    value_loss           | 8.7e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0178       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3000000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018729401 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.524       |\n",
      "|    explained_variance   | 0.0916       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000707    |\n",
      "|    n_updates            | 14640        |\n",
      "|    policy_gradient_loss | 0.00102      |\n",
      "|    value_loss           | 1.13e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0142     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3010000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012197859 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0415      |\n",
      "|    n_updates            | 14690       |\n",
      "|    policy_gradient_loss | -0.00306    |\n",
      "|    value_loss           | 2.34e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3020000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0397      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016668972 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.811      |\n",
      "|    explained_variance   | 0.0888      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00181    |\n",
      "|    n_updates            | 14740       |\n",
      "|    policy_gradient_loss | -0.00512    |\n",
      "|    value_loss           | 6.6e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0475      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3030000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008154033 |\n",
      "|    clip_fraction        | 0.0817      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.0828      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00366     |\n",
      "|    n_updates            | 14790       |\n",
      "|    policy_gradient_loss | -0.000252   |\n",
      "|    value_loss           | 7.27e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0987      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017117886 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 14840       |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 5.5e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=0.04 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0355      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012030908 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.738      |\n",
      "|    explained_variance   | 0.302       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0196      |\n",
      "|    n_updates            | 14890       |\n",
      "|    policy_gradient_loss | -0.00266    |\n",
      "|    value_loss           | 9.56e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=0.11 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.113       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016347822 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.79       |\n",
      "|    explained_variance   | 0.343       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0219      |\n",
      "|    n_updates            | 14940       |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    value_loss           | 8.29e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.359      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1496        |\n",
      "|    time_elapsed         | 4727        |\n",
      "|    total_timesteps      | 3063808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009322986 |\n",
      "|    clip_fraction        | 0.0731      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0168     |\n",
      "|    n_updates            | 14950       |\n",
      "|    policy_gradient_loss | -0.000975   |\n",
      "|    value_loss           | 3.98e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0579      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3070000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021574067 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.31       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0218     |\n",
      "|    n_updates            | 14990       |\n",
      "|    policy_gradient_loss | 0.0208      |\n",
      "|    value_loss           | 6.16e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0225      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019076873 |\n",
      "|    clip_fraction        | 0.0577      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.243      |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00176     |\n",
      "|    n_updates            | 15030       |\n",
      "|    policy_gradient_loss | -0.000543   |\n",
      "|    value_loss           | 2.65e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0292      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013201917 |\n",
      "|    clip_fraction        | 0.0902      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.0695      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 15080       |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    value_loss           | 1.26e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0789      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012415233 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.728      |\n",
      "|    explained_variance   | 0.013       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00473    |\n",
      "|    n_updates            | 15130       |\n",
      "|    policy_gradient_loss | -0.00122    |\n",
      "|    value_loss           | 1.52e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.034       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011534388 |\n",
      "|    clip_fraction        | 0.0871      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.0846      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00291    |\n",
      "|    n_updates            | 15180       |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    value_loss           | 2.35e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3120000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0425      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012481997 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.719      |\n",
      "|    explained_variance   | -0.0567     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.02       |\n",
      "|    n_updates            | 15230       |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 3.68e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0769      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.036851835 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0307     |\n",
      "|    n_updates            | 15280       |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.9e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=0.11 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.109       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012213236 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.778      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0133     |\n",
      "|    n_updates            | 15330       |\n",
      "|    policy_gradient_loss | -0.00459    |\n",
      "|    value_loss           | 5.64e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00522     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3150000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015961727 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.654      |\n",
      "|    explained_variance   | 0.629       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000721   |\n",
      "|    n_updates            | 15380       |\n",
      "|    policy_gradient_loss | 0.00264     |\n",
      "|    value_loss           | 7.07e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.353      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1540        |\n",
      "|    time_elapsed         | 4867        |\n",
      "|    total_timesteps      | 3153920     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010936173 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | 0.0497      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00612     |\n",
      "|    n_updates            | 15390       |\n",
      "|    policy_gradient_loss | 0.00353     |\n",
      "|    value_loss           | 4.63e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0289      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016178932 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0157     |\n",
      "|    n_updates            | 15420       |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 2.34e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0417     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024540033 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.374      |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0203     |\n",
      "|    n_updates            | 15470       |\n",
      "|    policy_gradient_loss | -0.000121   |\n",
      "|    value_loss           | 1.7e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.053      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3180000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00659596 |\n",
      "|    clip_fraction        | 0.0692     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.582     |\n",
      "|    explained_variance   | 0.191      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00773    |\n",
      "|    n_updates            | 15520      |\n",
      "|    policy_gradient_loss | 0.000143   |\n",
      "|    value_loss           | 1.48e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00276     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3190000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012472392 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.714      |\n",
      "|    explained_variance   | 0.0137      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00904     |\n",
      "|    n_updates            | 15570       |\n",
      "|    policy_gradient_loss | -0.00579    |\n",
      "|    value_loss           | 9.91e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0133     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3200000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01481548 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.497     |\n",
      "|    explained_variance   | 0.117      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0301     |\n",
      "|    n_updates            | 15620      |\n",
      "|    policy_gradient_loss | -0.000738  |\n",
      "|    value_loss           | 2.37e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0439      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3210000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017925885 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.00114     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00601    |\n",
      "|    n_updates            | 15670       |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    value_loss           | 4.45e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.014     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3220000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02064434 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.647     |\n",
      "|    explained_variance   | -0.079     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 15720      |\n",
      "|    policy_gradient_loss | -0.0116    |\n",
      "|    value_loss           | 7.44e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=-0.06 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0643     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3230000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011034571 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0277     |\n",
      "|    n_updates            | 15770       |\n",
      "|    policy_gradient_loss | -0.00242    |\n",
      "|    value_loss           | 1.46e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0673     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015722632 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00344    |\n",
      "|    n_updates            | 15820       |\n",
      "|    policy_gradient_loss | -0.00953    |\n",
      "|    value_loss           | 1.55e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.348      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1584        |\n",
      "|    time_elapsed         | 5005        |\n",
      "|    total_timesteps      | 3244032     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008957076 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.851      |\n",
      "|    explained_variance   | 0.00961     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0186      |\n",
      "|    n_updates            | 15830       |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 1.92e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0268      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016964948 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.181       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0407     |\n",
      "|    n_updates            | 15860       |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 2.61e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0475      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018941842 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.494      |\n",
      "|    explained_variance   | -0.188      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0288     |\n",
      "|    n_updates            | 15910       |\n",
      "|    policy_gradient_loss | -0.00759    |\n",
      "|    value_loss           | 1.88e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0203     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3270000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01740034 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.792     |\n",
      "|    explained_variance   | -0.106     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0168     |\n",
      "|    n_updates            | 15960      |\n",
      "|    policy_gradient_loss | 0.00214    |\n",
      "|    value_loss           | 5.85e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011562028 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.622      |\n",
      "|    explained_variance   | 0.0466      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0168      |\n",
      "|    n_updates            | 16010       |\n",
      "|    policy_gradient_loss | -0.00729    |\n",
      "|    value_loss           | 1.13e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0485       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3290000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073628123 |\n",
      "|    clip_fraction        | 0.0867       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.466       |\n",
      "|    explained_variance   | 0.493        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0129      |\n",
      "|    n_updates            | 16060        |\n",
      "|    policy_gradient_loss | -0.00452     |\n",
      "|    value_loss           | 7.71e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0404      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013222423 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.724      |\n",
      "|    explained_variance   | -0.102      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0144      |\n",
      "|    n_updates            | 16110       |\n",
      "|    policy_gradient_loss | -0.0037     |\n",
      "|    value_loss           | 5.28e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0698      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3310000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018958665 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.55       |\n",
      "|    explained_variance   | -0.175      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00414    |\n",
      "|    n_updates            | 16160       |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 1.92e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0663      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3320000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009483848 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.0281      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00319    |\n",
      "|    n_updates            | 16210       |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 9.37e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3330000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0473      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3330000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023839489 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00123    |\n",
      "|    n_updates            | 16250       |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 7.54e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9e+04        |\n",
      "|    ep_rew_mean          | -0.337       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 648          |\n",
      "|    iterations           | 1628         |\n",
      "|    time_elapsed         | 5144         |\n",
      "|    total_timesteps      | 3334144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0104389535 |\n",
      "|    clip_fraction        | 0.101        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.616       |\n",
      "|    explained_variance   | 0.106        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00937     |\n",
      "|    n_updates            | 16270        |\n",
      "|    policy_gradient_loss | 0.00458      |\n",
      "|    value_loss           | 2.19e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0281      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019471627 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00314     |\n",
      "|    n_updates            | 16300       |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 2.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00129    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011196925 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.228       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00884     |\n",
      "|    n_updates            | 16350       |\n",
      "|    policy_gradient_loss | -0.00866    |\n",
      "|    value_loss           | 1e-05       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0202      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016672742 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.79       |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00449    |\n",
      "|    n_updates            | 16400       |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    value_loss           | 8.48e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3370000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.056       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3370000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010682372 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.176       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.015      |\n",
      "|    n_updates            | 16450       |\n",
      "|    policy_gradient_loss | -0.00299    |\n",
      "|    value_loss           | 1.62e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0636      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015926518 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.00299     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00527    |\n",
      "|    n_updates            | 16500       |\n",
      "|    policy_gradient_loss | 0.00302     |\n",
      "|    value_loss           | 2.09e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0463      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3390000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011382146 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.731      |\n",
      "|    explained_variance   | 0.142       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00205     |\n",
      "|    n_updates            | 16550       |\n",
      "|    policy_gradient_loss | -0.00162    |\n",
      "|    value_loss           | 5.59e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0521      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013459811 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00874    |\n",
      "|    n_updates            | 16600       |\n",
      "|    policy_gradient_loss | -0.00858    |\n",
      "|    value_loss           | 8.34e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0125      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3410000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031436294 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | -0.0439     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 16650       |\n",
      "|    policy_gradient_loss | -0.000546   |\n",
      "|    value_loss           | 2.32e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3420000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0739     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3420000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01634324 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.56      |\n",
      "|    explained_variance   | 0.201      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.000799   |\n",
      "|    n_updates            | 16690      |\n",
      "|    policy_gradient_loss | -0.00299   |\n",
      "|    value_loss           | 8.04e-06   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.331     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 1672       |\n",
      "|    time_elapsed         | 5283       |\n",
      "|    total_timesteps      | 3424256    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02086567 |\n",
      "|    clip_fraction        | 0.0527     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.223     |\n",
      "|    explained_variance   | 0.0339     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00919    |\n",
      "|    n_updates            | 16710      |\n",
      "|    policy_gradient_loss | 0.00223    |\n",
      "|    value_loss           | 1.71e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0782      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3430000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018258184 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.072       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 16740       |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    value_loss           | 1.89e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0575      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018077591 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.179       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 16790       |\n",
      "|    policy_gradient_loss | -0.00961    |\n",
      "|    value_loss           | 1.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3450000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0604      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3450000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009047049 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000214    |\n",
      "|    n_updates            | 16840       |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 7.21e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0395      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012951274 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0237     |\n",
      "|    n_updates            | 16890       |\n",
      "|    policy_gradient_loss | -0.00753    |\n",
      "|    value_loss           | 1.05e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0477      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010257426 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.286       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00793    |\n",
      "|    n_updates            | 16940       |\n",
      "|    policy_gradient_loss | -0.00293    |\n",
      "|    value_loss           | 2.63e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.105       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3480000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014164025 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.000752   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 16990       |\n",
      "|    policy_gradient_loss | -0.00501    |\n",
      "|    value_loss           | 5.28e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3490000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0762      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3490000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018883303 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0259     |\n",
      "|    n_updates            | 17040       |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 3.5e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00263     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029732112 |\n",
      "|    clip_fraction        | 0.247       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.76       |\n",
      "|    explained_variance   | -0.501      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0157      |\n",
      "|    n_updates            | 17080       |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 7.24e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00562    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3510000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007826783 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00546    |\n",
      "|    n_updates            | 17130       |\n",
      "|    policy_gradient_loss | 0.000128    |\n",
      "|    value_loss           | 4.76e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.314      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1716        |\n",
      "|    time_elapsed         | 5423        |\n",
      "|    total_timesteps      | 3514368     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009406971 |\n",
      "|    clip_fraction        | 0.0832      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.437      |\n",
      "|    explained_variance   | 0.171       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00288    |\n",
      "|    n_updates            | 17150       |\n",
      "|    policy_gradient_loss | 0.00508     |\n",
      "|    value_loss           | 4.22e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0559      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015784798 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000954   |\n",
      "|    n_updates            | 17180       |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    value_loss           | 3.49e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3530000, episode_reward=-0.01 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00749    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3530000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009583272 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0151     |\n",
      "|    n_updates            | 17230       |\n",
      "|    policy_gradient_loss | -0.00811    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.000354    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3540000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058559333 |\n",
      "|    clip_fraction        | 0.0706       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.351       |\n",
      "|    explained_variance   | 0.072        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0059      |\n",
      "|    n_updates            | 17280        |\n",
      "|    policy_gradient_loss | 0.00503      |\n",
      "|    value_loss           | 8.7e-06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.000252   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3550000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014696682 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00736     |\n",
      "|    n_updates            | 17330       |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    value_loss           | 1.42e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.000512    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012338452 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.739      |\n",
      "|    explained_variance   | -0.0263     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00545     |\n",
      "|    n_updates            | 17380       |\n",
      "|    policy_gradient_loss | -0.00572    |\n",
      "|    value_loss           | 7.76e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0057     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3570000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010931704 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.93       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 17430       |\n",
      "|    policy_gradient_loss | -0.00607    |\n",
      "|    value_loss           | 5.62e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3580000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0224      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016184006 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | -0.0648     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 17480       |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 9.33e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0592      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3590000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014941832 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.89       |\n",
      "|    explained_variance   | -0.0449     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0136     |\n",
      "|    n_updates            | 17520       |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    value_loss           | 6.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3600000, episode_reward=-0.02 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0171     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011662172 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.708      |\n",
      "|    explained_variance   | -0.00603    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 17570       |\n",
      "|    policy_gradient_loss | -0.0059     |\n",
      "|    value_loss           | 3.17e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.311      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 1760        |\n",
      "|    time_elapsed         | 5562        |\n",
      "|    total_timesteps      | 3604480     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007543653 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.195      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00679     |\n",
      "|    n_updates            | 17590       |\n",
      "|    policy_gradient_loss | 0.000223    |\n",
      "|    value_loss           | 2.84e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=0.11 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.11        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3610000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015744595 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0351      |\n",
      "|    n_updates            | 17620       |\n",
      "|    policy_gradient_loss | -0.00461    |\n",
      "|    value_loss           | 2.46e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0636     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3620000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01965633 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.656     |\n",
      "|    explained_variance   | 0.194      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0485    |\n",
      "|    n_updates            | 17670      |\n",
      "|    policy_gradient_loss | -0.00846   |\n",
      "|    value_loss           | 1.19e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0252     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3630000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02414725 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.519     |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.072      |\n",
      "|    n_updates            | 17720      |\n",
      "|    policy_gradient_loss | 0.00495    |\n",
      "|    value_loss           | 9.25e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.023       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015355872 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 17770       |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.58e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0229      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3650000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009969779 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.722      |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 17820       |\n",
      "|    policy_gradient_loss | -0.00765    |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=0.04 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0364       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3660000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145560885 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.765       |\n",
      "|    explained_variance   | -0.226       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00645     |\n",
      "|    n_updates            | 17870        |\n",
      "|    policy_gradient_loss | -0.00877     |\n",
      "|    value_loss           | 5.66e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0536       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 3670000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061473027 |\n",
      "|    clip_fraction        | 0.0732       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.593       |\n",
      "|    explained_variance   | 0.0895       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0126      |\n",
      "|    n_updates            | 17910        |\n",
      "|    policy_gradient_loss | -0.00457     |\n",
      "|    value_loss           | 1.41e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=0.06 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0568      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3680000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020429289 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.897      |\n",
      "|    explained_variance   | -0.238      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0165     |\n",
      "|    n_updates            | 17960       |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    value_loss           | 9.2e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.018       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3690000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011197797 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0158      |\n",
      "|    n_updates            | 18010       |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    value_loss           | 5.02e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.297      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1804        |\n",
      "|    time_elapsed         | 5701        |\n",
      "|    total_timesteps      | 3694592     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010995172 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.748      |\n",
      "|    explained_variance   | 0.0424      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00249     |\n",
      "|    n_updates            | 18030       |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    value_loss           | 2.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3700000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0121      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3700000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011623405 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.704      |\n",
      "|    explained_variance   | 0.2         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 18060       |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    value_loss           | 2.29e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3710000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0124     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3710000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021071067 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.714      |\n",
      "|    explained_variance   | -0.178      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 18110       |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 1.37e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0105     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3720000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011540016 |\n",
      "|    clip_fraction        | 0.0906      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.006      |\n",
      "|    n_updates            | 18160       |\n",
      "|    policy_gradient_loss | -0.00234    |\n",
      "|    value_loss           | 1.86e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=-0.01 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0148     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3730000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028887946 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0439     |\n",
      "|    n_updates            | 18210       |\n",
      "|    policy_gradient_loss | -0.0068     |\n",
      "|    value_loss           | 1.37e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00104    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3740000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017534127 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.708      |\n",
      "|    explained_variance   | 0.238       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0501      |\n",
      "|    n_updates            | 18260       |\n",
      "|    policy_gradient_loss | -0.00376    |\n",
      "|    value_loss           | 1.05e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00524    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3750000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012027802 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.88       |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 18310       |\n",
      "|    policy_gradient_loss | -0.00269    |\n",
      "|    value_loss           | 6.41e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00264    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010288028 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.0428      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 18350       |\n",
      "|    policy_gradient_loss | -0.00602    |\n",
      "|    value_loss           | 1.08e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0365      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3770000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016255068 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.757      |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00509    |\n",
      "|    n_updates            | 18400       |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    value_loss           | 9.26e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3780000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00684    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011957419 |\n",
      "|    clip_fraction        | 0.0859      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.471      |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00213    |\n",
      "|    n_updates            | 18450       |\n",
      "|    policy_gradient_loss | 0.000504    |\n",
      "|    value_loss           | 7.02e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.29       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1848        |\n",
      "|    time_elapsed         | 5839        |\n",
      "|    total_timesteps      | 3784704     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021360535 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.825      |\n",
      "|    explained_variance   | -0.071      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0531      |\n",
      "|    n_updates            | 18470       |\n",
      "|    policy_gradient_loss | 0.00592     |\n",
      "|    value_loss           | 1.84e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3790000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00316     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3790000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014657013 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.235       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 18500       |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    value_loss           | 2.82e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3800000, episode_reward=-0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0292     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3800000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018166902 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.529      |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 18550       |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 1.81e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3810000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | -0.0267   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3810000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0114238 |\n",
      "|    clip_fraction        | 0.136     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.658    |\n",
      "|    explained_variance   | 0.0139    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00444  |\n",
      "|    n_updates            | 18600     |\n",
      "|    policy_gradient_loss | 0.000338  |\n",
      "|    value_loss           | 7.42e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0303      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3820000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014115485 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.0445      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 18650       |\n",
      "|    policy_gradient_loss | -0.00903    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0457     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3830000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01615766 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.646     |\n",
      "|    explained_variance   | 0.0349     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 18700      |\n",
      "|    policy_gradient_loss | -0.00637   |\n",
      "|    value_loss           | 7.05e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.00684   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3840000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00972929 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.676     |\n",
      "|    explained_variance   | 0.18       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 18740      |\n",
      "|    policy_gradient_loss | -0.00614   |\n",
      "|    value_loss           | 8.33e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3850000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012196571 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.54       |\n",
      "|    explained_variance   | -0.102      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 18790       |\n",
      "|    policy_gradient_loss | -0.00827    |\n",
      "|    value_loss           | 8.64e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00462     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3860000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011987215 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 0.153       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0268      |\n",
      "|    n_updates            | 18840       |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 1.24e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3870000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015352334 |\n",
      "|    clip_fraction        | 0.0861      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.402      |\n",
      "|    explained_variance   | -0.00247    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0231      |\n",
      "|    n_updates            | 18890       |\n",
      "|    policy_gradient_loss | 0.00169     |\n",
      "|    value_loss           | 7.36e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.29       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1892        |\n",
      "|    time_elapsed         | 5979        |\n",
      "|    total_timesteps      | 3874816     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015182058 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | -0.0156     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00308     |\n",
      "|    n_updates            | 18910       |\n",
      "|    policy_gradient_loss | 0.000512    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3880000, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0796     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011246996 |\n",
      "|    clip_fraction        | 0.0961      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.011       |\n",
      "|    n_updates            | 18940       |\n",
      "|    policy_gradient_loss | -0.000346   |\n",
      "|    value_loss           | 4.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=-0.02 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0225     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017823055 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 18990       |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 2.11e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005678127 |\n",
      "|    clip_fraction        | 0.0919      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.00849     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00499     |\n",
      "|    n_updates            | 19040       |\n",
      "|    policy_gradient_loss | -0.000215   |\n",
      "|    value_loss           | 8.87e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0421     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016843518 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00116     |\n",
      "|    n_updates            | 19090       |\n",
      "|    policy_gradient_loss | -0.00661    |\n",
      "|    value_loss           | 1.6e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3920000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0489    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3920000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01900794 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.578     |\n",
      "|    explained_variance   | -0.225     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0355    |\n",
      "|    n_updates            | 19140      |\n",
      "|    policy_gradient_loss | -0.0119    |\n",
      "|    value_loss           | 6.33e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 9.99e+03 |\n",
      "|    mean_reward          | 0.0569   |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 3930000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.016544 |\n",
      "|    clip_fraction        | 0.147    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.774   |\n",
      "|    explained_variance   | 0.174    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 0.0266   |\n",
      "|    n_updates            | 19180    |\n",
      "|    policy_gradient_loss | -0.00828 |\n",
      "|    value_loss           | 7.33e-06 |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0368     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3940000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01758121 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.533     |\n",
      "|    explained_variance   | 0.022      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0123    |\n",
      "|    n_updates            | 19230      |\n",
      "|    policy_gradient_loss | -0.00689   |\n",
      "|    value_loss           | 1.02e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0507      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3950000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021533791 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | 0.0662      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0179      |\n",
      "|    n_updates            | 19280       |\n",
      "|    policy_gradient_loss | -0.00492    |\n",
      "|    value_loss           | 9.27e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0136     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3960000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008933464 |\n",
      "|    clip_fraction        | 0.0724      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.356      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0236      |\n",
      "|    n_updates            | 19330       |\n",
      "|    policy_gradient_loss | 0.00112     |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.272      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1936        |\n",
      "|    time_elapsed         | 6117        |\n",
      "|    total_timesteps      | 3964928     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009361253 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 0.00296     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0149      |\n",
      "|    n_updates            | 19350       |\n",
      "|    policy_gradient_loss | 0.0039      |\n",
      "|    value_loss           | 1.81e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00349    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3970000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024827344 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.541      |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 19380       |\n",
      "|    policy_gradient_loss | -0.00799    |\n",
      "|    value_loss           | 2.39e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0462      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3980000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023992246 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | -0.032      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0314     |\n",
      "|    n_updates            | 19430       |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 2.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 3990000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007786496 |\n",
      "|    clip_fraction        | 0.0908      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00645    |\n",
      "|    n_updates            | 19480       |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    value_loss           | 1.19e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.00993    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03229259 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.54      |\n",
      "|    explained_variance   | 0.317      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00163   |\n",
      "|    n_updates            | 19530      |\n",
      "|    policy_gradient_loss | -0.00825   |\n",
      "|    value_loss           | 1.36e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4010000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4010000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018150184 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.496       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0234      |\n",
      "|    n_updates            | 19580       |\n",
      "|    policy_gradient_loss | -0.00804    |\n",
      "|    value_loss           | 4.74e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00195     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017244572 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.031       |\n",
      "|    n_updates            | 19620       |\n",
      "|    policy_gradient_loss | -0.0037     |\n",
      "|    value_loss           | 6.15e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4030000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0176      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4030000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0139825195 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.484        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00688     |\n",
      "|    n_updates            | 19670        |\n",
      "|    policy_gradient_loss | -0.0077      |\n",
      "|    value_loss           | 5.05e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0321      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012261785 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00512    |\n",
      "|    n_updates            | 19720       |\n",
      "|    policy_gradient_loss | -0.00742    |\n",
      "|    value_loss           | 6.66e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0719     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4050000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02205475 |\n",
      "|    clip_fraction        | 0.0693     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.338     |\n",
      "|    explained_variance   | 0.283      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00534    |\n",
      "|    n_updates            | 19770      |\n",
      "|    policy_gradient_loss | -0.00079   |\n",
      "|    value_loss           | 5.97e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.27       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 1980        |\n",
      "|    time_elapsed         | 6257        |\n",
      "|    total_timesteps      | 4055040     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010471104 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | -0.0112     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00318    |\n",
      "|    n_updates            | 19790       |\n",
      "|    policy_gradient_loss | -0.00151    |\n",
      "|    value_loss           | 1.53e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=-0.00 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00036    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014502587 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 0.362       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00597    |\n",
      "|    n_updates            | 19820       |\n",
      "|    policy_gradient_loss | -0.00911    |\n",
      "|    value_loss           | 2.53e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0133      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4070000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025110574 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | -0.0073     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0589     |\n",
      "|    n_updates            | 19870       |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    value_loss           | 1.76e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0105      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014513894 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.83       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 19920       |\n",
      "|    policy_gradient_loss | 0.0016      |\n",
      "|    value_loss           | 6.86e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.044      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012910824 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.363       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 19970       |\n",
      "|    policy_gradient_loss | -0.00916    |\n",
      "|    value_loss           | 1.69e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.029      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4100000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03426259 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.545     |\n",
      "|    explained_variance   | 0.168      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00173   |\n",
      "|    n_updates            | 20010      |\n",
      "|    policy_gradient_loss | 0.00339    |\n",
      "|    value_loss           | 5.26e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0326     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015582871 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | -0.0776     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.03       |\n",
      "|    n_updates            | 20060       |\n",
      "|    policy_gradient_loss | -0.00587    |\n",
      "|    value_loss           | 5.05e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0319     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023470769 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | -0.295      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.026       |\n",
      "|    n_updates            | 20110       |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 5.77e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4130000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0168     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010708986 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.0702      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00831    |\n",
      "|    n_updates            | 20160       |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 1.07e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0279      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4140000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051466143 |\n",
      "|    clip_fraction        | 0.0876       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.516       |\n",
      "|    explained_variance   | 0.092        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00054      |\n",
      "|    n_updates            | 20210        |\n",
      "|    policy_gradient_loss | 0.00174      |\n",
      "|    value_loss           | 1.33e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.27       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 2024        |\n",
      "|    time_elapsed         | 6398        |\n",
      "|    total_timesteps      | 4145152     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016385268 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 20230       |\n",
      "|    policy_gradient_loss | 0.00254     |\n",
      "|    value_loss           | 3.38e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.026      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4150000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014357014 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00519    |\n",
      "|    n_updates            | 20260       |\n",
      "|    policy_gradient_loss | -0.00518    |\n",
      "|    value_loss           | 3.22e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0134     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4160000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01989916 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.718     |\n",
      "|    explained_variance   | -0.234     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.037     |\n",
      "|    n_updates            | 20310      |\n",
      "|    policy_gradient_loss | -0.0189    |\n",
      "|    value_loss           | 1.89e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0175      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014471134 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.0783      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.025       |\n",
      "|    n_updates            | 20360       |\n",
      "|    policy_gradient_loss | 0.00143     |\n",
      "|    value_loss           | 9.47e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0223     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014440562 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.772       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.038      |\n",
      "|    n_updates            | 20410       |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 1.27e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.00808      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4190000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038740744 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0477      |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -2.94e-05    |\n",
      "|    n_updates            | 20450        |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 6.74e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0282     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014748397 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.35        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0138     |\n",
      "|    n_updates            | 20500       |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    value_loss           | 4.4e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0772      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4210000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010443468 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.345       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 20550       |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    value_loss           | 8.25e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0567      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025419481 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0292     |\n",
      "|    n_updates            | 20600       |\n",
      "|    policy_gradient_loss | -0.000724   |\n",
      "|    value_loss           | 1.8e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0685     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4230000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008699605 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.229      |\n",
      "|    explained_variance   | 0.0958      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0061     |\n",
      "|    n_updates            | 20650       |\n",
      "|    policy_gradient_loss | 0.00421     |\n",
      "|    value_loss           | 2.43e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.259      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 2068        |\n",
      "|    time_elapsed         | 6537        |\n",
      "|    total_timesteps      | 4235264     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013739103 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.871      |\n",
      "|    explained_variance   | 0.206       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0357      |\n",
      "|    n_updates            | 20670       |\n",
      "|    policy_gradient_loss | 0.00385     |\n",
      "|    value_loss           | 2.46e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0541     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017348204 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.554      |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 20700       |\n",
      "|    policy_gradient_loss | 0.000408    |\n",
      "|    value_loss           | 4.38e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0578     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018969685 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.0525      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00206    |\n",
      "|    n_updates            | 20750       |\n",
      "|    policy_gradient_loss | -0.00884    |\n",
      "|    value_loss           | 1.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4260000, episode_reward=-0.04 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0412     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008901782 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.631      |\n",
      "|    explained_variance   | 0.206       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00261    |\n",
      "|    n_updates            | 20800       |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    value_loss           | 7.89e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4270000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0137     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4270000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013302054 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.431      |\n",
      "|    explained_variance   | 0.0677      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0333     |\n",
      "|    n_updates            | 20840       |\n",
      "|    policy_gradient_loss | -0.00863    |\n",
      "|    value_loss           | 1.88e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4280000, episode_reward=-0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0972     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015026331 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.698      |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00305    |\n",
      "|    n_updates            | 20890       |\n",
      "|    policy_gradient_loss | 0.0039      |\n",
      "|    value_loss           | 8.84e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=-0.14 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.142      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4290000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012717687 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | -0.0836     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00767    |\n",
      "|    n_updates            | 20940       |\n",
      "|    policy_gradient_loss | -0.00542    |\n",
      "|    value_loss           | 5.09e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0174     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016466489 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | -0.288      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0426     |\n",
      "|    n_updates            | 20990       |\n",
      "|    policy_gradient_loss | -0.0098     |\n",
      "|    value_loss           | 8.07e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.117      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4310000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022881892 |\n",
      "|    clip_fraction        | 0.0952      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.141       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00771     |\n",
      "|    n_updates            | 21040       |\n",
      "|    policy_gradient_loss | -0.00512    |\n",
      "|    value_loss           | 1.86e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0723      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4320000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071660094 |\n",
      "|    clip_fraction        | 0.0987       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.593       |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00871      |\n",
      "|    n_updates            | 21090        |\n",
      "|    policy_gradient_loss | -0.000195    |\n",
      "|    value_loss           | 4.18e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.255      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 2112        |\n",
      "|    time_elapsed         | 6676        |\n",
      "|    total_timesteps      | 4325376     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008956827 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.845      |\n",
      "|    explained_variance   | 0.0796      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00331     |\n",
      "|    n_updates            | 21110       |\n",
      "|    policy_gradient_loss | -0.00056    |\n",
      "|    value_loss           | 1.66e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0215     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4330000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018643115 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00692    |\n",
      "|    n_updates            | 21140       |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    value_loss           | 3.6e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=-0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.101      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015084593 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | -0.119      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 21190       |\n",
      "|    policy_gradient_loss | -0.00886    |\n",
      "|    value_loss           | 1.74e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4350000, episode_reward=-0.04 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0439     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054415897 |\n",
      "|    clip_fraction        | 0.0773      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.209       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0327     |\n",
      "|    n_updates            | 21240       |\n",
      "|    policy_gradient_loss | -0.00295    |\n",
      "|    value_loss           | 6.23e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=-0.14 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.143      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014197893 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.363      |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00897    |\n",
      "|    n_updates            | 21280       |\n",
      "|    policy_gradient_loss | -0.00854    |\n",
      "|    value_loss           | 1.71e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=-0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.128      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4370000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016110897 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.76       |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00897    |\n",
      "|    n_updates            | 21330       |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 8e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=-0.12 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.115      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013833612 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.028       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0268     |\n",
      "|    n_updates            | 21380       |\n",
      "|    policy_gradient_loss | -0.00582    |\n",
      "|    value_loss           | 6e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0282    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4390000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01936595 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.633     |\n",
      "|    explained_variance   | -0.226     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.04e-05   |\n",
      "|    n_updates            | 21430      |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    value_loss           | 7.44e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0529      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020366851 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0254      |\n",
      "|    n_updates            | 21480       |\n",
      "|    policy_gradient_loss | -0.00165    |\n",
      "|    value_loss           | 1.28e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=0.08 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0823     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4410000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01766523 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.866     |\n",
      "|    explained_variance   | 0.18       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0112    |\n",
      "|    n_updates            | 21530      |\n",
      "|    policy_gradient_loss | -0.00398   |\n",
      "|    value_loss           | 1.05e-05   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.24      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 648        |\n",
      "|    iterations           | 2156       |\n",
      "|    time_elapsed         | 6813       |\n",
      "|    total_timesteps      | 4415488    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01978077 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.659     |\n",
      "|    explained_variance   | 0.0436     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0323     |\n",
      "|    n_updates            | 21550      |\n",
      "|    policy_gradient_loss | 0.00544    |\n",
      "|    value_loss           | 4.4e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0195      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013233209 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.648      |\n",
      "|    explained_variance   | 0.055       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0259      |\n",
      "|    n_updates            | 21580       |\n",
      "|    policy_gradient_loss | 0.00183     |\n",
      "|    value_loss           | 3.31e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0322      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4430000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028168492 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.546      |\n",
      "|    explained_variance   | -0.449      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0473     |\n",
      "|    n_updates            | 21630       |\n",
      "|    policy_gradient_loss | -0.0135     |\n",
      "|    value_loss           | 1.46e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0634      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026421294 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.621      |\n",
      "|    explained_variance   | 0.0688      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0106     |\n",
      "|    n_updates            | 21670       |\n",
      "|    policy_gradient_loss | -9.47e-05   |\n",
      "|    value_loss           | 2.2e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0535     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4450000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012461932 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00775     |\n",
      "|    n_updates            | 21720       |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    value_loss           | 2.8e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0713      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011916036 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.805      |\n",
      "|    explained_variance   | -0.0244     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.016       |\n",
      "|    n_updates            | 21770       |\n",
      "|    policy_gradient_loss | 0.00365     |\n",
      "|    value_loss           | 8.12e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0798      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017857213 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | -0.0337     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0084     |\n",
      "|    n_updates            | 21820       |\n",
      "|    policy_gradient_loss | -0.00738    |\n",
      "|    value_loss           | 6.05e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4480000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.052       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4480000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019321758 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | -0.369      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000955    |\n",
      "|    n_updates            | 21870       |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 5.68e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4490000, episode_reward=0.07 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0702      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4490000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029576104 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.546      |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0724      |\n",
      "|    n_updates            | 21920       |\n",
      "|    policy_gradient_loss | -0.00845    |\n",
      "|    value_loss           | 5.72e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.025       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012347832 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00362    |\n",
      "|    n_updates            | 21970       |\n",
      "|    policy_gradient_loss | 0.0012      |\n",
      "|    value_loss           | 7.59e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.229      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 2200        |\n",
      "|    time_elapsed         | 6949        |\n",
      "|    total_timesteps      | 4505600     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040887777 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.43       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000709   |\n",
      "|    n_updates            | 21990       |\n",
      "|    policy_gradient_loss | 0.00766     |\n",
      "|    value_loss           | 3.5e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00542    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4510000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012943481 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00447    |\n",
      "|    n_updates            | 22020       |\n",
      "|    policy_gradient_loss | -0.00434    |\n",
      "|    value_loss           | 3.33e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00019     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016698854 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000674    |\n",
      "|    n_updates            | 22070       |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 2.31e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.02       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4530000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014231954 |\n",
      "|    clip_fraction        | 0.0627      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.0996      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0116      |\n",
      "|    n_updates            | 22110       |\n",
      "|    policy_gradient_loss | 0.00199     |\n",
      "|    value_loss           | 1.83e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00147    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4540000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019698728 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.037       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0353     |\n",
      "|    n_updates            | 22160       |\n",
      "|    policy_gradient_loss | -0.00465    |\n",
      "|    value_loss           | 2.97e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0242      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4550000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023823503 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.0624      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0552      |\n",
      "|    n_updates            | 22210       |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    value_loss           | 1.03e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0544      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010457876 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | 0.0623      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0127     |\n",
      "|    n_updates            | 22260       |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    value_loss           | 6.35e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0436      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4570000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018543335 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.634      |\n",
      "|    explained_variance   | -0.00408    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0238     |\n",
      "|    n_updates            | 22310       |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 9.87e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0285      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014407626 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 22360       |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    value_loss           | 2.16e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=0.08 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0763      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4590000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019503227 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.0571      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00748     |\n",
      "|    n_updates            | 22410       |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    value_loss           | 3.97e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 9e+04     |\n",
      "|    ep_rew_mean          | -0.221    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 648       |\n",
      "|    iterations           | 2244      |\n",
      "|    time_elapsed         | 7086      |\n",
      "|    total_timesteps      | 4595712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0181951 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.511    |\n",
      "|    explained_variance   | 0.133     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0161    |\n",
      "|    n_updates            | 22430     |\n",
      "|    policy_gradient_loss | 0.0036    |\n",
      "|    value_loss           | 4.97e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00724     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013845312 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.45       |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0184     |\n",
      "|    n_updates            | 22460       |\n",
      "|    policy_gradient_loss | -0.00123    |\n",
      "|    value_loss           | 2.58e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0396      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4610000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015794374 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.558      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0132      |\n",
      "|    n_updates            | 22500       |\n",
      "|    policy_gradient_loss | -0.0044     |\n",
      "|    value_loss           | 1.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0231     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010210393 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.365      |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00992    |\n",
      "|    n_updates            | 22550       |\n",
      "|    policy_gradient_loss | 0.00459     |\n",
      "|    value_loss           | 1.86e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.028     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4630000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02247902 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.564     |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0139    |\n",
      "|    n_updates            | 22600      |\n",
      "|    policy_gradient_loss | -0.00775   |\n",
      "|    value_loss           | 1.91e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0132      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010878516 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.652      |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000489   |\n",
      "|    n_updates            | 22650       |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    value_loss           | 8.63e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0362      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4650000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025173847 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | -0.0932     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0153     |\n",
      "|    n_updates            | 22700       |\n",
      "|    policy_gradient_loss | -0.00692    |\n",
      "|    value_loss           | 7.04e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0114      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4660000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014345145 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | -1.48       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00874    |\n",
      "|    n_updates            | 22750       |\n",
      "|    policy_gradient_loss | -0.00731    |\n",
      "|    value_loss           | 1.05e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0367      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4670000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021148678 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0236      |\n",
      "|    n_updates            | 22800       |\n",
      "|    policy_gradient_loss | -0.00619    |\n",
      "|    value_loss           | 1.74e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0614     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4680000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01096747 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.567     |\n",
      "|    explained_variance   | 0.0457     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0266     |\n",
      "|    n_updates            | 22850      |\n",
      "|    policy_gradient_loss | -0.00539   |\n",
      "|    value_loss           | 1.61e-05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.219      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 2288        |\n",
      "|    time_elapsed         | 7224        |\n",
      "|    total_timesteps      | 4685824     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024725337 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.274       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 22870       |\n",
      "|    policy_gradient_loss | 0.000256    |\n",
      "|    value_loss           | 2.26e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.026       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4690000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021008397 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.327       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00175    |\n",
      "|    n_updates            | 22900       |\n",
      "|    policy_gradient_loss | -0.00573    |\n",
      "|    value_loss           | 2.39e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0526      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4700000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010583505 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 22940       |\n",
      "|    policy_gradient_loss | -0.00278    |\n",
      "|    value_loss           | 1.48e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0518      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4710000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017166087 |\n",
      "|    clip_fraction        | 0.0799      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00689     |\n",
      "|    n_updates            | 22990       |\n",
      "|    policy_gradient_loss | 0.00375     |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.116       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4720000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026346738 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.0307      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0325     |\n",
      "|    n_updates            | 23040       |\n",
      "|    policy_gradient_loss | -0.0089     |\n",
      "|    value_loss           | 1.66e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0213     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4730000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014117464 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | 0.00493     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0166     |\n",
      "|    n_updates            | 23090       |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    value_loss           | 1.4e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0179     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4740000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015727192 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.594      |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 23140       |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    value_loss           | 8.26e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0808      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4750000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023130124 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | -0.312      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 23190       |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    value_loss           | 6.63e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.002      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4760000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02949361 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.681     |\n",
      "|    explained_variance   | 0.142      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0356    |\n",
      "|    n_updates            | 23240      |\n",
      "|    policy_gradient_loss | -0.00322   |\n",
      "|    value_loss           | 2.09e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0171     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4770000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019224301 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.196       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0137     |\n",
      "|    n_updates            | 23290       |\n",
      "|    policy_gradient_loss | 0.00277     |\n",
      "|    value_loss           | 1.42e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.221      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 2332        |\n",
      "|    time_elapsed         | 7360        |\n",
      "|    total_timesteps      | 4775936     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010491628 |\n",
      "|    clip_fraction        | 0.092       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.66       |\n",
      "|    explained_variance   | 0.653       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0171      |\n",
      "|    n_updates            | 23310       |\n",
      "|    policy_gradient_loss | 0.000868    |\n",
      "|    value_loss           | 2.65e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0606     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014333507 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | -0.0448     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0209     |\n",
      "|    n_updates            | 23330       |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    value_loss           | 2.89e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=0.14 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.138      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4790000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02039211 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.639     |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 23380      |\n",
      "|    policy_gradient_loss | -0.00938   |\n",
      "|    value_loss           | 1.11e-05   |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4800000, episode_reward=0.11 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.107       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4800000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016476613 |\n",
      "|    clip_fraction        | 0.0822      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0127      |\n",
      "|    n_updates            | 23430       |\n",
      "|    policy_gradient_loss | 0.00256     |\n",
      "|    value_loss           | 7.81e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0438      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015669875 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.498      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00124     |\n",
      "|    n_updates            | 23480       |\n",
      "|    policy_gradient_loss | -0.00879    |\n",
      "|    value_loss           | 2.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0122    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4820000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02555703 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.627     |\n",
      "|    explained_variance   | -0.00233   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0239     |\n",
      "|    n_updates            | 23530      |\n",
      "|    policy_gradient_loss | -0.00142   |\n",
      "|    value_loss           | 9.16e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4830000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00986    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4830000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016391078 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.103       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000646    |\n",
      "|    n_updates            | 23580       |\n",
      "|    policy_gradient_loss | -0.00174    |\n",
      "|    value_loss           | 1.07e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0403      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020403288 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0508     |\n",
      "|    n_updates            | 23630       |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 7.37e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0208      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4850000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0117152035 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.56        |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00862     |\n",
      "|    n_updates            | 23680        |\n",
      "|    policy_gradient_loss | -0.00456     |\n",
      "|    value_loss           | 3.55e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0221     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4860000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015629172 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0245      |\n",
      "|    n_updates            | 23730       |\n",
      "|    policy_gradient_loss | -0.00139    |\n",
      "|    value_loss           | 1.54e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.227      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 2376        |\n",
      "|    time_elapsed         | 7497        |\n",
      "|    total_timesteps      | 4866048     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013177408 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.86       |\n",
      "|    explained_variance   | 0.0335      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00891     |\n",
      "|    n_updates            | 23750       |\n",
      "|    policy_gradient_loss | -0.00028    |\n",
      "|    value_loss           | 1.88e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.121      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4870000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016015965 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.522      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0101     |\n",
      "|    n_updates            | 23770       |\n",
      "|    policy_gradient_loss | -0.00775    |\n",
      "|    value_loss           | 4.39e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4880000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0254     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025280785 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.569      |\n",
      "|    explained_variance   | -0.0152     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 23820       |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 1.11e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=-0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.13       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014846951 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.673      |\n",
      "|    explained_variance   | 0.000769    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00628     |\n",
      "|    n_updates            | 23870       |\n",
      "|    policy_gradient_loss | -0.000687   |\n",
      "|    value_loss           | 3.87e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=-0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.129     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4900000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01946443 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.668     |\n",
      "|    explained_variance   | 0.207      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0215    |\n",
      "|    n_updates            | 23920      |\n",
      "|    policy_gradient_loss | -0.0133    |\n",
      "|    value_loss           | 2.13e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=-0.16 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.162      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020178765 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.0995      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.025      |\n",
      "|    n_updates            | 23970       |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    value_loss           | 9.6e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0811      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 4920000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0127101485 |\n",
      "|    clip_fraction        | 0.132        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | 0.298        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0142      |\n",
      "|    n_updates            | 24020        |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    value_loss           | 7.13e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0579     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4930000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017918915 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.655       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0209     |\n",
      "|    n_updates            | 24070       |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    value_loss           | 4.93e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=-0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0819     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4940000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013519438 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | -0.0451     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00431    |\n",
      "|    n_updates            | 24120       |\n",
      "|    policy_gradient_loss | 3.59e-05    |\n",
      "|    value_loss           | 1.19e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4950000, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0821     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4950000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014160121 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0944      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0104     |\n",
      "|    n_updates            | 24160       |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    value_loss           | 7.96e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.232      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 2420        |\n",
      "|    time_elapsed         | 7633        |\n",
      "|    total_timesteps      | 4956160     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019755336 |\n",
      "|    clip_fraction        | 0.091       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.00252     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0177     |\n",
      "|    n_updates            | 24190       |\n",
      "|    policy_gradient_loss | 0.00957     |\n",
      "|    value_loss           | 1.4e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00996    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4960000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017393025 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | -0.138      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0573     |\n",
      "|    n_updates            | 24210       |\n",
      "|    policy_gradient_loss | -0.00562    |\n",
      "|    value_loss           | 2.02e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0351     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4970000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017025428 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0154     |\n",
      "|    n_updates            | 24260       |\n",
      "|    policy_gradient_loss | -0.00657    |\n",
      "|    value_loss           | 1.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0304    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4980000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01306847 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.576     |\n",
      "|    explained_variance   | 0.167      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0163     |\n",
      "|    n_updates            | 24310      |\n",
      "|    policy_gradient_loss | 0.000555   |\n",
      "|    value_loss           | 6.52e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4990000, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0653     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 4990000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016989276 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.12        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0158      |\n",
      "|    n_updates            | 24360       |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    value_loss           | 3.33e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5000000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0116      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015289651 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00238     |\n",
      "|    n_updates            | 24410       |\n",
      "|    policy_gradient_loss | -0.00689    |\n",
      "|    value_loss           | 1.74e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5010000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0397     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5010000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026775448 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 24460       |\n",
      "|    policy_gradient_loss | -0.000536   |\n",
      "|    value_loss           | 5.57e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5020000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00912     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023964724 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0792      |\n",
      "|    n_updates            | 24510       |\n",
      "|    policy_gradient_loss | -0.00451    |\n",
      "|    value_loss           | 7.28e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5030000, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0391     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5030000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021928025 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.656      |\n",
      "|    explained_variance   | -0.00274    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 24560       |\n",
      "|    policy_gradient_loss | 0.00216     |\n",
      "|    value_loss           | 1.28e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5040000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0263     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008739165 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.622      |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000396    |\n",
      "|    n_updates            | 24600       |\n",
      "|    policy_gradient_loss | -0.0024     |\n",
      "|    value_loss           | 8.43e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.234      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 2464        |\n",
      "|    time_elapsed         | 7769        |\n",
      "|    total_timesteps      | 5046272     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014558466 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.733      |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0557      |\n",
      "|    n_updates            | 24630       |\n",
      "|    policy_gradient_loss | 0.00238     |\n",
      "|    value_loss           | 1.65e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5050000, episode_reward=-0.07 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0699    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5050000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02234241 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.608     |\n",
      "|    explained_variance   | 0.116      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0323     |\n",
      "|    n_updates            | 24650      |\n",
      "|    policy_gradient_loss | -0.00701   |\n",
      "|    value_loss           | 2.59e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5060000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0634     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023347199 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0237     |\n",
      "|    n_updates            | 24700       |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 1.42e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5070000, episode_reward=-0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0979     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5070000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032215483 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.504      |\n",
      "|    explained_variance   | 0.043       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0274      |\n",
      "|    n_updates            | 24750       |\n",
      "|    policy_gradient_loss | 0.00237     |\n",
      "|    value_loss           | 8.74e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5080000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0439     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015539838 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.034      |\n",
      "|    n_updates            | 24800       |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 2.58e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5090000, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0882     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012205157 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0521      |\n",
      "|    n_updates            | 24850       |\n",
      "|    policy_gradient_loss | -0.00189    |\n",
      "|    value_loss           | 1.19e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5100000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.061     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5100000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01449832 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.574     |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0469     |\n",
      "|    n_updates            | 24900      |\n",
      "|    policy_gradient_loss | -0.000969  |\n",
      "|    value_loss           | 1.51e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5110000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0605    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5110000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02189207 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.681     |\n",
      "|    explained_variance   | 0.357      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 24950      |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    value_loss           | 5.1e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5120000, episode_reward=-0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.122      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016459536 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | -0.00301    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 24990       |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    value_loss           | 9.09e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5130000, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0702     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015194427 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.539      |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00764     |\n",
      "|    n_updates            | 25040       |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    value_loss           | 7.97e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.237      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 2508        |\n",
      "|    time_elapsed         | 7907        |\n",
      "|    total_timesteps      | 5136384     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024044778 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.828      |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0602      |\n",
      "|    n_updates            | 25070       |\n",
      "|    policy_gradient_loss | 0.00939     |\n",
      "|    value_loss           | 1.82e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5140000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0203     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017763581 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 25090       |\n",
      "|    policy_gradient_loss | -0.00312    |\n",
      "|    value_loss           | 3.93e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5150000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | -0.00876  |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5150000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0174543 |\n",
      "|    clip_fraction        | 0.125     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.441    |\n",
      "|    explained_variance   | 0.193     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0165   |\n",
      "|    n_updates            | 25140     |\n",
      "|    policy_gradient_loss | -0.0083   |\n",
      "|    value_loss           | 1.32e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=5160000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0406     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020692995 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.842      |\n",
      "|    explained_variance   | 0.243       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00483     |\n",
      "|    n_updates            | 25190       |\n",
      "|    policy_gradient_loss | 0.0048      |\n",
      "|    value_loss           | 7e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5170000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0147     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028163452 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00579     |\n",
      "|    n_updates            | 25240       |\n",
      "|    policy_gradient_loss | -0.00708    |\n",
      "|    value_loss           | 2.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5180000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.00996    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5180000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02777807 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.573     |\n",
      "|    explained_variance   | 0.327      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0081     |\n",
      "|    n_updates            | 25290      |\n",
      "|    policy_gradient_loss | -0.00545   |\n",
      "|    value_loss           | 9.41e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5190000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0215     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5190000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009378937 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | 0.0878      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 25340       |\n",
      "|    policy_gradient_loss | 0.000167    |\n",
      "|    value_loss           | 5.1e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5200000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0109      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 5200000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0105204275 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.447       |\n",
      "|    explained_variance   | 0.384        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0114      |\n",
      "|    n_updates            | 25390        |\n",
      "|    policy_gradient_loss | -0.00223     |\n",
      "|    value_loss           | 1.63e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=5210000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -6.8e-05   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5210000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01809652 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.618     |\n",
      "|    explained_variance   | 0.143      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0177     |\n",
      "|    n_updates            | 25430      |\n",
      "|    policy_gradient_loss | -0.00299   |\n",
      "|    value_loss           | 8.28e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5220000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0198     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020583857 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 25480       |\n",
      "|    policy_gradient_loss | -0.00477    |\n",
      "|    value_loss           | 1.43e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.237      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 2552        |\n",
      "|    time_elapsed         | 8044        |\n",
      "|    total_timesteps      | 5226496     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012709301 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.738      |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 25510       |\n",
      "|    policy_gradient_loss | 0.00329     |\n",
      "|    value_loss           | 1.66e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5230000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0136     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5230000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01947359 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.633     |\n",
      "|    explained_variance   | 0.484      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0216     |\n",
      "|    n_updates            | 25530      |\n",
      "|    policy_gradient_loss | -0.00542   |\n",
      "|    value_loss           | 2.8e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5240000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0127      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019728074 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.491      |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0256     |\n",
      "|    n_updates            | 25580       |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 1.41e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5250000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0132     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010639752 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.827      |\n",
      "|    explained_variance   | 0.0245      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0422      |\n",
      "|    n_updates            | 25630       |\n",
      "|    policy_gradient_loss | -0.000657   |\n",
      "|    value_loss           | 4.85e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5260000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 9.99e+03 |\n",
      "|    mean_reward          | -0.0414  |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 5260000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.017095 |\n",
      "|    clip_fraction        | 0.158    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.616   |\n",
      "|    explained_variance   | 0.17     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.00058 |\n",
      "|    n_updates            | 25680    |\n",
      "|    policy_gradient_loss | -0.011   |\n",
      "|    value_loss           | 1.98e-05 |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=5270000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00879     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5270000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011736698 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.531       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00475    |\n",
      "|    n_updates            | 25730       |\n",
      "|    policy_gradient_loss | -2.8e-05    |\n",
      "|    value_loss           | 9.41e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5280000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0248     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016307194 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00944    |\n",
      "|    n_updates            | 25780       |\n",
      "|    policy_gradient_loss | 0.00491     |\n",
      "|    value_loss           | 5.82e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5290000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0427      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5290000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012726842 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.671      |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000674    |\n",
      "|    n_updates            | 25830       |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    value_loss           | 1.49e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5300000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0729      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017485542 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.189       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 25870       |\n",
      "|    policy_gradient_loss | -0.000467   |\n",
      "|    value_loss           | 6.77e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5310000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0149      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5310000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018296579 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 25920       |\n",
      "|    policy_gradient_loss | 0.00293     |\n",
      "|    value_loss           | 4.97e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.236      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 2596        |\n",
      "|    time_elapsed         | 8181        |\n",
      "|    total_timesteps      | 5316608     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021180872 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00839    |\n",
      "|    n_updates            | 25950       |\n",
      "|    policy_gradient_loss | 0.0115      |\n",
      "|    value_loss           | 2.76e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5320000, episode_reward=0.13 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.129      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5320000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02351377 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.595     |\n",
      "|    explained_variance   | 0.252      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00792   |\n",
      "|    n_updates            | 25970      |\n",
      "|    policy_gradient_loss | -0.00586   |\n",
      "|    value_loss           | 2.71e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5330000, episode_reward=0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.125       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5330000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017746191 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0212     |\n",
      "|    n_updates            | 26020       |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 1.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5340000, episode_reward=0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.123       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010131582 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.846      |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0194      |\n",
      "|    n_updates            | 26070       |\n",
      "|    policy_gradient_loss | 0.00396     |\n",
      "|    value_loss           | 1.13e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5350000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.073       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013111668 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.366      |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0176     |\n",
      "|    n_updates            | 26120       |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    value_loss           | 2.38e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5360000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0581      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024678607 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.429      |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 26170       |\n",
      "|    policy_gradient_loss | -0.00889    |\n",
      "|    value_loss           | 9.43e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5370000, episode_reward=0.17 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.172       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5370000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017791504 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.48       |\n",
      "|    explained_variance   | 0.112       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00607     |\n",
      "|    n_updates            | 26220       |\n",
      "|    policy_gradient_loss | -0.00361    |\n",
      "|    value_loss           | 5.41e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5380000, episode_reward=0.17 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.175       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021423664 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.246       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 26260       |\n",
      "|    policy_gradient_loss | -0.00965    |\n",
      "|    value_loss           | 6.58e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5390000, episode_reward=0.14 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.139       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5390000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021805342 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.045      |\n",
      "|    n_updates            | 26310       |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 1.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5400000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0567      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020861715 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0822      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00802     |\n",
      "|    n_updates            | 26360       |\n",
      "|    policy_gradient_loss | 0.00598     |\n",
      "|    value_loss           | 6.07e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.234      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 2640        |\n",
      "|    time_elapsed         | 8317        |\n",
      "|    total_timesteps      | 5406720     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013662048 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.655      |\n",
      "|    explained_variance   | 0.0998      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00231     |\n",
      "|    n_updates            | 26390       |\n",
      "|    policy_gradient_loss | 0.000288    |\n",
      "|    value_loss           | 1.53e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5410000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0808      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5410000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014358219 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.109       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000138    |\n",
      "|    n_updates            | 26410       |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    value_loss           | 3.19e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5420000, episode_reward=0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.118      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5420000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02058515 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.435     |\n",
      "|    explained_variance   | -0.0997    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00732   |\n",
      "|    n_updates            | 26460      |\n",
      "|    policy_gradient_loss | -0.0136    |\n",
      "|    value_loss           | 3.95e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5430000, episode_reward=0.16 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.158       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5430000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020832878 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.81       |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00166    |\n",
      "|    n_updates            | 26510       |\n",
      "|    policy_gradient_loss | 0.00639     |\n",
      "|    value_loss           | 6.71e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5440000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.131       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015856633 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 26560       |\n",
      "|    policy_gradient_loss | -0.00807    |\n",
      "|    value_loss           | 1.53e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5450000, episode_reward=0.08 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0826      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5450000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016883321 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.572      |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0075      |\n",
      "|    n_updates            | 26610       |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 1.16e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5460000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0725      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018259613 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 26660       |\n",
      "|    policy_gradient_loss | -0.00095    |\n",
      "|    value_loss           | 9.88e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5470000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0654      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019241527 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.023       |\n",
      "|    n_updates            | 26700       |\n",
      "|    policy_gradient_loss | -0.00928    |\n",
      "|    value_loss           | 1.02e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5480000, episode_reward=0.05 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0518     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5480000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01831077 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.67      |\n",
      "|    explained_variance   | 0.116      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0129    |\n",
      "|    n_updates            | 26750      |\n",
      "|    policy_gradient_loss | -0.00491   |\n",
      "|    value_loss           | 7.8e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5490000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.044       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5490000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017554816 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.108       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0246     |\n",
      "|    n_updates            | 26800       |\n",
      "|    policy_gradient_loss | 0.00228     |\n",
      "|    value_loss           | 5.09e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.241      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 2684        |\n",
      "|    time_elapsed         | 8453        |\n",
      "|    total_timesteps      | 5496832     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021296166 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.79       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0271     |\n",
      "|    n_updates            | 26830       |\n",
      "|    policy_gradient_loss | 0.00142     |\n",
      "|    value_loss           | 2.15e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5500000, episode_reward=0.14 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.137       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027046684 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.705      |\n",
      "|    explained_variance   | 0.214       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 26850       |\n",
      "|    policy_gradient_loss | -0.007      |\n",
      "|    value_loss           | 2.94e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5510000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0506      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5510000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019998735 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.51       |\n",
      "|    explained_variance   | 0.157       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 26900       |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.68e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5520000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.000542   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5520000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01118947 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.668     |\n",
      "|    explained_variance   | 0.103      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0206     |\n",
      "|    n_updates            | 26950      |\n",
      "|    policy_gradient_loss | -0.00062   |\n",
      "|    value_loss           | 1.05e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5530000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0293     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5530000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01691614 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.682     |\n",
      "|    explained_variance   | 0.312      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 27000      |\n",
      "|    policy_gradient_loss | -0.011     |\n",
      "|    value_loss           | 2.13e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5540000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0269     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5540000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015086377 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00262    |\n",
      "|    n_updates            | 27050       |\n",
      "|    policy_gradient_loss | -0.00952    |\n",
      "|    value_loss           | 3.88e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5550000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0327     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5550000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019930389 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.663      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0073      |\n",
      "|    n_updates            | 27090       |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 6.44e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5560000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0753     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023706382 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0215     |\n",
      "|    n_updates            | 27140       |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    value_loss           | 9.22e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5570000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0109      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5570000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014957756 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 27190       |\n",
      "|    policy_gradient_loss | -0.00219    |\n",
      "|    value_loss           | 1.28e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5580000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00461     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019558938 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0187     |\n",
      "|    n_updates            | 27240       |\n",
      "|    policy_gradient_loss | 0.00249     |\n",
      "|    value_loss           | 5.52e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.25       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 2728        |\n",
      "|    time_elapsed         | 8590        |\n",
      "|    total_timesteps      | 5586944     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018215628 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | 0.187       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000888   |\n",
      "|    n_updates            | 27270       |\n",
      "|    policy_gradient_loss | 0.00347     |\n",
      "|    value_loss           | 2.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5590000, episode_reward=-0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0532     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5590000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014715992 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00167    |\n",
      "|    n_updates            | 27290       |\n",
      "|    policy_gradient_loss | -0.00471    |\n",
      "|    value_loss           | 3.75e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5600000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0231     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020388708 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | -0.124      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000769    |\n",
      "|    n_updates            | 27340       |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 1.36e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5610000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0501     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5610000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024645196 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0175      |\n",
      "|    n_updates            | 27390       |\n",
      "|    policy_gradient_loss | 0.000278    |\n",
      "|    value_loss           | 2.64e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5620000, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0788     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023345422 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0491     |\n",
      "|    n_updates            | 27440       |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 2.5e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5630000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.042      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5630000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012618315 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.316       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 27490       |\n",
      "|    policy_gradient_loss | -0.00641    |\n",
      "|    value_loss           | 2.42e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5640000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0572     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016946767 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0272     |\n",
      "|    n_updates            | 27530       |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 7.38e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5650000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.011      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5650000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018684912 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.674      |\n",
      "|    explained_variance   | 0.184       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0696     |\n",
      "|    n_updates            | 27580       |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 8.88e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5660000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0758      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5660000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018346464 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.16        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 27630       |\n",
      "|    policy_gradient_loss | -0.00682    |\n",
      "|    value_loss           | 1.12e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5670000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0978      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5670000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012181589 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.775      |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0162     |\n",
      "|    n_updates            | 27680       |\n",
      "|    policy_gradient_loss | 0.003       |\n",
      "|    value_loss           | 4.25e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.252     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 2772       |\n",
      "|    time_elapsed         | 8726       |\n",
      "|    total_timesteps      | 5677056    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03204137 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.691     |\n",
      "|    explained_variance   | 0.137      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00511   |\n",
      "|    n_updates            | 27710      |\n",
      "|    policy_gradient_loss | 0.00113    |\n",
      "|    value_loss           | 2e-05      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5680000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.127       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5680000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020970672 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.634      |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 27730       |\n",
      "|    policy_gradient_loss | -0.00889    |\n",
      "|    value_loss           | 3.03e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5690000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.13        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5690000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026125653 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | 0.0496      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 27780       |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 1.67e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5700000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.134       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5700000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013571283 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.203       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00222     |\n",
      "|    n_updates            | 27830       |\n",
      "|    policy_gradient_loss | 6.82e-05    |\n",
      "|    value_loss           | 1.55e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5710000, episode_reward=0.08 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0825      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5710000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017356087 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0167      |\n",
      "|    n_updates            | 27880       |\n",
      "|    policy_gradient_loss | -0.00437    |\n",
      "|    value_loss           | 1.41e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5720000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.104      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5720000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02048317 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.69      |\n",
      "|    explained_variance   | 0.0769     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0236    |\n",
      "|    n_updates            | 27920      |\n",
      "|    policy_gradient_loss | -0.00366   |\n",
      "|    value_loss           | 7.4e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5730000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0831      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5730000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014457978 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00219     |\n",
      "|    n_updates            | 27970       |\n",
      "|    policy_gradient_loss | -0.012      |\n",
      "|    value_loss           | 8e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5740000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0478      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5740000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016902225 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | 0.416       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00673    |\n",
      "|    n_updates            | 28020       |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 1.55e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5750000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.129       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5750000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023058929 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.475      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00338     |\n",
      "|    n_updates            | 28070       |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    value_loss           | 1.12e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5760000, episode_reward=0.16 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.164       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007817538 |\n",
      "|    clip_fraction        | 0.085       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.125       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0114      |\n",
      "|    n_updates            | 28120       |\n",
      "|    policy_gradient_loss | 0.00367     |\n",
      "|    value_loss           | 4.67e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.251      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 2816        |\n",
      "|    time_elapsed         | 8862        |\n",
      "|    total_timesteps      | 5767168     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027517857 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.559      |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 28150       |\n",
      "|    policy_gradient_loss | -0.00153    |\n",
      "|    value_loss           | 2.37e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5770000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0369      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5770000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021468189 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.307       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.026      |\n",
      "|    n_updates            | 28170       |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    value_loss           | 2.87e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5780000, episode_reward=0.14 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.137      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5780000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02228561 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.559     |\n",
      "|    explained_variance   | 0.0028     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0257    |\n",
      "|    n_updates            | 28220      |\n",
      "|    policy_gradient_loss | -0.00983   |\n",
      "|    value_loss           | 1.64e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5790000, episode_reward=0.17 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.171       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5790000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020475253 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.496      |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0263      |\n",
      "|    n_updates            | 28270       |\n",
      "|    policy_gradient_loss | 0.00841     |\n",
      "|    value_loss           | 6.43e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5800000, episode_reward=0.17 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.166      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5800000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02149715 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.488     |\n",
      "|    explained_variance   | 0.453      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0173    |\n",
      "|    n_updates            | 28320      |\n",
      "|    policy_gradient_loss | -0.00607   |\n",
      "|    value_loss           | 1.35e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5810000, episode_reward=0.19 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.189       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012096779 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.122       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0277     |\n",
      "|    n_updates            | 28360       |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    value_loss           | 5.92e-06    |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5820000, episode_reward=0.14 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.142      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5820000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02047149 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.641     |\n",
      "|    explained_variance   | 0.119      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0286    |\n",
      "|    n_updates            | 28410      |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    value_loss           | 1.14e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5830000, episode_reward=0.15 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.154       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5830000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020525377 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | -0.38       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00891    |\n",
      "|    n_updates            | 28460       |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 1.37e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5840000, episode_reward=0.07 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0721     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5840000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01974943 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.569     |\n",
      "|    explained_variance   | 0.392      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00031    |\n",
      "|    n_updates            | 28510      |\n",
      "|    policy_gradient_loss | -0.00221   |\n",
      "|    value_loss           | 2.79e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5850000, episode_reward=0.15 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.151       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5850000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010685051 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.506      |\n",
      "|    explained_variance   | 0.0371      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.013       |\n",
      "|    n_updates            | 28560       |\n",
      "|    policy_gradient_loss | 0.00172     |\n",
      "|    value_loss           | 3.38e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.252      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 650         |\n",
      "|    iterations           | 2860        |\n",
      "|    time_elapsed         | 8999        |\n",
      "|    total_timesteps      | 5857280     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029237404 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.321      |\n",
      "|    explained_variance   | 0.106       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0512      |\n",
      "|    n_updates            | 28590       |\n",
      "|    policy_gradient_loss | 0.00125     |\n",
      "|    value_loss           | 3.99e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5860000, episode_reward=0.16 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.16       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5860000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01287438 |\n",
      "|    clip_fraction        | 0.0638     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.27      |\n",
      "|    explained_variance   | 0.242      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 28610      |\n",
      "|    policy_gradient_loss | 0.00118    |\n",
      "|    value_loss           | 4.86e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5870000, episode_reward=0.09 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0908     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5870000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01596106 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.608     |\n",
      "|    explained_variance   | -0.35      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0299    |\n",
      "|    n_updates            | 28660      |\n",
      "|    policy_gradient_loss | -0.0045    |\n",
      "|    value_loss           | 1.49e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5880000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0.0955    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5880000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0448512 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.38     |\n",
      "|    explained_variance   | 0.409     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0199   |\n",
      "|    n_updates            | 28710     |\n",
      "|    policy_gradient_loss | 0.00564   |\n",
      "|    value_loss           | 2.81e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=5890000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.127       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017239042 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0383      |\n",
      "|    n_updates            | 28750       |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    value_loss           | 2.49e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5900000, episode_reward=0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.123       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010116288 |\n",
      "|    clip_fraction        | 0.0864      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.27        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00923    |\n",
      "|    n_updates            | 28800       |\n",
      "|    policy_gradient_loss | 0.002       |\n",
      "|    value_loss           | 4.38e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5910000, episode_reward=0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0544      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017427994 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.563      |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0563     |\n",
      "|    n_updates            | 28850       |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 1.03e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5920000, episode_reward=0.09 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0923      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5920000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040491417 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.0767      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0136      |\n",
      "|    n_updates            | 28900       |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 8.55e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5930000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0583      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5930000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022064675 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | -0.169      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0332     |\n",
      "|    n_updates            | 28950       |\n",
      "|    policy_gradient_loss | -0.00921    |\n",
      "|    value_loss           | 1.63e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5940000, episode_reward=0.16 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.162       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5940000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009549646 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00169    |\n",
      "|    n_updates            | 29000       |\n",
      "|    policy_gradient_loss | 0.00155     |\n",
      "|    value_loss           | 6.06e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.248     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 651        |\n",
      "|    iterations           | 2904       |\n",
      "|    time_elapsed         | 9135       |\n",
      "|    total_timesteps      | 5947392    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10187268 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.34      |\n",
      "|    explained_variance   | -0.0976    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0337    |\n",
      "|    n_updates            | 29030      |\n",
      "|    policy_gradient_loss | 0.0147     |\n",
      "|    value_loss           | 4.61e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5950000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0171    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5950000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02481726 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.518     |\n",
      "|    explained_variance   | 0.163      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0437     |\n",
      "|    n_updates            | 29050      |\n",
      "|    policy_gradient_loss | 0.000246   |\n",
      "|    value_loss           | 2.68e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5960000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0195     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5960000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02231799 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.525     |\n",
      "|    explained_variance   | 0.0858     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0106     |\n",
      "|    n_updates            | 29100      |\n",
      "|    policy_gradient_loss | -0.00402   |\n",
      "|    value_loss           | 1.52e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5970000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0395      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5970000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018201966 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.692      |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00583     |\n",
      "|    n_updates            | 29150       |\n",
      "|    policy_gradient_loss | 0.001       |\n",
      "|    value_loss           | 6.97e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5980000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00616     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5980000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014492083 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.411      |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00367    |\n",
      "|    n_updates            | 29190       |\n",
      "|    policy_gradient_loss | -0.00693    |\n",
      "|    value_loss           | 3.59e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=5990000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0286     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5990000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02538107 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.508     |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0146    |\n",
      "|    n_updates            | 29240      |\n",
      "|    policy_gradient_loss | 0.00411    |\n",
      "|    value_loss           | 6.21e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0228      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016101763 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.642      |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 29290       |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    value_loss           | 1.4e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6010000, episode_reward=0.01 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0112      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6010000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024474742 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.725      |\n",
      "|    explained_variance   | -0.0728     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 29340       |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 7.33e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6020000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0473      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025565855 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.216       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.017       |\n",
      "|    n_updates            | 29390       |\n",
      "|    policy_gradient_loss | -0.00424    |\n",
      "|    value_loss           | 1.12e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6030000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0282      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6030000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012657005 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.333      |\n",
      "|    explained_variance   | 0.508       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.017      |\n",
      "|    n_updates            | 29440       |\n",
      "|    policy_gradient_loss | 0.00258     |\n",
      "|    value_loss           | 3.81e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.245      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 2948        |\n",
      "|    time_elapsed         | 9273        |\n",
      "|    total_timesteps      | 6037504     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020861149 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.55       |\n",
      "|    explained_variance   | 0.166       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0417     |\n",
      "|    n_updates            | 29470       |\n",
      "|    policy_gradient_loss | -0.0049     |\n",
      "|    value_loss           | 4.79e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6040000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0628      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020095728 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.459      |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0174      |\n",
      "|    n_updates            | 29490       |\n",
      "|    policy_gradient_loss | -0.00111    |\n",
      "|    value_loss           | 3.81e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6050000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.032      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023582054 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.446      |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 29540       |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    value_loss           | 1.73e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6060000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0338     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009305319 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.406      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 29580       |\n",
      "|    policy_gradient_loss | -0.000391   |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6070000, episode_reward=0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0321      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6070000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023034986 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.631      |\n",
      "|    explained_variance   | 0.178       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0358     |\n",
      "|    n_updates            | 29630       |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 2.28e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6080000, episode_reward=0.09 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0902      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014151624 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.795      |\n",
      "|    explained_variance   | 0.033       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0263      |\n",
      "|    n_updates            | 29680       |\n",
      "|    policy_gradient_loss | 0.000186    |\n",
      "|    value_loss           | 5.97e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6090000, episode_reward=0.03 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0325      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019389566 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 29730       |\n",
      "|    policy_gradient_loss | -0.00297    |\n",
      "|    value_loss           | 4.87e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6100000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0807      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022672035 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | -0.442      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.024       |\n",
      "|    n_updates            | 29780       |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 8.85e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6110000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0684      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019961983 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.744      |\n",
      "|    explained_variance   | -0.0339     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 29830       |\n",
      "|    policy_gradient_loss | -0.00902    |\n",
      "|    value_loss           | 9.7e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6120000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.00303   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6120000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01616849 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.692     |\n",
      "|    explained_variance   | 0.127      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 29880      |\n",
      "|    policy_gradient_loss | -0.00383   |\n",
      "|    value_loss           | 8.88e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.238      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 2992        |\n",
      "|    time_elapsed         | 9410        |\n",
      "|    total_timesteps      | 6127616     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024745237 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | -0.0234     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0322     |\n",
      "|    n_updates            | 29910       |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 3.89e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6130000, episode_reward=0.11 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.106       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010756735 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.543      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00935    |\n",
      "|    n_updates            | 29930       |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    value_loss           | 5.03e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6140000, episode_reward=0.08 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0768      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019607944 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.519      |\n",
      "|    explained_variance   | -0.0104     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0105      |\n",
      "|    n_updates            | 29980       |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 1.85e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6150000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.105       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6150000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019434076 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.647      |\n",
      "|    explained_variance   | 0.399       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00954     |\n",
      "|    n_updates            | 30020       |\n",
      "|    policy_gradient_loss | 0.00223     |\n",
      "|    value_loss           | 1.55e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6160000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0303      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016126808 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.706      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 30070       |\n",
      "|    policy_gradient_loss | -0.00705    |\n",
      "|    value_loss           | 1.49e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6170000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0608      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024752233 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00871    |\n",
      "|    n_updates            | 30120       |\n",
      "|    policy_gradient_loss | 0.000832    |\n",
      "|    value_loss           | 7.1e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6180000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0652      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020724567 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.708      |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 30170       |\n",
      "|    policy_gradient_loss | -0.00781    |\n",
      "|    value_loss           | 1.32e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6190000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0702      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6190000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020517718 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.704      |\n",
      "|    explained_variance   | -1.26       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0289     |\n",
      "|    n_updates            | 30220       |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 6.45e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6200000, episode_reward=0.09 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0858      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014709303 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | -0.197      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0164      |\n",
      "|    n_updates            | 30270       |\n",
      "|    policy_gradient_loss | -0.000172   |\n",
      "|    value_loss           | 1.02e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6210000, episode_reward=0.08 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0756      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6210000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030989537 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0013      |\n",
      "|    n_updates            | 30320       |\n",
      "|    policy_gradient_loss | 0.00237     |\n",
      "|    value_loss           | 4.97e-06    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 9e+04     |\n",
      "|    ep_rew_mean          | -0.233    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 651       |\n",
      "|    iterations           | 3036      |\n",
      "|    time_elapsed         | 9547      |\n",
      "|    total_timesteps      | 6217728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0409238 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.426    |\n",
      "|    explained_variance   | -0.188    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00453  |\n",
      "|    n_updates            | 30350     |\n",
      "|    policy_gradient_loss | -0.0103   |\n",
      "|    value_loss           | 2.05e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=6220000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0811      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013205314 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0476      |\n",
      "|    n_updates            | 30370       |\n",
      "|    policy_gradient_loss | -0.000238   |\n",
      "|    value_loss           | 3.36e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6230000, episode_reward=0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0373      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6230000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017644595 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 30410       |\n",
      "|    policy_gradient_loss | -0.00323    |\n",
      "|    value_loss           | 1.25e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6240000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0187      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013345236 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00943     |\n",
      "|    n_updates            | 30460       |\n",
      "|    policy_gradient_loss | 0.00762     |\n",
      "|    value_loss           | 9.6e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6250000, episode_reward=0.12 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.118      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6250000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01571154 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.659     |\n",
      "|    explained_variance   | 0.338      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0344    |\n",
      "|    n_updates            | 30510      |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    value_loss           | 3.37e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6260000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.063      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6260000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01852781 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.692     |\n",
      "|    explained_variance   | 0.156      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.011      |\n",
      "|    n_updates            | 30560      |\n",
      "|    policy_gradient_loss | -0.000944  |\n",
      "|    value_loss           | 8.27e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6270000, episode_reward=0.08 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0799      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6270000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014377843 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | 0.115       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0571      |\n",
      "|    n_updates            | 30610       |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    value_loss           | 3.61e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6280000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0635     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6280000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03155992 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.612     |\n",
      "|    explained_variance   | -0.114     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 30660      |\n",
      "|    policy_gradient_loss | -0.0144    |\n",
      "|    value_loss           | 8.53e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6290000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0566      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6290000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021358676 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 30710       |\n",
      "|    policy_gradient_loss | -0.00249    |\n",
      "|    value_loss           | 2.83e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6300000, episode_reward=0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.047       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013974715 |\n",
      "|    clip_fraction        | 0.0933      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.566       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00371    |\n",
      "|    n_updates            | 30760       |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 5.75e-06    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.227     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 651        |\n",
      "|    iterations           | 3080       |\n",
      "|    time_elapsed         | 9683       |\n",
      "|    total_timesteps      | 6307840    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02695306 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.454     |\n",
      "|    explained_variance   | 0.187      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0392    |\n",
      "|    n_updates            | 30790      |\n",
      "|    policy_gradient_loss | -0.00944   |\n",
      "|    value_loss           | 2.63e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6310000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0286      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6310000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014231781 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.412      |\n",
      "|    explained_variance   | 0.271       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 30810       |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    value_loss           | 3.55e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6320000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0634     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6320000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01872191 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.496     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0115    |\n",
      "|    n_updates            | 30850      |\n",
      "|    policy_gradient_loss | -0.00781   |\n",
      "|    value_loss           | 1.3e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6330000, episode_reward=0.09 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0919      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6330000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012394855 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.513       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00156     |\n",
      "|    n_updates            | 30900       |\n",
      "|    policy_gradient_loss | 0.00109     |\n",
      "|    value_loss           | 1.57e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6340000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0548      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014279068 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.822      |\n",
      "|    explained_variance   | 0.168       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0303     |\n",
      "|    n_updates            | 30950       |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 1.15e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6350000, episode_reward=0.07 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0658      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007273551 |\n",
      "|    clip_fraction        | 0.0854      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.63        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00995    |\n",
      "|    n_updates            | 31000       |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    value_loss           | 8.7e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6360000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0559      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020828594 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00384    |\n",
      "|    n_updates            | 31050       |\n",
      "|    policy_gradient_loss | 0.000383    |\n",
      "|    value_loss           | 7.81e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6370000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.064       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6370000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017498508 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.528      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 31100       |\n",
      "|    policy_gradient_loss | -0.00719    |\n",
      "|    value_loss           | 8.61e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6380000, episode_reward=0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0407      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023735393 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.229       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0093      |\n",
      "|    n_updates            | 31150       |\n",
      "|    policy_gradient_loss | -0.000819   |\n",
      "|    value_loss           | 2.13e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6390000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.103       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6390000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.068998426 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0479     |\n",
      "|    n_updates            | 31200       |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 5.88e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.217      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 3124        |\n",
      "|    time_elapsed         | 9821        |\n",
      "|    total_timesteps      | 6397952     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028543277 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.423      |\n",
      "|    explained_variance   | -0.0962     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00851    |\n",
      "|    n_updates            | 31230       |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 3.29e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6400000, episode_reward=0.11 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.107       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029801726 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0287     |\n",
      "|    n_updates            | 31240       |\n",
      "|    policy_gradient_loss | 0.00342     |\n",
      "|    value_loss           | 3.52e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6410000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0675      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6410000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018409487 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.376       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0362     |\n",
      "|    n_updates            | 31290       |\n",
      "|    policy_gradient_loss | -0.0065     |\n",
      "|    value_loss           | 9.88e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6420000, episode_reward=0.11 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.107       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011387494 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.397      |\n",
      "|    explained_variance   | 0.0274      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0131     |\n",
      "|    n_updates            | 31340       |\n",
      "|    policy_gradient_loss | 0.000402    |\n",
      "|    value_loss           | 1.77e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6430000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0418       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 6430000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0144555755 |\n",
      "|    clip_fraction        | 0.144        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.654       |\n",
      "|    explained_variance   | 0.295        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00471     |\n",
      "|    n_updates            | 31390        |\n",
      "|    policy_gradient_loss | -0.00771     |\n",
      "|    value_loss           | 1.83e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=6440000, episode_reward=0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0391      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025314365 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.546      |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00775    |\n",
      "|    n_updates            | 31440       |\n",
      "|    policy_gradient_loss | 0.00475     |\n",
      "|    value_loss           | 1.94e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6450000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0176      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6450000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035101973 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.553      |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 31490       |\n",
      "|    policy_gradient_loss | -0.00307    |\n",
      "|    value_loss           | 5.01e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6460000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0424      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023096312 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | -0.503      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0088     |\n",
      "|    n_updates            | 31540       |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    value_loss           | 1.36e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6470000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0779      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009762537 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00665     |\n",
      "|    n_updates            | 31590       |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    value_loss           | 1e-05       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6480000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0735      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6480000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009949694 |\n",
      "|    clip_fraction        | 0.0889      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.571      |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0223      |\n",
      "|    n_updates            | 31640       |\n",
      "|    policy_gradient_loss | 0.000147    |\n",
      "|    value_loss           | 1.4e-05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.213      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 3168        |\n",
      "|    time_elapsed         | 9959        |\n",
      "|    total_timesteps      | 6488064     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021784086 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.42       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 31670       |\n",
      "|    policy_gradient_loss | -0.00917    |\n",
      "|    value_loss           | 3.5e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6490000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0572     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6490000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08377682 |\n",
      "|    clip_fraction        | 0.065      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.255     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0287    |\n",
      "|    n_updates            | 31680      |\n",
      "|    policy_gradient_loss | -0.00233   |\n",
      "|    value_loss           | 5.06e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6500000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00418    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025945395 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.448      |\n",
      "|    explained_variance   | 0.369       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 31730       |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    value_loss           | 1.15e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6510000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0276     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6510000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013231595 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00107    |\n",
      "|    n_updates            | 31780       |\n",
      "|    policy_gradient_loss | 0.00425     |\n",
      "|    value_loss           | 7.45e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6520000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0426    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6520000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01682353 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.698     |\n",
      "|    explained_variance   | 0.207      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00252   |\n",
      "|    n_updates            | 31830      |\n",
      "|    policy_gradient_loss | -0.0078    |\n",
      "|    value_loss           | 1.7e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6530000, episode_reward=0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.000658    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6530000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024753274 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0272     |\n",
      "|    n_updates            | 31880       |\n",
      "|    policy_gradient_loss | -0.002      |\n",
      "|    value_loss           | 6.21e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6540000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0269     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6540000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008435162 |\n",
      "|    clip_fraction        | 0.0803      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.435      |\n",
      "|    explained_variance   | 0.469       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00556    |\n",
      "|    n_updates            | 31930       |\n",
      "|    policy_gradient_loss | 0.000709    |\n",
      "|    value_loss           | 1.85e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6550000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0483     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6550000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014367722 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.653      |\n",
      "|    explained_variance   | -1.1        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0214     |\n",
      "|    n_updates            | 31980       |\n",
      "|    policy_gradient_loss | -0.00484    |\n",
      "|    value_loss           | 6.5e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6560000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.048       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013713293 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.0455      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0382      |\n",
      "|    n_updates            | 32030       |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    value_loss           | 1.56e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6570000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0573     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6570000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037657734 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0298     |\n",
      "|    n_updates            | 32080       |\n",
      "|    policy_gradient_loss | 0.00216     |\n",
      "|    value_loss           | 1.5e-05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.21       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 3212        |\n",
      "|    time_elapsed         | 10096       |\n",
      "|    total_timesteps      | 6578176     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027405694 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | -0.0513     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0159     |\n",
      "|    n_updates            | 32110       |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 3.2e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6580000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0496     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026566725 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.031      |\n",
      "|    n_updates            | 32120       |\n",
      "|    policy_gradient_loss | -0.000843   |\n",
      "|    value_loss           | 2.76e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6590000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.055      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6590000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02707728 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.468     |\n",
      "|    explained_variance   | 0.275      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00792   |\n",
      "|    n_updates            | 32170      |\n",
      "|    policy_gradient_loss | -0.00777   |\n",
      "|    value_loss           | 1.04e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6600000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00965     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023887323 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.401      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0201      |\n",
      "|    n_updates            | 32220       |\n",
      "|    policy_gradient_loss | 0.00344     |\n",
      "|    value_loss           | 8.1e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6610000, episode_reward=0.08 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.085      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6610000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04078082 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.569     |\n",
      "|    explained_variance   | 0.322      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00962   |\n",
      "|    n_updates            | 32270      |\n",
      "|    policy_gradient_loss | -0.0045    |\n",
      "|    value_loss           | 4.02e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6620000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0194     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013894465 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00986    |\n",
      "|    n_updates            | 32320       |\n",
      "|    policy_gradient_loss | 0.00457     |\n",
      "|    value_loss           | 7.92e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6630000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0177     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6630000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009543305 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.611      |\n",
      "|    explained_variance   | 0.529       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0199      |\n",
      "|    n_updates            | 32370       |\n",
      "|    policy_gradient_loss | -0.00324    |\n",
      "|    value_loss           | 1.72e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6640000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0798      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032220155 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.487      |\n",
      "|    explained_variance   | -0.505      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 32420       |\n",
      "|    policy_gradient_loss | -0.00272    |\n",
      "|    value_loss           | 2.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6650000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0559     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6650000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03633149 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.457     |\n",
      "|    explained_variance   | 0.126      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 32470      |\n",
      "|    policy_gradient_loss | 0.0104     |\n",
      "|    value_loss           | 7.69e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6660000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.00967   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6660000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02005841 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.593     |\n",
      "|    explained_variance   | 0.107      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0084    |\n",
      "|    n_updates            | 32510      |\n",
      "|    policy_gradient_loss | 0.00415    |\n",
      "|    value_loss           | 1.21e-05   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.207      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 3256        |\n",
      "|    time_elapsed         | 10232       |\n",
      "|    total_timesteps      | 6668288     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028109675 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | -0.62       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0492     |\n",
      "|    n_updates            | 32550       |\n",
      "|    policy_gradient_loss | -0.0152     |\n",
      "|    value_loss           | 2.03e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6670000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0431     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6670000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022065945 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | -0.0523     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0312     |\n",
      "|    n_updates            | 32560       |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    value_loss           | 1.54e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6680000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0122     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6680000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020715948 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.653      |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00647    |\n",
      "|    n_updates            | 32610       |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6690000, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0379     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6690000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017240759 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 32660       |\n",
      "|    policy_gradient_loss | 0.00113     |\n",
      "|    value_loss           | 6.83e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6700000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0462     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6700000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025750771 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00912     |\n",
      "|    n_updates            | 32710       |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    value_loss           | 2.2e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6710000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0144      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6710000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019356452 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.62       |\n",
      "|    explained_variance   | 0.425       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0311     |\n",
      "|    n_updates            | 32760       |\n",
      "|    policy_gradient_loss | -0.00557    |\n",
      "|    value_loss           | 7.33e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6720000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0478     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6720000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013712464 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.497       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00521    |\n",
      "|    n_updates            | 32810       |\n",
      "|    policy_gradient_loss | -0.00236    |\n",
      "|    value_loss           | 1.45e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6730000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0.0388    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 6730000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0398324 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.747    |\n",
      "|    explained_variance   | 0.149     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0283   |\n",
      "|    n_updates            | 32860     |\n",
      "|    policy_gradient_loss | -0.00544  |\n",
      "|    value_loss           | 8.44e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=6740000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.018      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6740000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028246075 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.163       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00643    |\n",
      "|    n_updates            | 32910       |\n",
      "|    policy_gradient_loss | 0.00694     |\n",
      "|    value_loss           | 1.94e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6750000, episode_reward=-0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0573    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6750000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03514829 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.532     |\n",
      "|    explained_variance   | 0.307      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0133    |\n",
      "|    n_updates            | 32950      |\n",
      "|    policy_gradient_loss | 0.00244    |\n",
      "|    value_loss           | 7.34e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.207      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 3300        |\n",
      "|    time_elapsed         | 10368       |\n",
      "|    total_timesteps      | 6758400     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026151177 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.604      |\n",
      "|    explained_variance   | -0.0154     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.018      |\n",
      "|    n_updates            | 32990       |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 2.61e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6760000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0345     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026898835 |\n",
      "|    clip_fraction        | 0.0963      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.381       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0224     |\n",
      "|    n_updates            | 33000       |\n",
      "|    policy_gradient_loss | 0.00808     |\n",
      "|    value_loss           | 2.53e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6770000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.011       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6770000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027267657 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | -0.0436     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0124      |\n",
      "|    n_updates            | 33050       |\n",
      "|    policy_gradient_loss | -0.00846    |\n",
      "|    value_loss           | 9.83e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6780000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0308      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011286824 |\n",
      "|    clip_fraction        | 0.081       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00752     |\n",
      "|    n_updates            | 33100       |\n",
      "|    policy_gradient_loss | 0.00958     |\n",
      "|    value_loss           | 2.1e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6790000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0397      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6790000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019031024 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 33150       |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 1.58e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6800000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00201    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6800000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015620636 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.444      |\n",
      "|    explained_variance   | 0.536       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0333     |\n",
      "|    n_updates            | 33200       |\n",
      "|    policy_gradient_loss | -0.00263    |\n",
      "|    value_loss           | 1.25e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6810000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.042       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018269006 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | -0.0184     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000201   |\n",
      "|    n_updates            | 33250       |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    value_loss           | 3.43e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6820000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0485     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6820000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031253897 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | -1.53       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0064      |\n",
      "|    n_updates            | 33300       |\n",
      "|    policy_gradient_loss | -0.00967    |\n",
      "|    value_loss           | 7.31e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6830000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00661    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6830000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029903334 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.04        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00229    |\n",
      "|    n_updates            | 33340       |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6840000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0335    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6840000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01909184 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.562     |\n",
      "|    explained_variance   | 0.125      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0302    |\n",
      "|    n_updates            | 33390      |\n",
      "|    policy_gradient_loss | -0.00194   |\n",
      "|    value_loss           | 4.74e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.203      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 651         |\n",
      "|    iterations           | 3344        |\n",
      "|    time_elapsed         | 10505       |\n",
      "|    total_timesteps      | 6848512     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021201277 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.455      |\n",
      "|    explained_variance   | 0.242       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 33430       |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 3.84e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6850000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00403    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6850000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025229651 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | 0.556       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00151     |\n",
      "|    n_updates            | 33440       |\n",
      "|    policy_gradient_loss | -0.000229   |\n",
      "|    value_loss           | 4.74e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6860000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00778    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6860000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015968606 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 33490       |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 1.13e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6870000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0413      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6870000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012573696 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.801      |\n",
      "|    explained_variance   | 0.0376      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0144     |\n",
      "|    n_updates            | 33540       |\n",
      "|    policy_gradient_loss | -0.00335    |\n",
      "|    value_loss           | 5.04e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6880000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0465     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6880000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01737555 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.562     |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0271    |\n",
      "|    n_updates            | 33590      |\n",
      "|    policy_gradient_loss | -0.00998   |\n",
      "|    value_loss           | 4.04e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6890000, episode_reward=0.14 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.139       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021884095 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.0773      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0296     |\n",
      "|    n_updates            | 33640       |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 1.29e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6900000, episode_reward=0.10 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.105       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019989278 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.489      |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00527    |\n",
      "|    n_updates            | 33690       |\n",
      "|    policy_gradient_loss | 0.00236     |\n",
      "|    value_loss           | 6.54e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6910000, episode_reward=0.12 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.121       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023442011 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.631      |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00183    |\n",
      "|    n_updates            | 33740       |\n",
      "|    policy_gradient_loss | -0.00806    |\n",
      "|    value_loss           | 2.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6920000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0645     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6920000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02156841 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.498     |\n",
      "|    explained_variance   | 0.25       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00413   |\n",
      "|    n_updates            | 33780      |\n",
      "|    policy_gradient_loss | -0.000696  |\n",
      "|    value_loss           | 7e-06      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6930000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0967      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6930000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017649638 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0315     |\n",
      "|    n_updates            | 33830       |\n",
      "|    policy_gradient_loss | -0.00389    |\n",
      "|    value_loss           | 3.09e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.204      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3388        |\n",
      "|    time_elapsed         | 10641       |\n",
      "|    total_timesteps      | 6938624     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021964349 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.582      |\n",
      "|    explained_variance   | -0.0338     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0321     |\n",
      "|    n_updates            | 33870       |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 2.66e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6940000, episode_reward=0.10 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0952     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6940000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01441328 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.816     |\n",
      "|    explained_variance   | 0.104      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0181     |\n",
      "|    n_updates            | 33880      |\n",
      "|    policy_gradient_loss | -0.00836   |\n",
      "|    value_loss           | 2.68e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6950000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0533      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6950000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026922263 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.585      |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0244     |\n",
      "|    n_updates            | 33930       |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 1.85e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6960000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0711     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6960000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01361934 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.917     |\n",
      "|    explained_variance   | 0.00487    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00113   |\n",
      "|    n_updates            | 33980      |\n",
      "|    policy_gradient_loss | 0.00149    |\n",
      "|    value_loss           | 7.71e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6970000, episode_reward=0.11 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.111       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6970000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017714772 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.339       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0158     |\n",
      "|    n_updates            | 34030       |\n",
      "|    policy_gradient_loss | -0.00802    |\n",
      "|    value_loss           | 1.49e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6980000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0835      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6980000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020321803 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.616      |\n",
      "|    explained_variance   | 0.0421      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0544     |\n",
      "|    n_updates            | 34080       |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 4.62e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=6990000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0621      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 6990000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020199295 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.621      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 34130       |\n",
      "|    policy_gradient_loss | -0.00407    |\n",
      "|    value_loss           | 3.83e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7000000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0487      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7000000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024634361 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | 0.21        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00228    |\n",
      "|    n_updates            | 34170       |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    value_loss           | 6.56e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7010000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00329     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7010000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021172628 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.197       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0354     |\n",
      "|    n_updates            | 34220       |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    value_loss           | 1.18e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7020000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00125    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014590192 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.7        |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00846     |\n",
      "|    n_updates            | 34270       |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 7.96e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3432        |\n",
      "|    time_elapsed         | 10779       |\n",
      "|    total_timesteps      | 7028736     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027125929 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0511      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0586     |\n",
      "|    n_updates            | 34310       |\n",
      "|    policy_gradient_loss | -0.0218     |\n",
      "|    value_loss           | 3.05e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7030000, episode_reward=-0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0312     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7030000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018225802 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.87       |\n",
      "|    explained_variance   | 0.0106      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000332    |\n",
      "|    n_updates            | 34320       |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 1.99e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7040000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0539      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026018705 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.652      |\n",
      "|    explained_variance   | -0.207      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0276     |\n",
      "|    n_updates            | 34370       |\n",
      "|    policy_gradient_loss | -0.00907    |\n",
      "|    value_loss           | 1.29e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7050000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0209      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008827293 |\n",
      "|    clip_fraction        | 0.0967      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.872      |\n",
      "|    explained_variance   | -0.112      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0112      |\n",
      "|    n_updates            | 34420       |\n",
      "|    policy_gradient_loss | -0.00179    |\n",
      "|    value_loss           | 6.26e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7060000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0288    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7060000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01675441 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.661     |\n",
      "|    explained_variance   | 0.192      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0126    |\n",
      "|    n_updates            | 34470      |\n",
      "|    policy_gradient_loss | -0.00937   |\n",
      "|    value_loss           | 1.42e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7070000, episode_reward=-0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0344    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7070000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01657369 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.721     |\n",
      "|    explained_variance   | 0.177      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0242    |\n",
      "|    n_updates            | 34520      |\n",
      "|    policy_gradient_loss | -0.0104    |\n",
      "|    value_loss           | 7.53e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7080000, episode_reward=-0.13 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.13      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7080000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01441779 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.691     |\n",
      "|    explained_variance   | 0.222      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00333    |\n",
      "|    n_updates            | 34570      |\n",
      "|    policy_gradient_loss | -0.00437   |\n",
      "|    value_loss           | 4.9e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7090000, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0779     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022389881 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | -0.245      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 34610       |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 7.55e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7100000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0383     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017664243 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.65       |\n",
      "|    explained_variance   | 0.0967      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0109      |\n",
      "|    n_updates            | 34660       |\n",
      "|    policy_gradient_loss | -0.00283    |\n",
      "|    value_loss           | 7.98e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7110000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0364      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011571969 |\n",
      "|    clip_fraction        | 0.0977      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0023     |\n",
      "|    n_updates            | 34710       |\n",
      "|    policy_gradient_loss | 0.00565     |\n",
      "|    value_loss           | 5.1e-06     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.205      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3476        |\n",
      "|    time_elapsed         | 10916       |\n",
      "|    total_timesteps      | 7118848     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030862242 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.143       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 34750       |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 2.72e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7120000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0316     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018678002 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.804      |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0377     |\n",
      "|    n_updates            | 34760       |\n",
      "|    policy_gradient_loss | -0.00801    |\n",
      "|    value_loss           | 2.98e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7130000, episode_reward=0.02 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0229      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029934594 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.649      |\n",
      "|    explained_variance   | -1.01       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0204      |\n",
      "|    n_updates            | 34810       |\n",
      "|    policy_gradient_loss | -0.00881    |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7140000, episode_reward=-0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.06       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009409267 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.942      |\n",
      "|    explained_variance   | 0.0094      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0298     |\n",
      "|    n_updates            | 34860       |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 4.01e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7150000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00511     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7150000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015964847 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.642      |\n",
      "|    explained_variance   | 0.146       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00661    |\n",
      "|    n_updates            | 34910       |\n",
      "|    policy_gradient_loss | -0.0064     |\n",
      "|    value_loss           | 1.58e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7160000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0999      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018723916 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.725      |\n",
      "|    explained_variance   | 0.096       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0277     |\n",
      "|    n_updates            | 34960       |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 8.7e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7170000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0136      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020753574 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 35000       |\n",
      "|    policy_gradient_loss | -0.00847    |\n",
      "|    value_loss           | 6.25e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7180000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.00459    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7180000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01523262 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.671     |\n",
      "|    explained_variance   | 0.287      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0149    |\n",
      "|    n_updates            | 35050      |\n",
      "|    policy_gradient_loss | -0.00984   |\n",
      "|    value_loss           | 9.26e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7190000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00443     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7190000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018936163 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.693      |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0316     |\n",
      "|    n_updates            | 35100       |\n",
      "|    policy_gradient_loss | -0.00684    |\n",
      "|    value_loss           | 1.1e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7200000, episode_reward=0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00182     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015301726 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.792      |\n",
      "|    explained_variance   | 0.264       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000681   |\n",
      "|    n_updates            | 35150       |\n",
      "|    policy_gradient_loss | 0.000674    |\n",
      "|    value_loss           | 5.61e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.204      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3520        |\n",
      "|    time_elapsed         | 11054       |\n",
      "|    total_timesteps      | 7208960     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025278457 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.303       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 35190       |\n",
      "|    policy_gradient_loss | -0.00761    |\n",
      "|    value_loss           | 3.24e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7210000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0344      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7210000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013990164 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.764      |\n",
      "|    explained_variance   | 0.265       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00552    |\n",
      "|    n_updates            | 35200       |\n",
      "|    policy_gradient_loss | -0.0066     |\n",
      "|    value_loss           | 2.69e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7220000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0133     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022646664 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.617      |\n",
      "|    explained_variance   | -0.0541     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00789    |\n",
      "|    n_updates            | 35250       |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 1.31e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7230000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0451     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7230000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01114141 |\n",
      "|    clip_fraction        | 0.093      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.752     |\n",
      "|    explained_variance   | 0.112      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 35300      |\n",
      "|    policy_gradient_loss | -0.000659  |\n",
      "|    value_loss           | 8.08e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7240000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0.0703    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 7240000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0217912 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.749    |\n",
      "|    explained_variance   | 0.184     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00253   |\n",
      "|    n_updates            | 35350     |\n",
      "|    policy_gradient_loss | -0.00695  |\n",
      "|    value_loss           | 1.19e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=7250000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0601      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020464085 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | -0.19       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0368     |\n",
      "|    n_updates            | 35400       |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 8.44e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7260000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0346      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018933034 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.0661      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00575    |\n",
      "|    n_updates            | 35440       |\n",
      "|    policy_gradient_loss | -0.00873    |\n",
      "|    value_loss           | 5.81e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7270000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0433     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7270000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02158308 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.64      |\n",
      "|    explained_variance   | 0.271      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00412   |\n",
      "|    n_updates            | 35490      |\n",
      "|    policy_gradient_loss | -0.0123    |\n",
      "|    value_loss           | 9.68e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7280000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0752      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017372157 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.707      |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00297    |\n",
      "|    n_updates            | 35540       |\n",
      "|    policy_gradient_loss | -0.00708    |\n",
      "|    value_loss           | 8.28e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7290000, episode_reward=0.12 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.123       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7290000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010636363 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.704      |\n",
      "|    explained_variance   | 0.364       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0195     |\n",
      "|    n_updates            | 35590       |\n",
      "|    policy_gradient_loss | 0.000109    |\n",
      "|    value_loss           | 4.85e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.201      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3564        |\n",
      "|    time_elapsed         | 11191       |\n",
      "|    total_timesteps      | 7299072     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026244782 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.671      |\n",
      "|    explained_variance   | -0.155      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -4.14e-05   |\n",
      "|    n_updates            | 35630       |\n",
      "|    policy_gradient_loss | -0.00899    |\n",
      "|    value_loss           | 2.28e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7300000, episode_reward=0.11 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.107       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7300000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020397931 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.727      |\n",
      "|    explained_variance   | 0.127       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 35640       |\n",
      "|    policy_gradient_loss | -0.0048     |\n",
      "|    value_loss           | 2.26e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7310000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0511      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7310000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020413473 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 0.0922      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00927    |\n",
      "|    n_updates            | 35690       |\n",
      "|    policy_gradient_loss | -0.00917    |\n",
      "|    value_loss           | 9.39e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7320000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0463       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7320000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068025095 |\n",
      "|    clip_fraction        | 0.0771       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.554       |\n",
      "|    explained_variance   | 0.163        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00392     |\n",
      "|    n_updates            | 35740        |\n",
      "|    policy_gradient_loss | 0.000596     |\n",
      "|    value_loss           | 1.24e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7330000, episode_reward=0.16 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.157      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7330000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01650922 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.698     |\n",
      "|    explained_variance   | 0.163      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00375   |\n",
      "|    n_updates            | 35790      |\n",
      "|    policy_gradient_loss | -0.0063    |\n",
      "|    value_loss           | 1.58e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7340000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0382      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013512521 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.0332      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0111      |\n",
      "|    n_updates            | 35830       |\n",
      "|    policy_gradient_loss | 0.00198     |\n",
      "|    value_loss           | 1.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7350000, episode_reward=0.06 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0649      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016971968 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.688      |\n",
      "|    explained_variance   | 0.155       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.032       |\n",
      "|    n_updates            | 35880       |\n",
      "|    policy_gradient_loss | -0.0067     |\n",
      "|    value_loss           | 6.59e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7360000, episode_reward=0.12 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.12        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018619962 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.568      |\n",
      "|    explained_variance   | -0.0328     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0501     |\n",
      "|    n_updates            | 35930       |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 9.18e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7370000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00814     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7370000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023607582 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.00592     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0145     |\n",
      "|    n_updates            | 35980       |\n",
      "|    policy_gradient_loss | -0.00458    |\n",
      "|    value_loss           | 8.55e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7380000, episode_reward=0.14 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.145       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014366696 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0239     |\n",
      "|    n_updates            | 36030       |\n",
      "|    policy_gradient_loss | -0.00122    |\n",
      "|    value_loss           | 5.78e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.205      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3608        |\n",
      "|    time_elapsed         | 11328       |\n",
      "|    total_timesteps      | 7389184     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021332558 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.0296      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 36070       |\n",
      "|    policy_gradient_loss | -0.00911    |\n",
      "|    value_loss           | 2.27e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7390000, episode_reward=0.07 +/- 0.05\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0747      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7390000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013422841 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.477      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0404      |\n",
      "|    n_updates            | 36080       |\n",
      "|    policy_gradient_loss | -0.00338    |\n",
      "|    value_loss           | 5.4e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7400000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0591      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021944895 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.0513     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00804    |\n",
      "|    n_updates            | 36130       |\n",
      "|    policy_gradient_loss | -0.00794    |\n",
      "|    value_loss           | 1.21e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7410000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0178       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7410000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049749087 |\n",
      "|    clip_fraction        | 0.0643       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | 0.243        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00566      |\n",
      "|    n_updates            | 36180        |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    value_loss           | 6.49e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7420000, episode_reward=0.13 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.133       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017546074 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.666      |\n",
      "|    explained_variance   | 0.0788      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 36230       |\n",
      "|    policy_gradient_loss | -0.00661    |\n",
      "|    value_loss           | 1.12e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7430000, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0175     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7430000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011665855 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00702    |\n",
      "|    n_updates            | 36270       |\n",
      "|    policy_gradient_loss | 0.000709    |\n",
      "|    value_loss           | 1.15e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7440000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0375      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017221013 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.239       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00834    |\n",
      "|    n_updates            | 36320       |\n",
      "|    policy_gradient_loss | -0.00239    |\n",
      "|    value_loss           | 7.33e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7450000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0492      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7450000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020353734 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 36370       |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    value_loss           | 8.86e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7460000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0149      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026329294 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.736      |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 36420       |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    value_loss           | 1.15e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7470000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0168     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009328842 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 0.436       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000325    |\n",
      "|    n_updates            | 36470       |\n",
      "|    policy_gradient_loss | 0.00203     |\n",
      "|    value_loss           | 6.53e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3652        |\n",
      "|    time_elapsed         | 11466       |\n",
      "|    total_timesteps      | 7479296     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026079573 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.678      |\n",
      "|    explained_variance   | 0.083       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 36510       |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 3.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7480000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.00985   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7480000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01675827 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.594     |\n",
      "|    explained_variance   | 0.0832     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 36520      |\n",
      "|    policy_gradient_loss | -0.00298   |\n",
      "|    value_loss           | 2.29e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7490000, episode_reward=0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0269      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7490000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015937975 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.695      |\n",
      "|    explained_variance   | 0.224       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00585     |\n",
      "|    n_updates            | 36570       |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 1.8e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7500000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0507      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011451904 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0179     |\n",
      "|    n_updates            | 36620       |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    value_loss           | 5.99e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7510000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0279     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7510000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02284643 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.689     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00411    |\n",
      "|    n_updates            | 36660      |\n",
      "|    policy_gradient_loss | -0.00732   |\n",
      "|    value_loss           | 2.47e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7520000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0106     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011458999 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.462       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00502     |\n",
      "|    n_updates            | 36710       |\n",
      "|    policy_gradient_loss | 2.23e-05    |\n",
      "|    value_loss           | 1.46e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7530000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00197     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7530000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023282394 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.0735      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00755     |\n",
      "|    n_updates            | 36760       |\n",
      "|    policy_gradient_loss | -0.00664    |\n",
      "|    value_loss           | 7.88e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7540000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.034      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7540000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023363326 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 36810       |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 8.97e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7550000, episode_reward=0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0122      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7550000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018421719 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0388     |\n",
      "|    n_updates            | 36860       |\n",
      "|    policy_gradient_loss | -0.00631    |\n",
      "|    value_loss           | 9.33e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7560000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00815    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7560000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038249657 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.458      |\n",
      "|    explained_variance   | 0.654       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0252     |\n",
      "|    n_updates            | 36910       |\n",
      "|    policy_gradient_loss | 0.0152      |\n",
      "|    value_loss           | 9.46e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.203      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3696        |\n",
      "|    time_elapsed         | 11603       |\n",
      "|    total_timesteps      | 7569408     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024536205 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.567      |\n",
      "|    explained_variance   | -0.0179     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.04       |\n",
      "|    n_updates            | 36950       |\n",
      "|    policy_gradient_loss | -0.0121     |\n",
      "|    value_loss           | 3.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7570000, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0409     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7570000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023063153 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | -0.132      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00278     |\n",
      "|    n_updates            | 36960       |\n",
      "|    policy_gradient_loss | -0.00426    |\n",
      "|    value_loss           | 3.07e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7580000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0188      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019938879 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.292       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 37010       |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 1.09e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7590000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0177     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7590000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014776403 |\n",
      "|    clip_fraction        | 0.0892      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.663      |\n",
      "|    explained_variance   | 0.0974      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.026       |\n",
      "|    n_updates            | 37060       |\n",
      "|    policy_gradient_loss | 0.000106    |\n",
      "|    value_loss           | 9.52e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7600000, episode_reward=-0.04 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0399    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7600000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02367878 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.688     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0255    |\n",
      "|    n_updates            | 37100      |\n",
      "|    policy_gradient_loss | -0.00794   |\n",
      "|    value_loss           | 2.41e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7610000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0138      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7610000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113321645 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.648       |\n",
      "|    explained_variance   | 0.363        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0261      |\n",
      "|    n_updates            | 37150        |\n",
      "|    policy_gradient_loss | -0.00629     |\n",
      "|    value_loss           | 1.06e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7620000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0335      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014902131 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.193       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0086     |\n",
      "|    n_updates            | 37200       |\n",
      "|    policy_gradient_loss | -0.00168    |\n",
      "|    value_loss           | 6.06e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7630000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00996    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7630000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024269886 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.426      |\n",
      "|    explained_variance   | 0.442       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0027     |\n",
      "|    n_updates            | 37250       |\n",
      "|    policy_gradient_loss | -0.00772    |\n",
      "|    value_loss           | 1.21e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7640000, episode_reward=0.06 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0587     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7640000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01967377 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.719     |\n",
      "|    explained_variance   | 0.503      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00361   |\n",
      "|    n_updates            | 37300      |\n",
      "|    policy_gradient_loss | -0.00574   |\n",
      "|    value_loss           | 8.68e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7650000, episode_reward=0.10 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.104       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7650000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020992134 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.746      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0338      |\n",
      "|    n_updates            | 37350       |\n",
      "|    policy_gradient_loss | -0.00748    |\n",
      "|    value_loss           | 6.72e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 9e+04        |\n",
      "|    ep_rew_mean          | -0.207       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 3740         |\n",
      "|    time_elapsed         | 11739        |\n",
      "|    total_timesteps      | 7659520      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0145415645 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.606       |\n",
      "|    explained_variance   | 0.215        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0295      |\n",
      "|    n_updates            | 37390        |\n",
      "|    policy_gradient_loss | -0.00837     |\n",
      "|    value_loss           | 2.17e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7660000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0157      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7660000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013455009 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.615      |\n",
      "|    explained_variance   | 0.152       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0202     |\n",
      "|    n_updates            | 37400       |\n",
      "|    policy_gradient_loss | -0.006      |\n",
      "|    value_loss           | 1.92e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7670000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0345     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7670000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01865039 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.611     |\n",
      "|    explained_variance   | 0.28       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0024    |\n",
      "|    n_updates            | 37450      |\n",
      "|    policy_gradient_loss | -0.00916   |\n",
      "|    value_loss           | 1.69e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7680000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0361      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7680000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010401797 |\n",
      "|    clip_fraction        | 0.0706      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.421      |\n",
      "|    explained_variance   | 0.0887      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0451     |\n",
      "|    n_updates            | 37490       |\n",
      "|    policy_gradient_loss | 0.000692    |\n",
      "|    value_loss           | 1.71e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7690000, episode_reward=0.03 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0257     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7690000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01682388 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.706     |\n",
      "|    explained_variance   | 0.164      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 37540      |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    value_loss           | 1.85e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7700000, episode_reward=0.01 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00891     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7700000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011561675 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.574      |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00656    |\n",
      "|    n_updates            | 37590       |\n",
      "|    policy_gradient_loss | -0.0061     |\n",
      "|    value_loss           | 1.01e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7710000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0449      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7710000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015311804 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.217       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00454    |\n",
      "|    n_updates            | 37640       |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    value_loss           | 6.68e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7720000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0729     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7720000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02189383 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.469     |\n",
      "|    explained_variance   | 0.14       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00311   |\n",
      "|    n_updates            | 37690      |\n",
      "|    policy_gradient_loss | -0.00731   |\n",
      "|    value_loss           | 9.92e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7730000, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0185     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7730000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014034813 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | 0.064       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0278     |\n",
      "|    n_updates            | 37740       |\n",
      "|    policy_gradient_loss | -0.00436    |\n",
      "|    value_loss           | 1.17e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7740000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0144     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7740000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01307383 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.65      |\n",
      "|    explained_variance   | 0.151      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0418    |\n",
      "|    n_updates            | 37790      |\n",
      "|    policy_gradient_loss | -0.0068    |\n",
      "|    value_loss           | 9.04e-06   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.21       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3784        |\n",
      "|    time_elapsed         | 11876       |\n",
      "|    total_timesteps      | 7749632     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018914789 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.626      |\n",
      "|    explained_variance   | 0.478       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00839    |\n",
      "|    n_updates            | 37830       |\n",
      "|    policy_gradient_loss | -0.00502    |\n",
      "|    value_loss           | 2.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7750000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00363     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7750000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008797191 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.621      |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.013      |\n",
      "|    n_updates            | 37840       |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 2.57e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7760000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0279      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012392969 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.532      |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 37890       |\n",
      "|    policy_gradient_loss | -0.00897    |\n",
      "|    value_loss           | 1.4e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7770000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0478      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7770000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023612678 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.736      |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0218      |\n",
      "|    n_updates            | 37930       |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    value_loss           | 1.05e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7780000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0697      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013159012 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.342       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00291    |\n",
      "|    n_updates            | 37980       |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    value_loss           | 1.84e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7790000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0277      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7790000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011312246 |\n",
      "|    clip_fraction        | 0.0955      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.627      |\n",
      "|    explained_variance   | 0.0695      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 38030       |\n",
      "|    policy_gradient_loss | -0.00312    |\n",
      "|    value_loss           | 6.71e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7800000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0788      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7800000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012183182 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.838      |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00418    |\n",
      "|    n_updates            | 38080       |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 8.25e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7810000, episode_reward=0.05 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0494      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030441582 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.578       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0494     |\n",
      "|    n_updates            | 38130       |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 5.14e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7820000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0286       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7820000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064379442 |\n",
      "|    clip_fraction        | 0.0854       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.429       |\n",
      "|    explained_variance   | 0.267        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0281      |\n",
      "|    n_updates            | 38180        |\n",
      "|    policy_gradient_loss | 0.000966     |\n",
      "|    value_loss           | 1.04e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7830000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0651      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7830000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017092776 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.027       |\n",
      "|    n_updates            | 38230       |\n",
      "|    policy_gradient_loss | 0.00388     |\n",
      "|    value_loss           | 9.36e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.211      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3828        |\n",
      "|    time_elapsed         | 12014       |\n",
      "|    total_timesteps      | 7839744     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020568026 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.296       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000181   |\n",
      "|    n_updates            | 38270       |\n",
      "|    policy_gradient_loss | -0.00527    |\n",
      "|    value_loss           | 2.62e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7840000, episode_reward=0.09 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0871      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015209455 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.684      |\n",
      "|    explained_variance   | 0.385       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.021       |\n",
      "|    n_updates            | 38280       |\n",
      "|    policy_gradient_loss | -0.00163    |\n",
      "|    value_loss           | 2.68e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7850000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.134      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7850000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03132917 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.478     |\n",
      "|    explained_variance   | 0.411      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0198    |\n",
      "|    n_updates            | 38330      |\n",
      "|    policy_gradient_loss | -0.0121    |\n",
      "|    value_loss           | 3.48e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7860000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0652       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 7860000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069027604 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | 0.64         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00197      |\n",
      "|    n_updates            | 38370        |\n",
      "|    policy_gradient_loss | 0.00268      |\n",
      "|    value_loss           | 6.6e-06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=7870000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0559      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7870000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057080083 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00218    |\n",
      "|    n_updates            | 38420       |\n",
      "|    policy_gradient_loss | 0.00877     |\n",
      "|    value_loss           | 1.7e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7880000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0698      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021694165 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.136       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 38470       |\n",
      "|    policy_gradient_loss | -0.00802    |\n",
      "|    value_loss           | 5.91e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7890000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.102       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030080583 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.657      |\n",
      "|    explained_variance   | 0.139       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0365      |\n",
      "|    n_updates            | 38520       |\n",
      "|    policy_gradient_loss | 0.00215     |\n",
      "|    value_loss           | 5.1e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7900000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0643      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020956142 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 38570       |\n",
      "|    policy_gradient_loss | -0.00957    |\n",
      "|    value_loss           | 6.95e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7910000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.062       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026538573 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.355       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00842     |\n",
      "|    n_updates            | 38620       |\n",
      "|    policy_gradient_loss | 0.000333    |\n",
      "|    value_loss           | 9.54e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7920000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0422      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7920000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017018495 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.592      |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0282      |\n",
      "|    n_updates            | 38670       |\n",
      "|    policy_gradient_loss | 0.00569     |\n",
      "|    value_loss           | 1.22e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.208      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3872        |\n",
      "|    time_elapsed         | 12151       |\n",
      "|    total_timesteps      | 7929856     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030022543 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.449      |\n",
      "|    explained_variance   | 0.411       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0416      |\n",
      "|    n_updates            | 38710       |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    value_loss           | 3.56e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7930000, episode_reward=0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0293      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7930000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022191424 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.427       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00854    |\n",
      "|    n_updates            | 38720       |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    value_loss           | 2.5e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7940000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0266      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7940000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024685116 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00376    |\n",
      "|    n_updates            | 38760       |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 1.39e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7950000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0312      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7950000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017342579 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.798      |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 38810       |\n",
      "|    policy_gradient_loss | -0.0025     |\n",
      "|    value_loss           | 7.62e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7960000, episode_reward=0.10 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0981      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7960000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018794797 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.629      |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000385   |\n",
      "|    n_updates            | 38860       |\n",
      "|    policy_gradient_loss | -0.00313    |\n",
      "|    value_loss           | 1.69e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7970000, episode_reward=0.09 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0861      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7970000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013936069 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.703      |\n",
      "|    explained_variance   | 0.453       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00515     |\n",
      "|    n_updates            | 38910       |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    value_loss           | 5.75e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7980000, episode_reward=0.07 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0713      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 7980000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018911395 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 0.262       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 38960       |\n",
      "|    policy_gradient_loss | -0.000489   |\n",
      "|    value_loss           | 5.98e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7990000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0509     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7990000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04830498 |\n",
      "|    clip_fraction        | 0.205      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.609     |\n",
      "|    explained_variance   | -0.527     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 39010      |\n",
      "|    policy_gradient_loss | -0.0153    |\n",
      "|    value_loss           | 6.07e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0573     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8000000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01774767 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.662     |\n",
      "|    explained_variance   | 0.372      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00427   |\n",
      "|    n_updates            | 39060      |\n",
      "|    policy_gradient_loss | -0.00345   |\n",
      "|    value_loss           | 1.09e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8010000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0655      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8010000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020167217 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.566      |\n",
      "|    explained_variance   | 0.672       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0247     |\n",
      "|    n_updates            | 39110       |\n",
      "|    policy_gradient_loss | 0.000844    |\n",
      "|    value_loss           | 7.33e-06    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.203      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 3916        |\n",
      "|    time_elapsed         | 12289       |\n",
      "|    total_timesteps      | 8019968     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022553261 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00958    |\n",
      "|    n_updates            | 39150       |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    value_loss           | 2.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8020000, episode_reward=0.06 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0634     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8020000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02526633 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.679     |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 39160      |\n",
      "|    policy_gradient_loss | -0.00502   |\n",
      "|    value_loss           | 2.41e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8030000, episode_reward=0.11 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.106       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8030000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015107879 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.526      |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0201     |\n",
      "|    n_updates            | 39200       |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    value_loss           | 1.4e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8040000, episode_reward=0.11 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.114       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010307806 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.699      |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00612    |\n",
      "|    n_updates            | 39250       |\n",
      "|    policy_gradient_loss | 0.00209     |\n",
      "|    value_loss           | 5.2e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8050000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0343      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021193484 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.293       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00218    |\n",
      "|    n_updates            | 39300       |\n",
      "|    policy_gradient_loss | -0.00564    |\n",
      "|    value_loss           | 1.37e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8060000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.00428    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8060000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01573075 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.654     |\n",
      "|    explained_variance   | 0.389      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0298    |\n",
      "|    n_updates            | 39350      |\n",
      "|    policy_gradient_loss | -0.00633   |\n",
      "|    value_loss           | 4.27e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8070000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0755      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8070000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014564976 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.763      |\n",
      "|    explained_variance   | 0.129       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00536     |\n",
      "|    n_updates            | 39400       |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 4.92e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8080000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0234      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8080000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025621183 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0349     |\n",
      "|    n_updates            | 39450       |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 8.05e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8090000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.133        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8090000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0152387675 |\n",
      "|    clip_fraction        | 0.158        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.598       |\n",
      "|    explained_variance   | 0.0805       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0259      |\n",
      "|    n_updates            | 39500        |\n",
      "|    policy_gradient_loss | -0.000502    |\n",
      "|    value_loss           | 4.74e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8100000, episode_reward=0.06 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.059       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019949425 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.614      |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0349     |\n",
      "|    n_updates            | 39550       |\n",
      "|    policy_gradient_loss | 0.00206     |\n",
      "|    value_loss           | 8.57e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8110000, episode_reward=0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0179      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8110000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019247305 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00508     |\n",
      "|    n_updates            | 39590       |\n",
      "|    policy_gradient_loss | -0.00505    |\n",
      "|    value_loss           | 1.63e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.201   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 3960     |\n",
      "|    time_elapsed    | 12438    |\n",
      "|    total_timesteps | 8110080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8120000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0237     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8120000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01664358 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.555     |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00662   |\n",
      "|    n_updates            | 39640      |\n",
      "|    policy_gradient_loss | -0.0105    |\n",
      "|    value_loss           | 1.41e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8130000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0239      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8130000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014601585 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.671      |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00398    |\n",
      "|    n_updates            | 39690       |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 6.1e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8140000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0615      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8140000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016164716 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | 0.234       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000279    |\n",
      "|    n_updates            | 39740       |\n",
      "|    policy_gradient_loss | -0.00313    |\n",
      "|    value_loss           | 1.33e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8150000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.061       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8150000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017466642 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.694      |\n",
      "|    explained_variance   | 0.188       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0192     |\n",
      "|    n_updates            | 39790       |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    value_loss           | 6.63e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8160000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0.0063    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 8160000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0353843 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.756    |\n",
      "|    explained_variance   | 0.264     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0202   |\n",
      "|    n_updates            | 39840     |\n",
      "|    policy_gradient_loss | -0.00847  |\n",
      "|    value_loss           | 5.91e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=8170000, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0657     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8170000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034451302 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.621      |\n",
      "|    explained_variance   | -1.25       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0588     |\n",
      "|    n_updates            | 39890       |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 7.49e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8180000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00219    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035306454 |\n",
      "|    clip_fraction        | 0.0961      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.0333      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00218    |\n",
      "|    n_updates            | 39940       |\n",
      "|    policy_gradient_loss | 0.00814     |\n",
      "|    value_loss           | 6.98e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8190000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00273     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8190000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015046136 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.653      |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00337     |\n",
      "|    n_updates            | 39990       |\n",
      "|    policy_gradient_loss | 0.0171      |\n",
      "|    value_loss           | 5.48e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8200000, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0154     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016237538 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.704      |\n",
      "|    explained_variance   | 0.199       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.023      |\n",
      "|    n_updates            | 40030       |\n",
      "|    policy_gradient_loss | -0.000445   |\n",
      "|    value_loss           | 1.23e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.192   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4004     |\n",
      "|    time_elapsed    | 12575    |\n",
      "|    total_timesteps | 8200192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8210000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00813    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8210000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014552032 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.73       |\n",
      "|    explained_variance   | -0.0417     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0216     |\n",
      "|    n_updates            | 40080       |\n",
      "|    policy_gradient_loss | -0.00928    |\n",
      "|    value_loss           | 1.22e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8220000, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.024     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8220000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01846416 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.622     |\n",
      "|    explained_variance   | 0.339      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0107     |\n",
      "|    n_updates            | 40130      |\n",
      "|    policy_gradient_loss | 0.0005     |\n",
      "|    value_loss           | 2.47e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8230000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0408      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8230000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015007464 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.697      |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00937    |\n",
      "|    n_updates            | 40180       |\n",
      "|    policy_gradient_loss | -0.00802    |\n",
      "|    value_loss           | 1.58e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8240000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0119     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8240000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017257228 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.732      |\n",
      "|    explained_variance   | 0.0174      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00918    |\n",
      "|    n_updates            | 40230       |\n",
      "|    policy_gradient_loss | -0.00127    |\n",
      "|    value_loss           | 1.96e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8250000, episode_reward=-0.03 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0299     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021574218 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.711      |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0281     |\n",
      "|    n_updates            | 40280       |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    value_loss           | 5.95e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8260000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.029      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029872466 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | -1.02       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0379     |\n",
      "|    n_updates            | 40330       |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 1.09e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8270000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00422    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8270000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019080725 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.636      |\n",
      "|    explained_variance   | 0.25        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0226     |\n",
      "|    n_updates            | 40380       |\n",
      "|    policy_gradient_loss | 0.000935    |\n",
      "|    value_loss           | 2.39e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8280000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0436      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8280000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010434758 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.642      |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00372    |\n",
      "|    n_updates            | 40420       |\n",
      "|    policy_gradient_loss | -0.00282    |\n",
      "|    value_loss           | 6.58e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8290000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0164      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8290000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016860556 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.716      |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0217      |\n",
      "|    n_updates            | 40470       |\n",
      "|    policy_gradient_loss | -0.00567    |\n",
      "|    value_loss           | 2.49e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.189   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4048     |\n",
      "|    time_elapsed    | 12712    |\n",
      "|    total_timesteps | 8290304  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8300000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | -0.0179   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 8300000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0249868 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.72     |\n",
      "|    explained_variance   | 0.289     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0319   |\n",
      "|    n_updates            | 40520     |\n",
      "|    policy_gradient_loss | -0.0128   |\n",
      "|    value_loss           | 1.89e-05  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=8310000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0302     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8310000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01808846 |\n",
      "|    clip_fraction        | 0.0892     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.699     |\n",
      "|    explained_variance   | 0.363      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0161     |\n",
      "|    n_updates            | 40570      |\n",
      "|    policy_gradient_loss | 0.00426    |\n",
      "|    value_loss           | 1.08e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8320000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0413      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8320000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014039165 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.656      |\n",
      "|    explained_variance   | 0.348       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0169     |\n",
      "|    n_updates            | 40620       |\n",
      "|    policy_gradient_loss | -0.00273    |\n",
      "|    value_loss           | 9.88e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8330000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.000774   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8330000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019253265 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0308     |\n",
      "|    n_updates            | 40670       |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 5.55e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8340000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0371      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020419985 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.781      |\n",
      "|    explained_variance   | -0.0626     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0262     |\n",
      "|    n_updates            | 40720       |\n",
      "|    policy_gradient_loss | -0.00338    |\n",
      "|    value_loss           | 5.21e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8350000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0379      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022494303 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 40770       |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 7.72e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8360000, episode_reward=0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0427      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012409224 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00486     |\n",
      "|    n_updates            | 40820       |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    value_loss           | 1.97e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8370000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0226      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8370000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016751885 |\n",
      "|    clip_fraction        | 0.0734      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.523      |\n",
      "|    explained_variance   | 0.144       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00845    |\n",
      "|    n_updates            | 40860       |\n",
      "|    policy_gradient_loss | -0.00418    |\n",
      "|    value_loss           | 2.84e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8380000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0262      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016618388 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.703      |\n",
      "|    explained_variance   | 0.477       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0018      |\n",
      "|    n_updates            | 40910       |\n",
      "|    policy_gradient_loss | -0.00362    |\n",
      "|    value_loss           | 3.32e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.184   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4092     |\n",
      "|    time_elapsed    | 12848    |\n",
      "|    total_timesteps | 8380416  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8390000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0766      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8390000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021079551 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.251       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0413     |\n",
      "|    n_updates            | 40960       |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 1.63e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8400000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0657     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8400000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00889813 |\n",
      "|    clip_fraction        | 0.0974     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.809     |\n",
      "|    explained_variance   | 0.622      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0161    |\n",
      "|    n_updates            | 41010      |\n",
      "|    policy_gradient_loss | 0.00144    |\n",
      "|    value_loss           | 8.29e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8410000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0151      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8410000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017241891 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.311       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 41060       |\n",
      "|    policy_gradient_loss | -0.00771    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8420000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0208      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8420000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023563743 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.708      |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 41110       |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    value_loss           | 6.62e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8430000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0265     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8430000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016487269 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.788      |\n",
      "|    explained_variance   | 0.177       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0185     |\n",
      "|    n_updates            | 41160       |\n",
      "|    policy_gradient_loss | -0.00349    |\n",
      "|    value_loss           | 4.38e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8440000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0685      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8440000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027516656 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | 0.0978      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.01        |\n",
      "|    n_updates            | 41210       |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    value_loss           | 6.28e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8450000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0755     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8450000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02013629 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.662     |\n",
      "|    explained_variance   | 0.446      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0188    |\n",
      "|    n_updates            | 41250      |\n",
      "|    policy_gradient_loss | -0.00718   |\n",
      "|    value_loss           | 9.21e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8460000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0838      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015832178 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.501      |\n",
      "|    explained_variance   | 0.371       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0485      |\n",
      "|    n_updates            | 41300       |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    value_loss           | 4.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8470000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0164      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011496924 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.512      |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00566    |\n",
      "|    n_updates            | 41350       |\n",
      "|    policy_gradient_loss | -0.00194    |\n",
      "|    value_loss           | 2.16e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.179   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4136     |\n",
      "|    time_elapsed    | 12985    |\n",
      "|    total_timesteps | 8470528  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8480000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0424      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8480000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028541094 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.644       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0476     |\n",
      "|    n_updates            | 41400       |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 1.16e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8490000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0384      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8490000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014479952 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.725      |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00723     |\n",
      "|    n_updates            | 41450       |\n",
      "|    policy_gradient_loss | 0.00233     |\n",
      "|    value_loss           | 8.18e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8500000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0498     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018518854 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.765       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0155     |\n",
      "|    n_updates            | 41500       |\n",
      "|    policy_gradient_loss | -0.00354    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8510000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00288     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8510000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017556636 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.663      |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0346     |\n",
      "|    n_updates            | 41550       |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    value_loss           | 1.2e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8520000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0196     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8520000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014968567 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.796      |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 41600       |\n",
      "|    policy_gradient_loss | -0.00237    |\n",
      "|    value_loss           | 4.73e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8530000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0461     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8530000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020818986 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.514      |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00533     |\n",
      "|    n_updates            | 41650       |\n",
      "|    policy_gradient_loss | -0.00992    |\n",
      "|    value_loss           | 6.1e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8540000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0516     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8540000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020861942 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.025      |\n",
      "|    n_updates            | 41690       |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    value_loss           | 1.44e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8550000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0462     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8550000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015712548 |\n",
      "|    clip_fraction        | 0.0857      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.386      |\n",
      "|    explained_variance   | 0.486       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00166     |\n",
      "|    n_updates            | 41740       |\n",
      "|    policy_gradient_loss | 0.000465    |\n",
      "|    value_loss           | 2.31e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8560000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0144     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8560000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.16939092 |\n",
      "|    clip_fraction        | 0.0682     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.253     |\n",
      "|    explained_variance   | 0.529      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0208    |\n",
      "|    n_updates            | 41790      |\n",
      "|    policy_gradient_loss | 0.342      |\n",
      "|    value_loss           | 3.2e-05    |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.177   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4180     |\n",
      "|    time_elapsed    | 13122    |\n",
      "|    total_timesteps | 8560640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8570000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0162     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8570000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016130174 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.469      |\n",
      "|    explained_variance   | 0.543       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0246     |\n",
      "|    n_updates            | 41840       |\n",
      "|    policy_gradient_loss | -0.0069     |\n",
      "|    value_loss           | 1.26e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8580000, episode_reward=-0.01 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0124    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8580000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03255088 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.454     |\n",
      "|    explained_variance   | 0.242      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00125   |\n",
      "|    n_updates            | 41890      |\n",
      "|    policy_gradient_loss | 0.00249    |\n",
      "|    value_loss           | 5.8e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8590000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.045      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8590000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018053241 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000762   |\n",
      "|    n_updates            | 41940       |\n",
      "|    policy_gradient_loss | -0.0096     |\n",
      "|    value_loss           | 1.31e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8600000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0427    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8600000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02049622 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.628     |\n",
      "|    explained_variance   | 0.49       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00999   |\n",
      "|    n_updates            | 41990      |\n",
      "|    policy_gradient_loss | -0.0042    |\n",
      "|    value_loss           | 8.15e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8610000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00185     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8610000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012436074 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.774      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 42040       |\n",
      "|    policy_gradient_loss | -0.00326    |\n",
      "|    value_loss           | 3.79e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8620000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0482     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016227886 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.618      |\n",
      "|    explained_variance   | -0.0571     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00989    |\n",
      "|    n_updates            | 42080       |\n",
      "|    policy_gradient_loss | -0.00971    |\n",
      "|    value_loss           | 5.8e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8630000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0213     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8630000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022789646 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.68       |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0325     |\n",
      "|    n_updates            | 42130       |\n",
      "|    policy_gradient_loss | -0.00388    |\n",
      "|    value_loss           | 2.56e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8640000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0476      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008926363 |\n",
      "|    clip_fraction        | 0.0859      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.459      |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00558     |\n",
      "|    n_updates            | 42180       |\n",
      "|    policy_gradient_loss | -0.00316    |\n",
      "|    value_loss           | 3.12e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8650000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0609     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8650000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02159614 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.583     |\n",
      "|    explained_variance   | 0.645      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 42230      |\n",
      "|    policy_gradient_loss | -0.00384   |\n",
      "|    value_loss           | 6.44e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.173   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4224     |\n",
      "|    time_elapsed    | 13257    |\n",
      "|    total_timesteps | 8650752  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8660000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0612      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8660000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034835186 |\n",
      "|    clip_fraction        | 0.154       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.482      |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 42280       |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 9.53e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8670000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.023       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8670000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013164193 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00414    |\n",
      "|    n_updates            | 42330       |\n",
      "|    policy_gradient_loss | 0.00185     |\n",
      "|    value_loss           | 4.25e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8680000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0142      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8680000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015570444 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.622      |\n",
      "|    explained_variance   | 0.42        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00118    |\n",
      "|    n_updates            | 42380       |\n",
      "|    policy_gradient_loss | -0.00421    |\n",
      "|    value_loss           | 1.27e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8690000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.00492   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8690000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02328143 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.681     |\n",
      "|    explained_variance   | -0.207     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0141    |\n",
      "|    n_updates            | 42430      |\n",
      "|    policy_gradient_loss | -0.00577   |\n",
      "|    value_loss           | 7.25e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8700000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0246     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8700000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012476472 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.738      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0358     |\n",
      "|    n_updates            | 42480       |\n",
      "|    policy_gradient_loss | -0.00571    |\n",
      "|    value_loss           | 5.16e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8710000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0106     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8710000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015403952 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.647      |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00241    |\n",
      "|    n_updates            | 42520       |\n",
      "|    policy_gradient_loss | -0.00758    |\n",
      "|    value_loss           | 5.24e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8720000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00578     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8720000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025335306 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.465       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00208    |\n",
      "|    n_updates            | 42570       |\n",
      "|    policy_gradient_loss | -0.00663    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8730000, episode_reward=0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0372      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8730000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008757871 |\n",
      "|    clip_fraction        | 0.0663      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0111      |\n",
      "|    n_updates            | 42620       |\n",
      "|    policy_gradient_loss | 0.00242     |\n",
      "|    value_loss           | 1.74e-08    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8740000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00953     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8740000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026420506 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.561      |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0241     |\n",
      "|    n_updates            | 42670       |\n",
      "|    policy_gradient_loss | -0.00952    |\n",
      "|    value_loss           | 5.8e-06     |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.171   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4268     |\n",
      "|    time_elapsed    | 13395    |\n",
      "|    total_timesteps | 8740864  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8750000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0411      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8750000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016608901 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0176     |\n",
      "|    n_updates            | 42720       |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 1.51e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8760000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0388      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012130995 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.644      |\n",
      "|    explained_variance   | 0.31        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.012      |\n",
      "|    n_updates            | 42770       |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 6.67e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8770000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.038      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8770000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01600878 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.52      |\n",
      "|    explained_variance   | 0.74       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0265     |\n",
      "|    n_updates            | 42820      |\n",
      "|    policy_gradient_loss | -0.00419   |\n",
      "|    value_loss           | 1.01e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8780000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0735      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022022977 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | -0.247      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0254     |\n",
      "|    n_updates            | 42870       |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 7e-06       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8790000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0754      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8790000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012215194 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 42910       |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 5.95e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8800000, episode_reward=0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0415      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8800000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027081922 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.0725      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0272     |\n",
      "|    n_updates            | 42960       |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    value_loss           | 3.2e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8810000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0734      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030517746 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | 0.551       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0304      |\n",
      "|    n_updates            | 43010       |\n",
      "|    policy_gradient_loss | 0.00157     |\n",
      "|    value_loss           | 6.95e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8820000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0518      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8820000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011954445 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.59       |\n",
      "|    explained_variance   | -1.6        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00988     |\n",
      "|    n_updates            | 43060       |\n",
      "|    policy_gradient_loss | -0.0083     |\n",
      "|    value_loss           | 5.7e-07     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8830000, episode_reward=0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0744     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8830000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01185194 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.629     |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00311    |\n",
      "|    n_updates            | 43110      |\n",
      "|    policy_gradient_loss | 0.00336    |\n",
      "|    value_loss           | 1.85e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.171   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4312     |\n",
      "|    time_elapsed    | 13531    |\n",
      "|    total_timesteps | 8830976  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8840000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0725      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020678386 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.468      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 43160       |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 1.34e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8850000, episode_reward=0.09 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0884      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8850000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022581976 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.644      |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0154     |\n",
      "|    n_updates            | 43210       |\n",
      "|    policy_gradient_loss | 0.00361     |\n",
      "|    value_loss           | 8.41e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8860000, episode_reward=0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0195      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8860000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019264814 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.464      |\n",
      "|    explained_variance   | 0.555       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0302     |\n",
      "|    n_updates            | 43260       |\n",
      "|    policy_gradient_loss | -0.00803    |\n",
      "|    value_loss           | 3.25e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8870000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00721     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8870000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024786087 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0482     |\n",
      "|    n_updates            | 43310       |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    value_loss           | 6.31e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8880000, episode_reward=0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.02        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8880000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016839541 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0137     |\n",
      "|    n_updates            | 43350       |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    value_loss           | 5.87e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8890000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0292      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023491273 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.623      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0126     |\n",
      "|    n_updates            | 43400       |\n",
      "|    policy_gradient_loss | -0.0155     |\n",
      "|    value_loss           | 6.85e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8900000, episode_reward=0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0292      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8900000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015014946 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 43450       |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    value_loss           | 7.27e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8910000, episode_reward=0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0205      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038286664 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.322      |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 43500       |\n",
      "|    policy_gradient_loss | 0.00723     |\n",
      "|    value_loss           | 3.25e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8920000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0324     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8920000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01684709 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.624     |\n",
      "|    explained_variance   | 0.343      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0185    |\n",
      "|    n_updates            | 43550      |\n",
      "|    policy_gradient_loss | -0.00341   |\n",
      "|    value_loss           | 1.74e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4356     |\n",
      "|    time_elapsed    | 13668    |\n",
      "|    total_timesteps | 8921088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8930000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.00059    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8930000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02431322 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.557     |\n",
      "|    explained_variance   | 0.029      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0167    |\n",
      "|    n_updates            | 43600      |\n",
      "|    policy_gradient_loss | -0.0148    |\n",
      "|    value_loss           | 1.39e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8940000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.00299      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8940000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077938307 |\n",
      "|    clip_fraction        | 0.12         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.629       |\n",
      "|    explained_variance   | 0.85         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0217      |\n",
      "|    n_updates            | 43650        |\n",
      "|    policy_gradient_loss | 0.0016       |\n",
      "|    value_loss           | 3.42e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8950000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0103     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8950000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014369115 |\n",
      "|    clip_fraction        | 0.0981      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.341      |\n",
      "|    explained_variance   | 0.493       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0134     |\n",
      "|    n_updates            | 43700       |\n",
      "|    policy_gradient_loss | -0.00575    |\n",
      "|    value_loss           | 1.16e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=8960000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0228     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8960000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03661416 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.213     |\n",
      "|    explained_variance   | 0.515      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.055     |\n",
      "|    n_updates            | 43740      |\n",
      "|    policy_gradient_loss | 0.00492    |\n",
      "|    value_loss           | 5.08e-07   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8970000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0119       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 8970000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0149519555 |\n",
      "|    clip_fraction        | 0.128        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.675       |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.025       |\n",
      "|    n_updates            | 43790        |\n",
      "|    policy_gradient_loss | -0.00774     |\n",
      "|    value_loss           | 6.48e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=8980000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.00806    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8980000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02463748 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.571     |\n",
      "|    explained_variance   | 0.249      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 43840      |\n",
      "|    policy_gradient_loss | -0.017     |\n",
      "|    value_loss           | 3.43e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8990000, episode_reward=0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.028       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 8990000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019827485 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.649       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0043      |\n",
      "|    n_updates            | 43890       |\n",
      "|    policy_gradient_loss | -0.00421    |\n",
      "|    value_loss           | 5.65e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9000000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.000224    |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9000000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055725877 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.366       |\n",
      "|    explained_variance   | 0.534        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00406      |\n",
      "|    n_updates            | 43940        |\n",
      "|    policy_gradient_loss | 0.0031       |\n",
      "|    value_loss           | 1.8e-05      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9010000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0203      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9010000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010488243 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.704      |\n",
      "|    explained_variance   | 0.245       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0185      |\n",
      "|    n_updates            | 43990       |\n",
      "|    policy_gradient_loss | -0.000918   |\n",
      "|    value_loss           | 2.21e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.168   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4400     |\n",
      "|    time_elapsed    | 13804    |\n",
      "|    total_timesteps | 9011200  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9020000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00484    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9020000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022440031 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.503      |\n",
      "|    explained_variance   | -0.26       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 44040       |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 1.9e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9030000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0153     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9030000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015102759 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.18        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0191     |\n",
      "|    n_updates            | 44090       |\n",
      "|    policy_gradient_loss | 0.00401     |\n",
      "|    value_loss           | 4.13e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9040000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00855     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9040000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008667566 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.517      |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 44140       |\n",
      "|    policy_gradient_loss | -0.000989   |\n",
      "|    value_loss           | 8.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9050000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00119    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9050000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021762991 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.34       |\n",
      "|    explained_variance   | 0.418       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00312     |\n",
      "|    n_updates            | 44180       |\n",
      "|    policy_gradient_loss | -0.00699    |\n",
      "|    value_loss           | 7.02e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9060000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.000442   |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9060000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023549367 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.687      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00173     |\n",
      "|    n_updates            | 44230       |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 8.68e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9070000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.018      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9070000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017717473 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.633      |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0436     |\n",
      "|    n_updates            | 44280       |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 7.16e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9080000, episode_reward=0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.018      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9080000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05804942 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.514     |\n",
      "|    explained_variance   | 0.165      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0167    |\n",
      "|    n_updates            | 44330      |\n",
      "|    policy_gradient_loss | 0.00966    |\n",
      "|    value_loss           | 1.08e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9090000, episode_reward=-0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0694     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9090000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013760161 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0192      |\n",
      "|    n_updates            | 44380       |\n",
      "|    policy_gradient_loss | 0.000548    |\n",
      "|    value_loss           | 5.78e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9100000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0224     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9100000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019403122 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00619    |\n",
      "|    n_updates            | 44430       |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    value_loss           | 3.31e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.173   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4444     |\n",
      "|    time_elapsed    | 13942    |\n",
      "|    total_timesteps | 9101312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9110000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0294    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9110000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03678505 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.526     |\n",
      "|    explained_variance   | -0.102     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0455    |\n",
      "|    n_updates            | 44480      |\n",
      "|    policy_gradient_loss | -0.0168    |\n",
      "|    value_loss           | 1.6e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9120000, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0835     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9120000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047858518 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.461      |\n",
      "|    explained_variance   | -0.0669     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0143      |\n",
      "|    n_updates            | 44530       |\n",
      "|    policy_gradient_loss | 0.0175      |\n",
      "|    value_loss           | 2.42e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9130000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0116     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9130000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01724852 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.493     |\n",
      "|    explained_variance   | 0.349      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0043    |\n",
      "|    n_updates            | 44580      |\n",
      "|    policy_gradient_loss | -0.00271   |\n",
      "|    value_loss           | 7.21e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9140000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0307    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9140000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04170453 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.609     |\n",
      "|    explained_variance   | 0.405      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0508     |\n",
      "|    n_updates            | 44620      |\n",
      "|    policy_gradient_loss | -0.00537   |\n",
      "|    value_loss           | 5.3e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9150000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0478     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9150000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009110792 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.658      |\n",
      "|    explained_variance   | 0.432       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00233    |\n",
      "|    n_updates            | 44670       |\n",
      "|    policy_gradient_loss | -0.00402    |\n",
      "|    value_loss           | 5.67e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9160000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0391     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9160000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020848226 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.714       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0432     |\n",
      "|    n_updates            | 44720       |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 6.05e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9170000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0554      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9170000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0151542565 |\n",
      "|    clip_fraction        | 0.151        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.588       |\n",
      "|    explained_variance   | 0.636        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0131      |\n",
      "|    n_updates            | 44770        |\n",
      "|    policy_gradient_loss | -0.000971    |\n",
      "|    value_loss           | 1.68e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9180000, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0794     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9180000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021044068 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.538      |\n",
      "|    explained_variance   | 0.378       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00797     |\n",
      "|    n_updates            | 44820       |\n",
      "|    policy_gradient_loss | 0.000197    |\n",
      "|    value_loss           | 1.23e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9190000, episode_reward=-0.04 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0421     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9190000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015447921 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.515      |\n",
      "|    explained_variance   | 0.154       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00496    |\n",
      "|    n_updates            | 44870       |\n",
      "|    policy_gradient_loss | -0.00247    |\n",
      "|    value_loss           | 1.92e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.168   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4488     |\n",
      "|    time_elapsed    | 14079    |\n",
      "|    total_timesteps | 9191424  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9200000, episode_reward=-0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0166     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9200000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029124804 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | -0.178      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0502     |\n",
      "|    n_updates            | 44920       |\n",
      "|    policy_gradient_loss | -0.0176     |\n",
      "|    value_loss           | 2.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9210000, episode_reward=-0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.0582      |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9210000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064711957 |\n",
      "|    clip_fraction        | 0.0904       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.315       |\n",
      "|    explained_variance   | 0.223        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0105      |\n",
      "|    n_updates            | 44970        |\n",
      "|    policy_gradient_loss | 0.00708      |\n",
      "|    value_loss           | 6.6e-06      |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9220000, episode_reward=-0.12 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.118      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9220000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017950226 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.658       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0192      |\n",
      "|    n_updates            | 45010       |\n",
      "|    policy_gradient_loss | -0.00462    |\n",
      "|    value_loss           | 3.73e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9230000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.000672    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9230000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015304802 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.527      |\n",
      "|    explained_variance   | 0.677       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0139     |\n",
      "|    n_updates            | 45060       |\n",
      "|    policy_gradient_loss | 0.00197     |\n",
      "|    value_loss           | 6.39e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9240000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | -0.00678     |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9240000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083014015 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.332       |\n",
      "|    explained_variance   | 0.859        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00109      |\n",
      "|    n_updates            | 45110        |\n",
      "|    policy_gradient_loss | 0.000808     |\n",
      "|    value_loss           | 6.72e-06     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9250000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00133    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9250000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014106199 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0222      |\n",
      "|    n_updates            | 45160       |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    value_loss           | 1.34e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9260000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0347      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9260000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017093698 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.698      |\n",
      "|    explained_variance   | 0.046       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0087     |\n",
      "|    n_updates            | 45210       |\n",
      "|    policy_gradient_loss | 0.00224     |\n",
      "|    value_loss           | 1.09e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9270000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.019       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9270000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016627029 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.776      |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0152      |\n",
      "|    n_updates            | 45260       |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    value_loss           | 6.53e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9280000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0437     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9280000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02171335 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.587     |\n",
      "|    explained_variance   | 0.0834     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0279    |\n",
      "|    n_updates            | 45310      |\n",
      "|    policy_gradient_loss | -0.0024    |\n",
      "|    value_loss           | 1.65e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.16    |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4532     |\n",
      "|    time_elapsed    | 14217    |\n",
      "|    total_timesteps | 9281536  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9290000, episode_reward=-0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0128     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9290000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021613121 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.0951      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00679    |\n",
      "|    n_updates            | 45360       |\n",
      "|    policy_gradient_loss | -0.00713    |\n",
      "|    value_loss           | 1.15e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9300000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.00185    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9300000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02167235 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.646     |\n",
      "|    explained_variance   | 0.257      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.05       |\n",
      "|    n_updates            | 45410      |\n",
      "|    policy_gradient_loss | 0.00592    |\n",
      "|    value_loss           | 5.1e-06    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9310000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0169     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9310000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029424671 |\n",
      "|    clip_fraction        | 0.184       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 0.338       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0229      |\n",
      "|    n_updates            | 45450       |\n",
      "|    policy_gradient_loss | -0.00535    |\n",
      "|    value_loss           | 1.58e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9320000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0425      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9320000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014053028 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.628      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00162    |\n",
      "|    n_updates            | 45500       |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    value_loss           | 6.15e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9330000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00791     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9330000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021881284 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.551      |\n",
      "|    explained_variance   | 0.739       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00768     |\n",
      "|    n_updates            | 45550       |\n",
      "|    policy_gradient_loss | -0.00458    |\n",
      "|    value_loss           | 5.62e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9340000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0259     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9340000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023073766 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.832       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00705     |\n",
      "|    n_updates            | 45600       |\n",
      "|    policy_gradient_loss | -0.00491    |\n",
      "|    value_loss           | 2.96e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9350000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0188     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9350000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018757937 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | 0.0526      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0277     |\n",
      "|    n_updates            | 45650       |\n",
      "|    policy_gradient_loss | 0.00482     |\n",
      "|    value_loss           | 8.34e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9360000, episode_reward=-0.02 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0155     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9360000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019112943 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00958    |\n",
      "|    n_updates            | 45700       |\n",
      "|    policy_gradient_loss | 0.00334     |\n",
      "|    value_loss           | 5.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9370000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0121    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9370000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02749525 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.494     |\n",
      "|    explained_variance   | 0.182      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0132     |\n",
      "|    n_updates            | 45750      |\n",
      "|    policy_gradient_loss | -0.00102   |\n",
      "|    value_loss           | 1.81e-05   |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.145   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4576     |\n",
      "|    time_elapsed    | 14354    |\n",
      "|    total_timesteps | 9371648  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9380000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0105     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9380000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023213044 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.508      |\n",
      "|    explained_variance   | 0.0488      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0188     |\n",
      "|    n_updates            | 45800       |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 8.65e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9390000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.000744    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9390000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020937188 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.754      |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0131      |\n",
      "|    n_updates            | 45840       |\n",
      "|    policy_gradient_loss | 0.000851    |\n",
      "|    value_loss           | 8.81e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9400000, episode_reward=0.09 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0875      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9400000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019595474 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.605      |\n",
      "|    explained_variance   | 0.353       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0603      |\n",
      "|    n_updates            | 45890       |\n",
      "|    policy_gradient_loss | -0.0035     |\n",
      "|    value_loss           | 1.88e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9410000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0275      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9410000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016618114 |\n",
      "|    clip_fraction        | 0.0993      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.433      |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00463     |\n",
      "|    n_updates            | 45940       |\n",
      "|    policy_gradient_loss | -0.000969   |\n",
      "|    value_loss           | 4.35e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9420000, episode_reward=0.09 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0886     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9420000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01301574 |\n",
      "|    clip_fraction        | 0.0949     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.501     |\n",
      "|    explained_variance   | 0.439      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 45990      |\n",
      "|    policy_gradient_loss | -0.0027    |\n",
      "|    value_loss           | 3.12e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9430000, episode_reward=0.06 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0619      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9430000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020702444 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0427     |\n",
      "|    n_updates            | 46040       |\n",
      "|    policy_gradient_loss | -0.00939    |\n",
      "|    value_loss           | 4.9e-06     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9440000, episode_reward=0.13 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.13       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9440000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03636425 |\n",
      "|    clip_fraction        | 0.165      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.586     |\n",
      "|    explained_variance   | 0.641      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0271    |\n",
      "|    n_updates            | 46090      |\n",
      "|    policy_gradient_loss | 0.0305     |\n",
      "|    value_loss           | 2.73e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9450000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0458      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9450000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015676644 |\n",
      "|    clip_fraction        | 0.0991      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.597      |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00729    |\n",
      "|    n_updates            | 46140       |\n",
      "|    policy_gradient_loss | 0.000225    |\n",
      "|    value_loss           | 1.43e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9460000, episode_reward=0.07 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.072       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9460000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019120585 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.204       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0103     |\n",
      "|    n_updates            | 46190       |\n",
      "|    policy_gradient_loss | -0.00129    |\n",
      "|    value_loss           | 2e-05       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.139   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4620     |\n",
      "|    time_elapsed    | 14491    |\n",
      "|    total_timesteps | 9461760  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9470000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0201      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9470000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022272725 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.565      |\n",
      "|    explained_variance   | 0.29        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0407     |\n",
      "|    n_updates            | 46240       |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 1.56e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9480000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0353      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9480000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020961009 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.742      |\n",
      "|    explained_variance   | 0.439       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0165      |\n",
      "|    n_updates            | 46280       |\n",
      "|    policy_gradient_loss | 0.00312     |\n",
      "|    value_loss           | 5.35e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9490000, episode_reward=0.02 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0165      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9490000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015035642 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.689      |\n",
      "|    explained_variance   | 0.22        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00812     |\n",
      "|    n_updates            | 46330       |\n",
      "|    policy_gradient_loss | -0.00534    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9500000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0314     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9500000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013667878 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.705      |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0107     |\n",
      "|    n_updates            | 46380       |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    value_loss           | 4.94e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9510000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0338     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9510000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007579159 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.549      |\n",
      "|    explained_variance   | 0.859       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0135     |\n",
      "|    n_updates            | 46430       |\n",
      "|    policy_gradient_loss | 0.000729    |\n",
      "|    value_loss           | 1.47e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9520000, episode_reward=0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 9.99e+03 |\n",
      "|    mean_reward          | 0.0524   |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 9520000  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.037879 |\n",
      "|    clip_fraction        | 0.201    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -0.634   |\n",
      "|    explained_variance   | 0.782    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | -0.0412  |\n",
      "|    n_updates            | 46480    |\n",
      "|    policy_gradient_loss | -0.0173  |\n",
      "|    value_loss           | 6.57e-06 |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=9530000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0432     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9530000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01672833 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.74      |\n",
      "|    explained_variance   | 0.0983     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0102    |\n",
      "|    n_updates            | 46530      |\n",
      "|    policy_gradient_loss | -0.000408  |\n",
      "|    value_loss           | 1.11e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9540000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0291     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9540000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.044386923 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.717      |\n",
      "|    explained_variance   | 0.0242      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.016      |\n",
      "|    n_updates            | 46580       |\n",
      "|    policy_gradient_loss | 0.0124      |\n",
      "|    value_loss           | 1.47e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9550000, episode_reward=-0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | -0.0272   |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9550000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0138647 |\n",
      "|    clip_fraction        | 0.125     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.569    |\n",
      "|    explained_variance   | 0.386     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.00756   |\n",
      "|    n_updates            | 46630     |\n",
      "|    policy_gradient_loss | -0.00367  |\n",
      "|    value_loss           | 1.44e-05  |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.128   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4664     |\n",
      "|    time_elapsed    | 14629    |\n",
      "|    total_timesteps | 9551872  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9560000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.00357   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9560000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01846863 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.476     |\n",
      "|    explained_variance   | 0.371      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00167   |\n",
      "|    n_updates            | 46670      |\n",
      "|    policy_gradient_loss | -0.00826   |\n",
      "|    value_loss           | 7.49e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9570000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0193      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9570000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016136922 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.746      |\n",
      "|    explained_variance   | 0.407       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00526     |\n",
      "|    n_updates            | 46720       |\n",
      "|    policy_gradient_loss | -0.00432    |\n",
      "|    value_loss           | 6.19e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9580000, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0865     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9580000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017902099 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.668      |\n",
      "|    explained_variance   | 0.678       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.024      |\n",
      "|    n_updates            | 46770       |\n",
      "|    policy_gradient_loss | -0.00815    |\n",
      "|    value_loss           | 1.04e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9590000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0312      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9590000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022296539 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.799      |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0441     |\n",
      "|    n_updates            | 46820       |\n",
      "|    policy_gradient_loss | -0.000659   |\n",
      "|    value_loss           | 6.39e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9600000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0214      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9600000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012503647 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0117     |\n",
      "|    n_updates            | 46870       |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 1.71e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9610000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0253      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9610000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018141676 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.64       |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0206     |\n",
      "|    n_updates            | 46920       |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 7.27e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9620000, episode_reward=0.06 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.065       |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9620000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019002654 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.685      |\n",
      "|    explained_variance   | 0.0156      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0165     |\n",
      "|    n_updates            | 46970       |\n",
      "|    policy_gradient_loss | 0.00104     |\n",
      "|    value_loss           | 1.11e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9630000, episode_reward=0.08 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0802      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9630000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023336966 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.662      |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0232     |\n",
      "|    n_updates            | 47020       |\n",
      "|    policy_gradient_loss | 0.0118      |\n",
      "|    value_loss           | 1.97e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9640000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0458      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9640000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011949779 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00678    |\n",
      "|    n_updates            | 47070       |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 1.93e-05    |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 9e+04    |\n",
      "|    ep_rew_mean     | -0.118   |\n",
      "| time/              |          |\n",
      "|    fps             | 652      |\n",
      "|    iterations      | 4708     |\n",
      "|    time_elapsed    | 14766    |\n",
      "|    total_timesteps | 9641984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9650000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0126      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9650000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018047005 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.693      |\n",
      "|    explained_variance   | 0.17        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0149     |\n",
      "|    n_updates            | 47110       |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    value_loss           | 1.27e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9660000, episode_reward=-0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0472     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9660000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013955465 |\n",
      "|    clip_fraction        | 0.084       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.445      |\n",
      "|    explained_variance   | 0.326       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0368      |\n",
      "|    n_updates            | 47160       |\n",
      "|    policy_gradient_loss | 0.00516     |\n",
      "|    value_loss           | 6.47e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9670000, episode_reward=-0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00418    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9670000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020672908 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.641      |\n",
      "|    explained_variance   | 0.467       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0281     |\n",
      "|    n_updates            | 47210       |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    value_loss           | 1.47e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9680000, episode_reward=-0.03 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0335    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9680000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03237631 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.616     |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0322     |\n",
      "|    n_updates            | 47260      |\n",
      "|    policy_gradient_loss | -0.000295  |\n",
      "|    value_loss           | 4.76e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9690000, episode_reward=-0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00649    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9690000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009578476 |\n",
      "|    clip_fraction        | 0.096       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.53       |\n",
      "|    explained_variance   | 0.631       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00928    |\n",
      "|    n_updates            | 47310       |\n",
      "|    policy_gradient_loss | -0.000244   |\n",
      "|    value_loss           | 6.42e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9700000, episode_reward=0.02 +/- 0.03\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 9.99e+03     |\n",
      "|    mean_reward          | 0.0181       |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 9700000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0146005135 |\n",
      "|    clip_fraction        | 0.148        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.66        |\n",
      "|    explained_variance   | 0.442        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0143      |\n",
      "|    n_updates            | 47360        |\n",
      "|    policy_gradient_loss | -0.00695     |\n",
      "|    value_loss           | 1.31e-05     |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=9710000, episode_reward=0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0796      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9710000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021010956 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.435       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0167     |\n",
      "|    n_updates            | 47410       |\n",
      "|    policy_gradient_loss | 0.000439    |\n",
      "|    value_loss           | 1.26e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9720000, episode_reward=0.03 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0255      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9720000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028069708 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0675      |\n",
      "|    n_updates            | 47460       |\n",
      "|    policy_gradient_loss | 0.0196      |\n",
      "|    value_loss           | 1.99e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9730000, episode_reward=-0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00183    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9730000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024878547 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.63       |\n",
      "|    explained_variance   | 0.205       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0162      |\n",
      "|    n_updates            | 47500       |\n",
      "|    policy_gradient_loss | 0.00116     |\n",
      "|    value_loss           | 1.9e-05     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.115      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 652         |\n",
      "|    iterations           | 4752        |\n",
      "|    time_elapsed         | 14904       |\n",
      "|    total_timesteps      | 9732096     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010461481 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.576      |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 47510       |\n",
      "|    policy_gradient_loss | -0.00302    |\n",
      "|    value_loss           | 2.03e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9740000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0137      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9740000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026025333 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.709      |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0112     |\n",
      "|    n_updates            | 47550       |\n",
      "|    policy_gradient_loss | -0.00676    |\n",
      "|    value_loss           | 1.3e-05     |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9750000, episode_reward=0.01 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0127      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9750000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020935033 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.447       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0153      |\n",
      "|    n_updates            | 47600       |\n",
      "|    policy_gradient_loss | 0.0031      |\n",
      "|    value_loss           | 2.06e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9760000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0455      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9760000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021705367 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.606      |\n",
      "|    explained_variance   | 0.359       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000103   |\n",
      "|    n_updates            | 47650       |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    value_loss           | 1.45e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9770000, episode_reward=0.05 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0459      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9770000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021338027 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.584      |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0184      |\n",
      "|    n_updates            | 47700       |\n",
      "|    policy_gradient_loss | -0.00339    |\n",
      "|    value_loss           | 6.07e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9780000, episode_reward=0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0545      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9780000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026849331 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.397       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0173     |\n",
      "|    n_updates            | 47750       |\n",
      "|    policy_gradient_loss | 0.00156     |\n",
      "|    value_loss           | 5.44e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9790000, episode_reward=0.02 +/- 0.04\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0208     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9790000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01986355 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.697     |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00937    |\n",
      "|    n_updates            | 47800      |\n",
      "|    policy_gradient_loss | -0.0101    |\n",
      "|    value_loss           | 8.35e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9800000, episode_reward=-0.09 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0892    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9800000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01992882 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.513     |\n",
      "|    explained_variance   | 0.304      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0298    |\n",
      "|    n_updates            | 47850      |\n",
      "|    policy_gradient_loss | -0.00383   |\n",
      "|    value_loss           | 6.74e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9810000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0202     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9810000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032766733 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.502      |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0093      |\n",
      "|    n_updates            | 47900       |\n",
      "|    policy_gradient_loss | 0.000116    |\n",
      "|    value_loss           | 3.12e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9820000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00713     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9820000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020093687 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.479       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00165    |\n",
      "|    n_updates            | 47940       |\n",
      "|    policy_gradient_loss | -0.0037     |\n",
      "|    value_loss           | 1.98e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 9e+04       |\n",
      "|    ep_rew_mean          | -0.112      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 4796        |\n",
      "|    time_elapsed         | 15041       |\n",
      "|    total_timesteps      | 9822208     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021595139 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.521      |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.046      |\n",
      "|    n_updates            | 47950       |\n",
      "|    policy_gradient_loss | -0.000993   |\n",
      "|    value_loss           | 1.29e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9830000, episode_reward=0.00 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.00104     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9830000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024469595 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.613      |\n",
      "|    explained_variance   | 0.405       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00157     |\n",
      "|    n_updates            | 47990       |\n",
      "|    policy_gradient_loss | -0.00695    |\n",
      "|    value_loss           | 1.02e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9840000, episode_reward=-0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0166     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9840000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016814707 |\n",
      "|    clip_fraction        | 0.0442      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.167      |\n",
      "|    explained_variance   | 0.746       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0207     |\n",
      "|    n_updates            | 48040       |\n",
      "|    policy_gradient_loss | 0.00384     |\n",
      "|    value_loss           | 1.26e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9850000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0498    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9850000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02524135 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.473     |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0282    |\n",
      "|    n_updates            | 48090      |\n",
      "|    policy_gradient_loss | -0.00938   |\n",
      "|    value_loss           | 1.9e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9860000, episode_reward=-0.01 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.00982   |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9860000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03127469 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.515     |\n",
      "|    explained_variance   | 0.43       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0323    |\n",
      "|    n_updates            | 48140      |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    value_loss           | 4.89e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9870000, episode_reward=0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0357     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9870000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01358656 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.694     |\n",
      "|    explained_variance   | 0.489      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0171    |\n",
      "|    n_updates            | 48190      |\n",
      "|    policy_gradient_loss | -0.00248   |\n",
      "|    value_loss           | 3.22e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9880000, episode_reward=0.02 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 9.99e+03  |\n",
      "|    mean_reward          | 0.0208    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9880000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0157796 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.685    |\n",
      "|    explained_variance   | 0.392     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0187   |\n",
      "|    n_updates            | 48240     |\n",
      "|    policy_gradient_loss | -0.00592  |\n",
      "|    value_loss           | 5.11e-06  |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=9890000, episode_reward=-0.00 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.00219    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9890000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011256504 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.55        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0544      |\n",
      "|    n_updates            | 48290       |\n",
      "|    policy_gradient_loss | 0.00305     |\n",
      "|    value_loss           | 1.07e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9900000, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | -0.0657    |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9900000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01280537 |\n",
      "|    clip_fraction        | 0.0956     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.432     |\n",
      "|    explained_variance   | 0.84       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0273    |\n",
      "|    n_updates            | 48330      |\n",
      "|    policy_gradient_loss | -0.0032    |\n",
      "|    value_loss           | 1.61e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9910000, episode_reward=-0.05 +/- 0.00\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.049      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9910000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019427173 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | 0.237       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000679    |\n",
      "|    n_updates            | 48380       |\n",
      "|    policy_gradient_loss | 0.0103      |\n",
      "|    value_loss           | 7.19e-05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 9e+04      |\n",
      "|    ep_rew_mean          | -0.11      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 653        |\n",
      "|    iterations           | 4840       |\n",
      "|    time_elapsed         | 15178      |\n",
      "|    total_timesteps      | 9912320    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02069094 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.618     |\n",
      "|    explained_variance   | 0.715      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0141     |\n",
      "|    n_updates            | 48390      |\n",
      "|    policy_gradient_loss | 0.00387    |\n",
      "|    value_loss           | 9.13e-06   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9920000, episode_reward=-0.08 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0764     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9920000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015615676 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.575      |\n",
      "|    explained_variance   | 0.282       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0269     |\n",
      "|    n_updates            | 48430       |\n",
      "|    policy_gradient_loss | -0.0081     |\n",
      "|    value_loss           | 1.53e-05    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9930000, episode_reward=-0.03 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0265     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9930000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009589642 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00497    |\n",
      "|    n_updates            | 48480       |\n",
      "|    policy_gradient_loss | -0.0042     |\n",
      "|    value_loss           | 8.79e-07    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9940000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0127     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9940000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02108286 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.562     |\n",
      "|    explained_variance   | 0.41       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0264    |\n",
      "|    n_updates            | 48530      |\n",
      "|    policy_gradient_loss | -0.00847   |\n",
      "|    value_loss           | 1.2e-05    |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9950000, episode_reward=-0.04 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0419     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9950000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024146345 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.583      |\n",
      "|    explained_variance   | -0.0629     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0219     |\n",
      "|    n_updates            | 48580       |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 6.64e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9960000, episode_reward=0.00 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.000598    |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9960000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020479742 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.701      |\n",
      "|    explained_variance   | 0.0645      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 48630       |\n",
      "|    policy_gradient_loss | -0.00475    |\n",
      "|    value_loss           | 6.98e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9970000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | 0.0121      |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9970000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017565396 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.69       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00152    |\n",
      "|    n_updates            | 48680       |\n",
      "|    policy_gradient_loss | -0.00479    |\n",
      "|    value_loss           | 7.05e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=9980000, episode_reward=0.01 +/- 0.01\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 9.99e+03   |\n",
      "|    mean_reward          | 0.0104     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9980000    |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00778176 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.631     |\n",
      "|    explained_variance   | 0.00147    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0271     |\n",
      "|    n_updates            | 48730      |\n",
      "|    policy_gradient_loss | -0.000103  |\n",
      "|    value_loss           | 2.18e-05   |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9990000, episode_reward=-0.07 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0717     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 9990000     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022404244 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.588      |\n",
      "|    explained_variance   | 0.314       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000819   |\n",
      "|    n_updates            | 48770       |\n",
      "|    policy_gradient_loss | 0.00148     |\n",
      "|    value_loss           | 2.41e-06    |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000000, episode_reward=-0.04 +/- 0.02\n",
      "Episode length: 9990.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 9.99e+03    |\n",
      "|    mean_reward          | -0.0405     |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000000    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028654728 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.533      |\n",
      "|    explained_variance   | 0.685       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0156     |\n",
      "|    n_updates            | 48820       |\n",
      "|    policy_gradient_loss | -0.00788    |\n",
      "|    value_loss           | 8.35e-06    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fdf44e87f50>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_freq = 10000\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env=create_env(train=False, penalty=0),\n",
    "    best_model_save_path=\"./logs/best_model/\",\n",
    "    log_path=\"./logs/eval_logs/\",\n",
    "    eval_freq=eval_freq,              \n",
    "    deterministic=True          \n",
    ")\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=TradingFeatureExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    ")\n",
    "\n",
    "rollouts_per_episode = len(train_price) // 2048  + 1\n",
    "\n",
    "model = PPO(\"MlpPolicy\", create_env(), policy_kwargs=policy_kwargs, verbose=2, device=\"cpu\")\n",
    "model.learn(total_timesteps=10**7, callback=eval_callback, log_interval=rollouts_per_episode)\n",
    "model.save(\"RLF_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae369d",
   "metadata": {},
   "source": [
    "### Last model vs best model\n",
    "Clearly, the last model is overfit as it produces signficantly less reward compare to the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "663be591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last - Training - PPO: (np.float64(0.45018), np.float64(0.0))\n",
      "Last - Test - PPO: (np.float64(0.08183), np.float64(0.0))\n",
      "Best - Training - PPO: (np.float64(0.20191), np.float64(0.0))\n",
      "Best - Test - PPO: (np.float64(0.17638), np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "l = PPO.load(\"RLF_v3\", create_env(penalty=0), device=\"cpu\")\n",
    "print(f\"Last - Training - PPO: {evaluate_policy(l, create_env(penalty=0), n_eval_episodes=1)}\")\n",
    "print(f\"Last - Test - PPO: {evaluate_policy(l, create_env(train=False, penalty=0), n_eval_episodes=1)}\")\n",
    "\n",
    "b = PPO.load(\"logs/best_model/best_model\", create_env(penalty=0), device=\"cpu\")\n",
    "print(f\"Best - Training - PPO: {evaluate_policy(b, b.env, n_eval_episodes=1)}\")\n",
    "print(f\"Best - Test - PPO: {evaluate_policy(b, create_env(train=False, penalty=0), n_eval_episodes=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b208173",
   "metadata": {},
   "source": [
    "## Reward Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d39c3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rewards were calculated properly by evaluate_policy: 0.1763800084590912\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "actions = []\n",
    "rewards = []\n",
    "obs_list = []\n",
    "env = create_env(train=False, penalty=0)\n",
    "obs = env.reset()\n",
    "prev_action = np.array([1])\n",
    "\n",
    "while not done:\n",
    "\n",
    "    action, _ = b.predict(obs, deterministic=True)\n",
    "\n",
    "    if action == prev_action or action == 1:\n",
    "        action = np.array([1])\n",
    "    elif np.abs(prev_action - action) == 2:\n",
    "        prev_action = np.array([1])\n",
    "    elif prev_action == 1:\n",
    "        prev_action = action \n",
    "\n",
    "    obs, reward, done, info = env.step(action)    \n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "    obs_list.append(obs)\n",
    "\n",
    "print(f\"rewards were calculated properly by evaluate_policy: {np.sum(rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91d813ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mplfinance as mpf\n",
    "\n",
    "tdf = df.tail(10**4 - 10).iloc[100:200]\n",
    "tac = np.array(actions[100:200]).flatten()\n",
    "\n",
    "\n",
    "buy_signals = pd.Series(np.where(tac == 2, tdf['Close'], np.nan), index=tdf.index)\n",
    "sell_signals = pd.Series(np.where(tac == 0, tdf['Close'], np.nan), index=tdf.index)\n",
    "\n",
    "\n",
    "apds = [\n",
    "    mpf.make_addplot(buy_signals, type='scatter', markersize=100, marker='^', color='g'),\n",
    "    mpf.make_addplot(sell_signals, type='scatter', markersize=100, marker='v', color='r')\n",
    "]\n",
    "\n",
    "mpf.plot(tdf, type='candle', addplot=apds, style='yahoo', ylabel=\"Price\", title=\"Agent's Actions\", figsize=(14, 4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
